<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>贝叶斯算法 Bayesian Classfier</title>
    <url>/2020/09/24/Bayesian-Classfier/</url>
    <content><![CDATA[<p>贝叶斯算法是一种分类算法，用于解决逆向概率（逆概）的问题。</p>
<a id="more"></a>
<p>首先我们需要了解正向概率和逆向概率的区别。</p>
<ul>
<li><strong>正向概率</strong>：假设袋子里有 <img src="https://math.now.sh?inline=N" style="display:inline-block;margin: 0;"> 个白球，<img src="https://math.now.sh?inline=M" style="display:inline-block;margin: 0;"> 个黑球，摸一次出黑球的概率。</li>
<li><strong>逆向概率</strong>：袋子中黑白球的比例未知，摸出一个（或几个）球，根据取出球的颜色，推测袋子中黑白球的比例。</li>
</ul>
<p>然后我们先用一个例子来引入和介绍贝叶斯算法。</p>
<h3 id="学生校服问题">学生校服问题</h3>
<p>假设学校中有 60% 的男生和 40% 的女生，其中男生总穿长裤，女生一半穿长裤一半穿裙子。</p>
<p>一个正向概率的问题可能是：随机选一个学生，TA 穿长裤的概率或穿裙子的概率是多大。而一个逆向概率的问题为：一个学生穿着长裤，TA 是女生的概率是多大。</p>
<h4 id="公式推导">公式推导</h4>
<p>假设学校里总人数为 <img src="https://math.now.sh?inline=U" style="display:inline-block;margin: 0;">。</p>
<p>则穿长裤的男生人数为</p>
<p><img src="https://math.now.sh?inline=U*P%28Boy%29*P(Pants%7CBoy)" style="display:inline-block;margin: 0;"></p>
<p>其中，</p>
<ul>
<li><img src="https://math.now.sh?inline=P%28Boy%29" style="display:inline-block;margin: 0;"> 表示男生的概率 = 60%</li>
<li><img src="https://math.now.sh?inline=P%28Pants%7CBoy%29" style="display:inline-block;margin: 0;"> 表示男生穿长裤的概率 = 100%</li>
</ul>
<p>穿长裤的女生人数为</p>
<p><img src="https://math.now.sh?inline=U*P%28Girl%29*P(Pants%7CGirl)" style="display:inline-block;margin: 0;"></p>
<p>我们想要知道穿长裤的学生中女生的概率，则需要用穿长裤的女生的人数除以穿长裤的学生的总人数。</p>
<p>穿长裤的学生的总人数为</p>
<p><img src="https://math.now.sh?inline=U*P%28Boy%29*P(Pants%7CBoy)%2BU*P(Girl)*P(Pants%7CGirl)" style="display:inline-block;margin: 0;"></p>
<p>所以穿长裤的学生中女生的比例为</p>
<p><img src="https://math.now.sh?inline=%5Cbegin%7Baligned%7D%20P%28Girl%7CPants%29%20%26%20%3D%5Cfrac%7BU*P(Girl)*P(Pants%7CGirl)%7D%7B%E7%A9%BF%E9%95%BF%E8%A3%A4%E6%80%BB%E4%BA%BA%E6%95%B0%7D%20%5C%5C%20%26%20%3D%5Cfrac%7BU*P(Girl)*P(Pants%7CGirl)%7D%7BU*P(Boy)*P(Pants%7CBoy)%2BU*P(Girl)*P(Pants%7CGirl)%7D%20%5C%5C%20%26%20%3D%20%5Cfrac%7BP(Girl)*P(Pants%7CGirl)%7D%7B%5Cunderbrace%7BP(Boy)*P(Pants%7CBoy)%2BP(Girl)*P(Pants%7CGirl)%7D_%7B%E5%AD%A6%E6%A0%A1%E4%B8%AD%E7%A9%BF%E8%A3%A4%E5%AD%90%E5%AD%A6%E7%94%9F%E7%9A%84%E6%AF%94%E4%BE%8B%7D%7D%20%5C%5C%20%26%20%3D%5Cfrac%7BP(Pants%2CGirl)%7D%7BP(Pants)%7D%20%5Cend%7Baligned%7D" style="display:inline-block;margin: 0;"></p>
<h3 id="贝叶斯公式">贝叶斯公式</h3>
<p>由上面的例子，我们可以看出贝叶斯公式的一般形式为</p>
<p><img src="https://math.now.sh?inline=P%28A%7CB%29%3D%5Cfrac%7BP(B%7CA)P(A)%7D%7BP(B)%7D" style="display:inline-block;margin: 0;"></p>
<p>即求在 B 的条件下 A 的概率，可以根据在 A 的条件下 B 的概率进行计算。</p>
<p>其中 <img src="https://math.now.sh?inline=P%28A%29" style="display:inline-block;margin: 0;"> 是 <strong> 先验概率</strong>。</p>
<p>这里我们再用另一个实例来解释和应用贝叶斯。</p>
<h3 id="拼写纠正实例">拼写纠正实例</h3>
<p>一个用户输入了一个不在词典中的单词，我们需要猜测他想输入的单词是什么，即计算 <img src="https://math.now.sh?inline=P%28%E7%8C%9C%E6%B5%8B%E4%BB%96%E6%83%B3%E8%BE%93%E5%85%A5%E7%9A%84%E5%8D%95%E8%AF%8D%7C%E4%BB%96%E5%AE%9E%E9%99%85%E8%BE%93%E5%85%A5%E7%9A%84%E5%8D%95%E8%AF%8D%29" style="display:inline-block;margin: 0;">。</p>
<p>我们设用户实际输入单词为 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"> (Data，即观测数据)，猜测是某个单词的概率为 <img src="https://math.now.sh?inline=P%28h%7CD%29" style="display:inline-block;margin: 0;">，即 <img src="https://math.now.sh?inline=h" style="display:inline-block;margin: 0;"> 是我们猜测的单词，该输入单词为 <img src="https://math.now.sh?inline=h" style="display:inline-block;margin: 0;"> 的概率为 <img src="https://math.now.sh?inline=P%28h%7CD%29" style="display:inline-block;margin: 0;">。</p>
<p>根据贝叶斯公式，</p>
<p><img src="https://math.now.sh?inline=P%28h%7CD%29%3D%5Cfrac%7BP(D%7Ch)P(h)%7D%7BP(D)%7D" style="display:inline-block;margin: 0;"></p>
<p>其中，</p>
<ul>
<li>因为对于每个猜测 h1,h2,h3…，<img src="https://math.now.sh?inline=P%28D%29" style="display:inline-block;margin: 0;"> 的值不变，所以可以忽略它</li>
<li>所以 <img src="https://math.now.sh?inline=P%28h%7CD%29%20%5Cpropto%20P(h)*P(D%7Ch)" style="display:inline-block;margin: 0;">，即它俩成正比
<ul>
<li>这里 <img src="https://math.now.sh?inline=P%28h%29" style="display:inline-block;margin: 0;"> 表示猜测 <img src="https://math.now.sh?inline=h" style="display:inline-block;margin: 0;"> 本身独立的可能性大小。
<ul>
<li>比如在词库中有 1w 个单词，其中 <img src="https://math.now.sh?inline=h" style="display:inline-block;margin: 0;"> 有 5000 个，那么 <img src="https://math.now.sh?inline=P%28h%29%3D%5Cfrac%7B5000%7D%7B10000%7D%3D%5Cfrac12" style="display:inline-block;margin: 0;">。</li>
<li>即猜测的单词的_词频_会影响它的可能性的概率。比如，用户输入单词 thaw，我们猜测他可能想输入 that 或者 than，这两个单词生成输入单词的可能性都很大，那么因为 that 的词频比较高，所以我们最后会猜测用户想输入的是 that。</li>
</ul>
</li>
<li><img src="https://math.now.sh?inline=P%28D%7Ch%29" style="display:inline-block;margin: 0;"> 表示猜测 <img src="https://math.now.sh?inline=h" style="display:inline-block;margin: 0;"> 生成输入单词的可能性大小。</li>
</ul>
</li>
</ul>
<h3 id="模型比较">模型比较</h3>
<ul>
<li><strong>最大似然估计</strong>：最符合观测数据的最有优势（即 <img src="https://math.now.sh?inline=P%28D%7Ch%29" style="display:inline-block;margin: 0;"> 最大的最有优势）。
<ul>
<li>比如，我们先投掷硬币 10w 次，观察每次硬币的正反，得出有 60% 的概率为正，40% 的概率为反，那么在第 10w+1 次投掷硬币时，我们就根据前 10w 次观察的概率判断这次硬币可能为正。</li>
</ul>
</li>
<li><strong>奥卡姆剃刀</strong>：<img src="https://math.now.sh?inline=P%28h%29" style="display:inline-block;margin: 0;"> 较大的模型有较大的优势（即越常见越好）。</li>
</ul>
<p>最大似然估计应该比较常用。</p>
<h3 id="垃圾邮件过滤">垃圾邮件过滤</h3>
<p>这个应该是贝叶斯分类的经典例子。</p>
<p>我们要判断一封邮件是否为垃圾邮件。</p>
<p>首先，假设这封邮件为 D，D 由 <img src="https://math.now.sh?inline=N" style="display:inline-block;margin: 0;"> 个单词组成。我们用 <img src="https://math.now.sh?inline=h%2B" style="display:inline-block;margin: 0;"> 表示垃圾邮件，<img src="https://math.now.sh?inline=h-" style="display:inline-block;margin: 0;"> 表示正常邮件。</p>
<p>那么 D 是垃圾邮件的概率为</p>
<p><img src="https://math.now.sh?inline=P%28h%2B%7CD%29%3D%5Cfrac%7BP(h%2B)P(D%7Ch%2B)%7D%7BP(D)%7D" style="display:inline-block;margin: 0;"></p>
<p>D 不是垃圾邮件的概率为</p>
<p><img src="https://math.now.sh?inline=P%28h-%7CD%29%3D%5Cfrac%7BP(h-)P(D%7Ch-)%7D%7BP(D)%7D" style="display:inline-block;margin: 0;"></p>
<p>这里_先验概率_ <img src="https://math.now.sh?inline=P%28h%2B%29" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=P%28h-%29" style="display:inline-block;margin: 0;"> 可以由一个邮件库中垃圾邮件和正常邮件的比例求得。</p>
<p>因为 D 中包含 <img src="https://math.now.sh?inline=N" style="display:inline-block;margin: 0;"> 个单词 d1,d2,…,dn，则 <img src="https://math.now.sh?inline=P%28D%7Ch%2B%29%3DP(d1%2Cd2%2C...%2Cdn%7Ch%2B)" style="display:inline-block;margin: 0;"> 表示在垃圾邮件中出现与邮件 D 一样的邮件的概率。即</p>
<p><img src="https://math.now.sh?inline=P%28d1%2Cd2%2C...%2Cdn%7Ch%2B%29%3DP(d1%7Ch%2B)*P(d2%7Cd1%2Ch%2B)*P(d3%7Cd2%2Cd1%2Ch%2B)*..." style="display:inline-block;margin: 0;"></p>
<p>其中，<img src="https://math.now.sh?inline=P%28d1%7Ch%2B%29" style="display:inline-block;margin: 0;"> 表示一个垃圾邮件中第一个词为 d1 的概率，<img src="https://math.now.sh?inline=P%28d2%7Cd1%2Ch%2B%29" style="display:inline-block;margin: 0;"> 表示一个垃圾邮件中第一个词为 d1 后第二个词恰好为 d2 的概率（其他类推）。</p>
<p>为了方便计算，我们假设 di 与 di-1 相互独立（即完全无关，互不影响），则将其转换为 <strong> 朴素贝叶斯</strong>（Naive Bayes，NB）：</p>
<p><img src="https://math.now.sh?inline=P%28d1%2Cd2%2C...%2Cdn%7Ch%2B%29%3DP(d1%7Ch%2B)*P(d2%7Ch%2B)*P(d3%7Ch%2B)*..." style="display:inline-block;margin: 0;"></p>
<p>于是只需要统计单词 di 在垃圾邮件中出现的频率即可。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>bayes algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>数据安全能力建设 Capacity-building for Data Security</title>
    <url>/2020/10/05/Build-Capacity-for-Data-Security/</url>
    <content><![CDATA[<p> 在 Freebuf 上看到的一篇文章，作者系统地介绍了数据安全的整体架构与一些细节，从各个方面描述了数据安全需要掌握的能力。我认为对于理解数据安全和规划数据安全的路线很有帮助，所以做了一张思维导图，供学习参考。</p>
<a id="more"></a>
<p><img src="/2020/10/05/Build-Capacity-for-Data-Security/Capacity-for-Data-Security.png" alt="img"></p>
<p> 至于详细内容，还请移步大佬原文进行学习。</p>
<p> 转载：</p>
<p><a href="https://www.freebuf.com/articles/database/248950.html"> 数据安全能力建设思路 </a>（作者：oneeleven）</p>
]]></content>
      <categories>
        <category>Data Security</category>
      </categories>
      <tags>
        <tag>data sec</tag>
        <tag>freebuf</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树 Decision Tree</title>
    <url>/2020/09/23/Decision-Tree/</url>
    <content><![CDATA[<p>决策树是机器学习中的一个预测模型，它表示对象属性和对象值之间的一种映射。树中的每一个非叶子节点（决策点）表示对象属性的判断条件，其分支表示符合节点条件的对象。树的叶子节点表示对象所属的预测结果。</p>
<a id="more"></a>
<h2 id="决策树">决策树</h2>
<h3 id="构建决策树">构建决策树</h3>
<p>决策树的构建分为两个阶段：</p>
<ol>
<li>
<p>训练阶段（构造决策树）</p>
<p><img src="https://math.now.sh?inline=class%20%3D%20DecisionTree%28Data%29" style="display:inline-block;margin: 0;"></p>
</li>
<li>
<p>分类阶段（获得分类 / 决策结果）</p>
<p><img src="https://math.now.sh?inline=y%20%3D%20DecisionTree%28x%29" style="display:inline-block;margin: 0;"></p>
</li>
</ol>
<p>构建决策树的基本思想是，随着树深度的增加，节点的熵迅速地降低；熵降低的速度越快越好，越快树的高度越矮。</p>
<h4 id="构建决策树的基本步骤">构建决策树的基本步骤</h4>
<ol>
<li>将所有数据看作一个节点；</li>
<li>遍历每个特征的每一种分割方式，找到最好的分割点；</li>
<li>分割成多个节点 <img src="https://math.now.sh?inline=N_1" style="display:inline-block;margin: 0;"> 到 <img src="https://math.now.sh?inline=N_i" style="display:inline-block;margin: 0;">；</li>
<li>对 <img src="https://math.now.sh?inline=N_1" style="display:inline-block;margin: 0;"> 到 <img src="https://math.now.sh?inline=N_i" style="display:inline-block;margin: 0;"> 分别执行 2、3 步，直到每个节点足够 <strong> 纯粹</strong>。</li>
</ol>
<h5 id="评价分割点的好坏">评价分割点的好坏</h5>
<p>如果一个分割点可以将当前的所有节点分为两类，使得每一类都 <strong> 纯度 </strong> 很高，也就是同一类的数据较多，那么就是一个好分割点。</p>
<p>于是，我们需要量化“纯度”这个指标。</p>
<h5 id="量化纯度">量化纯度</h5>
<p>如果一个特征被分为 <img src="https://math.now.sh?inline=n" style="display:inline-block;margin: 0;"> 类，则每一类的比例 <img src="https://math.now.sh?inline=P%28i%29%3D%E7%AC%AC%20i%20%E7%B1%BB%E7%9A%84%E6%95%B0%E7%9B%AE%2F%E6%80%BB%E6%95%B0%E7%9B%AE" style="display:inline-block;margin: 0;">。这个特征节点会有 <img src="https://math.now.sh?inline=n" style="display:inline-block;margin: 0;"> 个分支。</p>
<p>有三种方法来度量纯度：</p>
<ul>
<li>
<p>Gini 系数</p>
<p><img src="https://math.now.sh?inline=Gini%20%3D%201%20-%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7DP%28i%29%5E2" style="display:inline-block;margin: 0;"></p>
</li>
<li>
<p>熵（一般选择）</p>
<p><img src="https://math.now.sh?inline=Entropy%20%3D%20-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%28P(i%29%20%5Ctimes%20%5Clog_2P(i))" style="display:inline-block;margin: 0;"></p>
<p>这里 <img src="https://math.now.sh?inline=%5Clog" style="display:inline-block;margin: 0;"> 底数可以取 <img src="https://math.now.sh?inline=2" style="display:inline-block;margin: 0;"> 或 <img src="https://math.now.sh?inline=e" style="display:inline-block;margin: 0;">。</p>
<p>熵代表_混乱程度_，混乱程度越大，熵越大（即越不纯）。</p>
</li>
<li>
<p>错误率</p>
<p><img src="https://math.now.sh?inline=Error%20%3D%201%20-%20%5Cmax%28P(i%29%7Ci%20%5Cin%20%5B1%2Cn%5D)" style="display:inline-block;margin: 0;"></p>
</li>
</ul>
<p>这三个_值越大，表示纯度越低_。</p>
<p>然后有纯度差的概念，也就是 <strong> 信息增益 Information Gain</strong>。信息增益表示当以一个属性作为节点进行分裂后，它未分裂时的纯度与各个分支的纯度之和的差值。</p>
<p><img src="https://math.now.sh?inline=%5CDelta%20%3D%20I%28parent%29%20-%20%5Csum_%7Bj%3D1%7D%5E%7BK%7D(%5Cfrac%7BN(v_j)%7D%7BN%7D%20%5Ctimes%20I(v_j))" style="display:inline-block;margin: 0;"></p>
<p>其中，<img src="https://math.now.sh?inline=I" style="display:inline-block;margin: 0;"> 代表不纯度（i.e. 上面三个公式之一），<img src="https://math.now.sh?inline=K" style="display:inline-block;margin: 0;"> 代表分割的节点数（一般 <img src="https://math.now.sh?inline=K%3D2" style="display:inline-block;margin: 0;">），<img src="https://math.now.sh?inline=v_j" style="display:inline-block;margin: 0;"> 表示子节点中的记录数。</p>
<p>即，当前节点的不纯度减去子节点不纯度的加权平均数，权重由子节点记录数与当前节点记录数的比例决定。</p>
<h3 id="常用算法">常用算法</h3>
<h4 id="ID3- 算法">ID3 算法</h4>
<p>ID3 算法的核心思想是以_信息增益_度量属性选择，选择分裂后信息增益最大的属性进行分裂。</p>
<h5 id="算法步骤">算法步骤</h5>
<ol>
<li>
<p>设 D 为父节点，则 D 的熵为</p>
<p><img src="https://math.now.sh?inline=Entropy%28D%29%20%3D%20-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D(P(i)%20%5Ctimes%20%5Clog_2P(i))" style="display:inline-block;margin: 0;"></p>
<p>（即，数据集中 y 的各个分类的占比为 P）</p>
</li>
<li>
<p>对 D 的每个属性 A，计算 A 对 D 划分的期望信息</p>
<p><img src="https://math.now.sh?inline=Entropy_A%28D%29%20%3D%20-%5Csum_%7Bj%3D1%7D%5E%7Bv%7D(%5Cfrac%7B%7CD_j%7C%7D%7B%7CD%7C%7DEntropy(D_j))" style="display:inline-block;margin: 0;"></p>
<p>（即，一个属性中各个分类的熵乘以这个属性的占比）</p>
</li>
<li>
<p>信息增益即为两者差值</p>
<p><img src="https://math.now.sh?inline=Gain%28A%29%20%3D%20Entropy(D)%20-%20Entropy_A(D)" style="display:inline-block;margin: 0;"></p>
<p>（这里信息增益即，以 A 属性为节点，熵值下降了多少）</p>
</li>
</ol>
<p>ID3 算法就是在每次需要分裂时，计算每个属性的增益，然后选择增益最大的属性进行分裂。但 ID3 算法倾向于选择多属性，这种属性可能对分类没有太大作用，比如，假设一个属性有 10 种分类，每个分类中都只有一个数据，那么它的每个分类都很纯，信息增益会很大，但这个属性本身对决策没有帮助。C4.5 算法可以避免这个问题。</p>
<h4 id="C4-5- 算法">C4.5 算法</h4>
<p>C4.5 算法是 ID3 算法的优化，引入_信息增益率_的概念，用它替代信息增益作为选择属性的度量依据。</p>
<h5 id="算法步骤 -2">算法步骤</h5>
<ol>
<li>
<p>定义分裂信息</p>
<p><img src="https://math.now.sh?inline=split%5C_Entropy_A%28D%29%20%3D%20-%5Csum_%7Bj%3D1%7D%5E%7Bv%7D(%5Cfrac%7B%7CD_j%7C%7D%7B%7CD%7C%7D%5Clog_2(%7B%5Cfrac%7B%7CD_j%7C%7D%7B%7CD%7C%7D%7D))" style="display:inline-block;margin: 0;"></p>
</li>
<li>
<p>计算属性的信息增益（这一步与 ID3 中一致）</p>
</li>
<li>
<p>增益率为</p>
<p><img src="https://math.now.sh?inline=GainRatio%28A%29%20%3D%20%5Cfrac%7BGain(A)%7D%7Bsplit%5C_Entropy_A(D)%7D" style="display:inline-block;margin: 0;"></p>
</li>
</ol>
<p>C4.5 算法选择增益率最大的属性。</p>
<h4 id="CART- 算法">CART 算法</h4>
<p>区别于前两个算法，CART 算法基于 Gini 系数。</p>
<p>CART 算法计算以一个属性为节点的分支的_评价函数_（类似损失函数，越小越好）：</p>
<p><img src="https://math.now.sh?inline=C%28T%29%20%3D%20%5Csum_%7Bt%20%5Cin%20leaf%7DN_t%20%5Ccdot%20H(t)" style="display:inline-block;margin: 0;"></p>
<p>其中 <img src="https://math.now.sh?inline=N_t" style="display:inline-block;margin: 0;"> 为叶子节点中分类出的数据个数，<img src="https://math.now.sh?inline=H%28t%29" style="display:inline-block;margin: 0;"> 为熵值（这里是 Gini 系数，即 <img src="https://math.now.sh?inline=H%28t%29%3DGini" style="display:inline-block;margin: 0;">）</p>
<h3 id="连续值离散化">连续值离散化</h3>
<p>对于连续值（比如 <img src="https://math.now.sh?inline=%5B0%2C100%5D" style="display:inline-block;margin: 0;">），需要把它划分成离散的范围（比如 <img src="https://math.now.sh?inline=%5B0%2C20%5D%2C%5B21%2C40%5D%2C...%2C%5B81%2C100%5D" style="display:inline-block;margin: 0;">）。可以用 <strong> 贪婪算法 </strong> 选取分界点。</p>
<h3 id="剪枝">剪枝</h3>
<p>决策树_过度拟合_是因为节点过多（即决策树太高，分支太多），所以需要裁剪枝叶。</p>
<h4 id="裁剪策略">裁剪策略</h4>
<ul>
<li>
<p>预剪枝：在决策树构造时进行剪枝。当一个节点包含的样本数小于阈值，则不再划分。</p>
</li>
<li>
<p>后剪枝（常用方案）：生成决策树后再剪枝</p>
<p><img src="https://math.now.sh?inline=C_%5Calpha%28T%29%20%3D%20C(T)%20%2B%20%5Calpha%20%5Ccdot%20%7CT_%7Bleaf%7D%7C" style="display:inline-block;margin: 0;"></p>
<p>其中，<img src="https://math.now.sh?inline=C%28T%29" style="display:inline-block;margin: 0;"> 是评价函数（见 CART 算法），<img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"> 决定了叶子节点个数对最终的评价函数 <img src="https://math.now.sh?inline=C_%5Calpha%28T%29" style="display:inline-block;margin: 0;"> 的影响大小。叶子节点个数越多，损失越大。</p>
<p>当一个分支需要被裁剪时，或者用单一叶子节点代替整个子树，叶节点的分类为子树中的主要分类；或者用一个子树替代另一个子树。</p>
<p>但是后剪枝有一个主要问题，就是计算效率比较低（毕竟被裁剪的树枝也都被计算了一遍）。</p>
</li>
</ul>
<h2 id="随机森林">随机森林</h2>
<p>随机森林有两个步骤：</p>
<ol>
<li>Bootstraping 有放回采样（随机样本，随机特征）</li>
<li>Bagging 有放回采样 <img src="https://math.now.sh?inline=n" style="display:inline-block;margin: 0;"> 个样本共同建立分类器</li>
</ol>
<p>即，随机森林需要构造多颗决策树，将测试样本放入这些决策树得到结果，如果是分类，则取众数；如果是回归，则取平均值。其中需要选择随机的样本和特征构造决策树（避免干扰样本每次都被使用）。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>decision tree</tag>
        <tag>random forest</tag>
      </tags>
  </entry>
  <entry>
    <title>ISO 27000</title>
    <url>/2020/10/21/ISO-27000/</url>
    <content><![CDATA[<p> 最近打算系统地归纳一下 ISO 27000 中信息安全管理制度（ISMS）系列标准（以下简称 ISMS 系列标准）。这个系列是欧洲的信息安全管理标准，其中 27000 是这个系列的综述（或者前导），里面包含了对系列中各个标准的介绍和对信息安全管理（ISM）中术语的定义；27001 （还有 27006，27009）是这个系列的主要标准，即 requirement standards，可以理解为必须要遵循的标准；27002 ~ 27005 等标准属于 guideline standards，即对一些安全管理方面流程的指导准则；其他就是一些针对特定方面的指导标准。我会主要关注 ISO 27001，27002 和 27005，其他的标准如果以后涉及会再做补充。</p>
<a id="more"></a>
<p><img src="/2020/10/21/ISO-27000/image-20201021191152112.png" alt="image-20201021191152112"></p>
<p> 作为 ISMS 系列的综述，我认为 ISO 27000 是需要首先阅读的。ISO 27000 虽然只是一个总体概述，但通过它我们能对整个 ISMS 系列标准有一个全局的认知，对于我们了解整个安全管理体系、建立安全管理体系框架有较好的帮助。</p>
<p>ISO 27000 的名称很清晰地总结了它的内容：Information technology - Security techniques - Information security management systems - Overview and vocabulary，信息技术、安全技术、信息安全管理系统的概述和词汇，即 ISO/IEC 27000 的目的是对 ISMS 做一个概述，并定义其相关术语。其中 “ <em>Information technology - Security techniques</em> ” 是大部分 ISMS 标准共同的标题，之后的介绍中会省略它。</p>
<p> 根据目录，ISO 27000 主要可以分为 5 个部分：总体的介绍（Introduction，包括对 27000 这个标准和对整个 ISMS 系列的介绍），范围（Scope，ISMS 系列标准的适用范围），术语定义（Defining Terms），对信息安全管理制度（ISMS）的介绍，和对 ISMS 系列中各个标准的介绍。</p>
<p> 我认为第一章介绍是需要在学习其他标准之前阅读的，其他比如术语定义可以在之后学习中进行参照，并不需要现在就完全理解和掌握。</p>
<p> 第一章阐述了整个 ISMS 系列标准的用途，即组织机构可以根据这些标准构建和实施一个维护组织的信息资产安全的框架，或者用这些标准检查他们现有的安全管理体系。这里组织中的信息资产包括其经济资产、知识产权、雇工信息，或由第三方机构（或个人）委托给该组织的信息。</p>
<p> 我们需要知道，ISMS 系列标准 </p>
<ul>
<li> 定义了一个安全管理制度和验证一个已有的安全管理制度的需求；</li>
<li> 为建立、实施、维护和改善一个安全管理制度的过程提供了详细的指导和解释；</li>
<li> 针对具体部门提供指导方针；</li>
<li> 为安全管理制度提供合格评定标准。</li>
</ul>
<p> 它旨在帮助_各种规模和类型_的组织建立实施安全管理制度（这表明其适用性非常广泛，任何机构和组织都可以使用）。</p>
<p>ISO 27000 中对 ISMS 系列中的术语进行了定义，但第一章的介绍中也提到，这里定义的术语并不代表全部，系列中其他标准也会定义新的术语。</p>
<p> 之后在 ISO 27000 中对  ISMS 和其他标准的介绍会在学习其他标准时看到，这里就不重复了。</p>
<p> 除正文以外，27000 的附录部分有一些需要注意的内容。</p>
<p> 附录 A 中对于 ISMS 系列标准中条例的表述（shall，should，may，can 之类）的规定：</p>
<ul>
<li><em>shall</em> 和 <em>shall not</em> 表示需要严格执行；</li>
<li><em>should</em> 和 <em>should not</em> 表示推荐这样做，但并不强求；</li>
<li><em>may</em> 和 <em>need not</em> 表示允许这样做；</li>
<li><em>can</em> 和 <em>cannot</em> 表示可能发生。</li>
</ul>
]]></content>
      <categories>
        <category>Information Security Management</category>
      </categories>
      <tags>
        <tag>ISM</tag>
        <tag>ISO 27000 family</tag>
      </tags>
  </entry>
  <entry>
    <title>半边数据结构与网格细分算法 Loop subdivision（附代码）</title>
    <url>/2020/12/19/Loop-subdivision/</url>
    <content><![CDATA[<p>网格细分的原理其实并不难理解，它的难点主要在于如何实现。在看过无数有原理无代码的博客后，终于决定写一写我的实现方法，并附上代码供大家参考。c++ 写的可能比较笨拙，望见谅。</p>
<a id="more"></a>
<h3 id="1- 半边数据结构">1. 半边数据结构</h3>
<p>很好理解，就是把网格的每一条边分成两个半边，半边是有方向的同一条边的两个半边方向相反。并且一条边是属于两个面，则半边完全属于一个面。</p>
<p>综合上述，得到半边的数据结构为一条半边的起始顶点 origin，这条半边指向的下一条半边 next，这条半边对应的另外半边 opposite，和这条半边所在的面 IncFace。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">HalfEdge</span>// 半边结构</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> origin;</span><br><span class="line"></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">HalfEdge</span>* <span class="title">next</span>;</span></span><br><span class="line"></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">HalfEdge</span>* <span class="title">opposite</span>;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> IncFace;</span><br><span class="line"></span><br><span class="line">&#125;HalfEdge;</span><br></pre></td></tr></table></figure>
<h3 id="2- 网格细分算法 -Loop-subdivision">2. 网格细分算法 Loop subdivision</h3>
<p>我只写了 <strong> 三角形网格的细分</strong>，即实现了 Loop subdivision 算法。</p>
<p>首先，我们应该清楚这个算法是怎样细分的：</p>
<p><img src="/2020/12/19/Loop-subdivision/20181103145823583.png" alt="img"></p>
<p>图中上面两种情况适用于 <strong> 内部点和内部边产生新点 </strong> 的更新，下面两种情况适用于 <strong> 边界点和边界产生新点 </strong> 的更新。</p>
<p>由于我只细分了闭合的三维图形，所以代码只实现了第一种情况，就不讨论第二种了。</p>
<p><strong>细分过程中，</strong></p>
<ul>
<li>每一条边都会产生一个新的顶点，这个顶点由该边所在的两个面中所有顶点决定，即 <img src="https://math.now.sh?inline=v%20%3D%20%5Cfrac38%20%5Ctimes%20%28v_1%20%2B%20v_3%29%20%2B%20%5Cfrac18%20%5Ctimes%20(v_0%20%2B%20v_2)" style="display:inline-block;margin: 0;">；</li>
<li>每一个原来的顶点都会更新自己的坐标，这个更新由所有与它相邻的原顶点决定，即<img src="/2020/12/19/Loop-subdivision/98bad6c0736843a1b577ac7f39464294.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"> ，其中<img src="/2020/12/19/Loop-subdivision/a8b68da52ca54620891813c71d1f0364.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"> 。</li>
</ul>
<p>所以，<strong>每次细分要做的就是处理每一个旧顶点和每一条边，生成新的顶点，再将新的顶点连接成半边和面</strong>。</p>
<h4 id="算法思路">算法思路</h4>
<ol>
<li>
<p><strong>读入图形文件 </strong>，分别定义数据结构保存图形的点，线，面。定义半边数据结构，<strong> 将所有边转换为半边</strong>。然后开始细分。</p>
</li>
<li>
<p><strong>更新旧顶点</strong>：第一步，遍历所有半边，找到一条以该顶点为起始顶点的半边。第二步，通过这条半边找到它的 next 半边，next 半边的 origin 为 v1。再找 next 半边的 next 半边 的 opposite 半边，进入下一个面。第三步，重复第二步直到找到的 origin 点 vi 等于 v1，则已经找到所有 vi，可计算出旧顶点的更新后坐标。</p>
</li>
<li>
<p><strong>在每条旧边上生成新顶点</strong>：遍历每一条半边，通过半边和半边的 opposite 边的 next 与 origin 找到两个面的所有顶点，计算出新的顶点。</p>
</li>
<li>
<p>已经得到了所有新的顶点，连接它们得到新的半边和面。画出新图，细分完成。</p>
</li>
</ol>
<p>大概就是这样了，一些地方不知道是否描述清楚，不清楚的请通过代码自行理解。谢谢~</p>
<p>附：（写于 ubuntu16.04，g++ 编译）</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;GL/glut.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;assert.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PI 3.1415926536</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">Vertex</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">	<span class="keyword">float</span> x,y,z;</span><br><span class="line">&#125;Vertex;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">Face</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">	<span class="keyword">int</span> num;</span><br><span class="line">	<span class="keyword">int</span> order[<span class="number">3</span>];</span><br><span class="line">&#125;Face;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">HalfEdge</span>// 半边结构</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">	<span class="keyword">int</span> origin;</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">HalfEdge</span>* <span class="title">next</span>;</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">HalfEdge</span>* <span class="title">opposite</span>;</span></span><br><span class="line">	<span class="keyword">int</span> IncFace;</span><br><span class="line">&#125;HalfEdge;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">Map</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">	<span class="keyword">int</span> vs,ve,e;</span><br><span class="line">	</span><br><span class="line">&#125;Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">char</span> *filename=<span class="string">&quot;cube.off&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">vector</span>&lt;Vertex&gt; vertex;</span><br><span class="line"><span class="built_in">vector</span>&lt;Face&gt; face;</span><br><span class="line"><span class="built_in">vector</span>&lt;HalfEdge*&gt; edge;</span><br><span class="line"><span class="keyword">int</span> e_num;</span><br><span class="line"><span class="keyword">int</span> n_node,n_face,n_edge;</span><br><span class="line"><span class="keyword">int</span> width=<span class="number">800</span>; </span><br><span class="line"><span class="keyword">int</span> height=<span class="number">800</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">readoff</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* filename)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	FILE *fp;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span>(!(fp=fopen(filename,<span class="string">&quot;r&quot;</span>)))</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="built_in">fprintf</span>(<span class="built_in">stderr</span>,<span class="string">&quot;Open fail&quot;</span>);</span><br><span class="line">		<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">    	<span class="keyword">char</span> buffer[<span class="number">1024</span>];</span><br><span class="line">	<span class="keyword">if</span>(fgets(buffer,<span class="number">1023</span>,fp))</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">if</span>(!<span class="built_in">strstr</span>(buffer,<span class="string">&quot;OFF&quot;</span>))</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">&quot;It&#x27;s not a OFF FILE&quot;</span>);</span><br><span class="line">			<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span>(fgets(buffer,<span class="number">1023</span>,fp))</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="built_in">sscanf</span>(buffer,<span class="string">&quot;%d %d %d&quot;</span>,&amp;n_node,&amp;n_face,&amp;n_edge);</span><br><span class="line">			</span><br><span class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n_node;i++)</span><br><span class="line">			&#123;</span><br><span class="line">				Vertex ver;</span><br><span class="line">				fgets(buffer,<span class="number">1023</span>,fp);</span><br><span class="line">				<span class="built_in">sscanf</span>(buffer,<span class="string">&quot;%f%f%f&quot;</span>,&amp;ver.x,&amp;ver.y,&amp;ver.z);</span><br><span class="line">				vertex.push_back(ver);</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n_face;i++)</span><br><span class="line">			&#123;</span><br><span class="line">				fgets(buffer,<span class="number">1023</span>,fp);</span><br><span class="line">				Face f;</span><br><span class="line">				<span class="built_in">sscanf</span>(buffer,<span class="string">&quot;%d%d%d%d&quot;</span>,&amp;f.num,&amp;f.order[<span class="number">0</span>],&amp;f.order[<span class="number">1</span>],&amp;f.order[<span class="number">2</span>]);</span><br><span class="line">				face.push_back(f);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">    	&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">for(int i=0;i&lt;n_node;i++)</span></span><br><span class="line"><span class="comment">&#123;</span></span><br><span class="line"><span class="comment">	cout&lt;&lt;vertex[i].x&lt;&lt;&quot; &quot;&lt;&lt;vertex[i].y&lt;&lt;&quot; &quot;&lt;&lt;vertex[i].z&lt;&lt;endl;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initEdge</span><span class="params">()</span><span class="comment">// 生成半边存入 vector</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> <span class="built_in">map</span>[n_node][n_node]=&#123;<span class="number">0</span>&#125;;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n_node;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;n_node;j++)</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="built_in">map</span>[i][j]=<span class="number">-1</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;map=&quot;</span>&lt;&lt;<span class="keyword">sizeof</span>(<span class="built_in">map</span>[<span class="number">0</span>])/<span class="keyword">sizeof</span>(<span class="keyword">int</span>)&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">	e_num=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n_face;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		HalfEdge* edge1=<span class="keyword">new</span> HalfEdge();</span><br><span class="line">		HalfEdge* edge2=<span class="keyword">new</span> HalfEdge();</span><br><span class="line">		HalfEdge* edge3=<span class="keyword">new</span> HalfEdge();</span><br><span class="line"></span><br><span class="line">		edge1-&gt;origin=face[i].order[<span class="number">0</span>];</span><br><span class="line">		edge2-&gt;origin=face[i].order[<span class="number">1</span>];</span><br><span class="line">		edge3-&gt;origin=face[i].order[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">		edge1-&gt;next=edge2;</span><br><span class="line">		edge2-&gt;next=edge3;</span><br><span class="line">		edge3-&gt;next=edge1;</span><br><span class="line"></span><br><span class="line">		HalfEdge* tmpe=<span class="keyword">new</span> HalfEdge();</span><br><span class="line">		<span class="keyword">if</span>(<span class="built_in">map</span>[face[i].order[<span class="number">1</span>]][face[i].order[<span class="number">0</span>]]!=<span class="number">-1</span>)</span><br><span class="line">		&#123;</span><br><span class="line">			tmpe=edge[<span class="built_in">map</span>[face[i].order[<span class="number">1</span>]][face[i].order[<span class="number">0</span>]]];</span><br><span class="line">			edge1-&gt;opposite=tmpe;</span><br><span class="line">			tmpe-&gt;opposite=edge1;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">else</span></span><br><span class="line">		&#123;</span><br><span class="line">			edge1-&gt;opposite=<span class="literal">NULL</span>;</span><br><span class="line">			<span class="built_in">map</span>[face[i].order[<span class="number">0</span>]][face[i].order[<span class="number">1</span>]]=e_num;</span><br><span class="line">		&#125;</span><br><span class="line">		e_num++;</span><br><span class="line">		<span class="keyword">if</span>(<span class="built_in">map</span>[face[i].order[<span class="number">2</span>]][face[i].order[<span class="number">1</span>]]!=<span class="number">-1</span>)</span><br><span class="line">		&#123;</span><br><span class="line">			tmpe=edge[<span class="built_in">map</span>[face[i].order[<span class="number">2</span>]][face[i].order[<span class="number">1</span>]]];</span><br><span class="line">			edge2-&gt;opposite=tmpe;</span><br><span class="line">			tmpe-&gt;opposite=edge2;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">else</span></span><br><span class="line">		&#123;</span><br><span class="line">			edge2-&gt;opposite=<span class="literal">NULL</span>;</span><br><span class="line">			<span class="built_in">map</span>[face[i].order[<span class="number">1</span>]][face[i].order[<span class="number">2</span>]]=e_num;</span><br><span class="line">		&#125;</span><br><span class="line">		e_num++;</span><br><span class="line">		<span class="keyword">if</span>(<span class="built_in">map</span>[face[i].order[<span class="number">0</span>]][face[i].order[<span class="number">2</span>]]!=<span class="number">-1</span>)</span><br><span class="line">		&#123;</span><br><span class="line">			tmpe=edge[<span class="built_in">map</span>[face[i].order[<span class="number">0</span>]][face[i].order[<span class="number">2</span>]]];</span><br><span class="line">			edge3-&gt;opposite=tmpe;</span><br><span class="line">			tmpe-&gt;opposite=edge3;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">else</span></span><br><span class="line">		&#123;</span><br><span class="line">			edge3-&gt;opposite=<span class="literal">NULL</span>;</span><br><span class="line">			<span class="built_in">map</span>[face[i].order[<span class="number">2</span>]][face[i].order[<span class="number">0</span>]]=e_num;</span><br><span class="line">		&#125;</span><br><span class="line">		e_num++;</span><br><span class="line"></span><br><span class="line">		edge1-&gt;IncFace=i;</span><br><span class="line">		edge2-&gt;IncFace=i;</span><br><span class="line">		edge3-&gt;IncFace=i;</span><br><span class="line"></span><br><span class="line">		edge.push_back(edge1);</span><br><span class="line">		edge.push_back(edge2);</span><br><span class="line">		edge.push_back(edge3);</span><br><span class="line">	&#125;</span><br><span class="line">	n_edge=edge.size();</span><br><span class="line"></span><br><span class="line"><span class="comment">//cout&lt;&lt;n_edge&lt;&lt;endl;</span></span><br><span class="line">	<span class="comment">//for(int i=0;i&lt;n_edge;i++)</span></span><br><span class="line">		<span class="comment">//cout&lt;&lt;edge[i].origin&lt;&lt;&quot; &quot;&lt;&lt;edge[i].next-&gt;origin&lt;&lt;&quot; &quot;&lt;&lt;edge[i].next-&gt;next-&gt;origin&lt;&lt;&quot; &quot;&lt;&lt;edge[i].IncFace&lt;&lt;endl;</span></span><br><span class="line">		<span class="comment">//cout&lt;&lt;edge[i]-&gt;origin&lt;&lt;&quot; &quot;&lt;&lt;edge[i]-&gt;opposite-&gt;origin&lt;&lt;endl;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">HalfEdge* <span class="title">findOriginEdge</span><span class="params">(<span class="keyword">int</span> v)</span><span class="comment">// 找到从该定点出发的一条半边</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">0</span>;k&lt;n_edge;k++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">if</span>(edge[k]-&gt;origin==v)</span><br><span class="line">			<span class="keyword">return</span> edge[k];</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">subdivide</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="built_in">vector</span>&lt;Vertex&gt; vertex2;</span><br><span class="line">	<span class="built_in">vector</span>&lt;Face&gt; face2;</span><br><span class="line">	<span class="built_in">vector</span>&lt;HalfEdge*&gt; edge2;</span><br><span class="line">	HalfEdge* he=<span class="keyword">new</span> HalfEdge();</span><br><span class="line">	<span class="keyword">int</span> n;</span><br><span class="line">	<span class="keyword">float</span> p_sumx,p_sumy,p_sumz;</span><br><span class="line">	<span class="keyword">float</span> px,py,pz;</span><br><span class="line">	<span class="keyword">float</span> beta;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">&quot; 细分开始 &quot;</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n_node;i++)<span class="comment">// 旧点更新</span></span><br><span class="line">	&#123;</span><br><span class="line">		he=findOriginEdge(i);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span>(he!=<span class="literal">NULL</span>)</span><br><span class="line">		&#123;</span><br><span class="line">			n=<span class="number">0</span>;</span><br><span class="line">			p_sumx=<span class="number">0</span>;</span><br><span class="line">			p_sumy=<span class="number">0</span>;</span><br><span class="line">			p_sumz=<span class="number">0</span>;</span><br><span class="line">			HalfEdge* e=<span class="keyword">new</span> HalfEdge();</span><br><span class="line">			e=he-&gt;next;</span><br><span class="line">			<span class="keyword">int</span> p0=e-&gt;origin;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">while</span>(e-&gt;next-&gt;origin!=p0)</span><br><span class="line">			&#123;</span><br><span class="line">				n++;</span><br><span class="line">				p_sumx+=vertex[e-&gt;next-&gt;origin].x;</span><br><span class="line">				p_sumy+=vertex[e-&gt;next-&gt;origin].y;</span><br><span class="line">				p_sumz+=vertex[e-&gt;next-&gt;origin].z;</span><br><span class="line">				HalfEdge* te=<span class="keyword">new</span> HalfEdge();</span><br><span class="line">				te=e-&gt;next-&gt;opposite;</span><br><span class="line">				e=te-&gt;next;</span><br><span class="line">			&#125;</span><br><span class="line">			n++;</span><br><span class="line">			p_sumx+=vertex[p0].x;</span><br><span class="line">			p_sumy+=vertex[p0].y;</span><br><span class="line">			p_sumz+=vertex[p0].z;</span><br><span class="line">			beta=<span class="number">1</span>/(<span class="keyword">double</span>)n*(<span class="number">0.625</span>-<span class="built_in">pow</span>(<span class="number">0.375</span>+<span class="number">0.25</span>*<span class="built_in">cos</span>(<span class="number">2</span>*PI/n),<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line">			px=(<span class="number">1</span>-n*beta)*vertex[i].x+beta*p_sumx;</span><br><span class="line">			py=(<span class="number">1</span>-n*beta)*vertex[i].y+beta*p_sumy;</span><br><span class="line">			pz=(<span class="number">1</span>-n*beta)*vertex[i].z+beta*p_sumz;</span><br><span class="line"></span><br><span class="line">			Vertex v;</span><br><span class="line">			v.x=px;</span><br><span class="line">			v.y=py;</span><br><span class="line">			v.z=pz;</span><br><span class="line">			vertex2.push_back(v);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">int</span> map1[n_node][n_node]=&#123;<span class="number">0</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;map1=&quot;</span>&lt;&lt;<span class="keyword">sizeof</span>(map1[<span class="number">0</span>])/<span class="keyword">sizeof</span>(<span class="keyword">int</span>)&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">	<span class="keyword">float</span> qx,qy,qz;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n_edge;i++)<span class="comment">// 新点生成</span></span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">if</span>(!map1[edge[i]-&gt;origin][edge[i]-&gt;next-&gt;origin])</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="keyword">int</span> p=edge[i]-&gt;origin;</span><br><span class="line">			<span class="keyword">int</span> pi=edge[i]-&gt;next-&gt;origin;</span><br><span class="line">			<span class="keyword">int</span> pi1=edge[i]-&gt;next-&gt;next-&gt;origin;</span><br><span class="line">			<span class="keyword">int</span> pi0=edge[i]-&gt;opposite-&gt;next-&gt;next-&gt;origin;</span><br><span class="line">			qx=<span class="number">0.375</span>*(vertex[p].x+vertex[pi].x)+<span class="number">0.125</span>*(vertex[pi1].x+vertex[pi0].x);</span><br><span class="line">			qy=<span class="number">0.375</span>*(vertex[p].y+vertex[pi].y)+<span class="number">0.125</span>*(vertex[pi1].y+vertex[pi0].y);</span><br><span class="line">			qz=<span class="number">0.375</span>*(vertex[p].z+vertex[pi].z)+<span class="number">0.125</span>*(vertex[pi1].z+vertex[pi0].z);</span><br><span class="line"></span><br><span class="line">			Vertex v;</span><br><span class="line">			v.x=qx;</span><br><span class="line">			v.y=qy;</span><br><span class="line">			v.z=qz;</span><br><span class="line">			vertex2.push_back(v);</span><br><span class="line"></span><br><span class="line">			map1[edge[i]-&gt;origin][edge[i]-&gt;next-&gt;origin]=vertex2.size()<span class="number">-1</span>;</span><br><span class="line">			map1[edge[i]-&gt;next-&gt;origin][edge[i]-&gt;origin]=vertex2.size()<span class="number">-1</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">cout&lt;&lt;&quot; 新点 &quot;&lt;&lt;endl;</span></span><br><span class="line"><span class="comment">for(int i=0;i&lt;vertex2.size();i++)</span></span><br><span class="line"><span class="comment">&#123;</span></span><br><span class="line"><span class="comment">	cout&lt;&lt;vertex2[i].x&lt;&lt;&quot; &quot;&lt;&lt;vertex2[i].y&lt;&lt;&quot; &quot;&lt;&lt;vertex2[i].z&lt;&lt;endl;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n_face;i++)<span class="comment">// 新面</span></span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">int</span> a,b,c,d,e,f;</span><br><span class="line">		a=face[i].order[<span class="number">0</span>];</span><br><span class="line">		b=face[i].order[<span class="number">1</span>];</span><br><span class="line">		c=face[i].order[<span class="number">2</span>];</span><br><span class="line">		d=map1[a][b];</span><br><span class="line">		e=map1[b][c];</span><br><span class="line">		f=map1[a][c];</span><br><span class="line"></span><br><span class="line">		Face f2;</span><br><span class="line">		f2.num=<span class="number">3</span>;</span><br><span class="line"></span><br><span class="line">		f2.order[<span class="number">0</span>]=a;</span><br><span class="line">		f2.order[<span class="number">1</span>]=d;</span><br><span class="line">		f2.order[<span class="number">2</span>]=f;</span><br><span class="line">		face2.push_back(f2);</span><br><span class="line"></span><br><span class="line">		f2.order[<span class="number">0</span>]=d;</span><br><span class="line">		f2.order[<span class="number">1</span>]=b;</span><br><span class="line">		f2.order[<span class="number">2</span>]=e;</span><br><span class="line">		face2.push_back(f2);</span><br><span class="line"></span><br><span class="line">		f2.order[<span class="number">0</span>]=d;</span><br><span class="line">		f2.order[<span class="number">1</span>]=e;</span><br><span class="line">		f2.order[<span class="number">2</span>]=f;</span><br><span class="line">		face2.push_back(f2);</span><br><span class="line"></span><br><span class="line">		f2.order[<span class="number">0</span>]=f;</span><br><span class="line">		f2.order[<span class="number">1</span>]=e;</span><br><span class="line">		f2.order[<span class="number">2</span>]=c;</span><br><span class="line">		face2.push_back(f2);</span><br><span class="line">	&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">cout&lt;&lt;&quot; 新面 &quot;&lt;&lt;endl;</span></span><br><span class="line"><span class="comment">for(int i=0;i&lt;face2.size();i++)</span></span><br><span class="line"><span class="comment">&#123;</span></span><br><span class="line"><span class="comment">	cout&lt;&lt;face2[i].order[0]&lt;&lt;&quot; &quot;&lt;&lt;face2[i].order[1]&lt;&lt;&quot; &quot;&lt;&lt;face2[i].order[2]&lt;&lt;endl;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line">	n_face=face2.size();</span><br><span class="line">	n_node=vertex2.size();</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;n_node&lt;&lt;<span class="string">&quot; &quot;</span>&lt;&lt;n_face&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span> map2[n_node][n_node]=&#123;<span class="number">0</span>&#125;;</span><br><span class="line">	<span class="comment">//int * map2=new int[n_node][n_node];</span></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n_node;i++)</span><br><span class="line">	&#123;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;n_node;j++)</span><br><span class="line">		&#123;</span><br><span class="line">			map2[i][j]=<span class="number">-1</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;map2=&quot;</span>&lt;&lt;<span class="keyword">sizeof</span>(map2[<span class="number">0</span>])/<span class="keyword">sizeof</span>(<span class="keyword">int</span>)&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">	</span><br><span class="line">	e_num=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n_face;i++)<span class="comment">// 新边</span></span><br><span class="line">	&#123;</span><br><span class="line">		HalfEdge* edge4=<span class="keyword">new</span> HalfEdge();</span><br><span class="line">		HalfEdge* edge5=<span class="keyword">new</span> HalfEdge();</span><br><span class="line">		HalfEdge* edge6=<span class="keyword">new</span> HalfEdge();</span><br><span class="line"></span><br><span class="line">		edge4-&gt;origin=face2[i].order[<span class="number">0</span>];</span><br><span class="line">		edge5-&gt;origin=face2[i].order[<span class="number">1</span>];</span><br><span class="line">		edge6-&gt;origin=face2[i].order[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">		edge4-&gt;next=edge5;</span><br><span class="line">		edge5-&gt;next=edge6;</span><br><span class="line">		edge6-&gt;next=edge4;</span><br><span class="line"></span><br><span class="line">		HalfEdge* tmpe=<span class="keyword">new</span> HalfEdge();</span><br><span class="line">		<span class="keyword">if</span>(map2[face2[i].order[<span class="number">1</span>]][face2[i].order[<span class="number">0</span>]]!=<span class="number">-1</span>)</span><br><span class="line">		&#123;</span><br><span class="line">			tmpe=edge2[map2[face2[i].order[<span class="number">1</span>]][face2[i].order[<span class="number">0</span>]]];</span><br><span class="line">			edge4-&gt;opposite=tmpe;</span><br><span class="line">			tmpe-&gt;opposite=edge4;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">else</span></span><br><span class="line">		&#123;</span><br><span class="line">			edge4-&gt;opposite=<span class="literal">NULL</span>;</span><br><span class="line">			map2[face2[i].order[<span class="number">0</span>]][face2[i].order[<span class="number">1</span>]]=e_num;</span><br><span class="line">		&#125;</span><br><span class="line">		e_num++;</span><br><span class="line">		<span class="keyword">if</span>(map2[face2[i].order[<span class="number">2</span>]][face2[i].order[<span class="number">1</span>]]!=<span class="number">-1</span>)</span><br><span class="line">		&#123;</span><br><span class="line">			tmpe=edge2[map2[face2[i].order[<span class="number">2</span>]][face2[i].order[<span class="number">1</span>]]];</span><br><span class="line">			edge5-&gt;opposite=tmpe;</span><br><span class="line">			tmpe-&gt;opposite=edge5;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">else</span></span><br><span class="line">		&#123;</span><br><span class="line">			edge5-&gt;opposite=<span class="literal">NULL</span>;</span><br><span class="line">			map2[face2[i].order[<span class="number">1</span>]][face2[i].order[<span class="number">2</span>]]=e_num;</span><br><span class="line">		&#125;</span><br><span class="line">		e_num++;</span><br><span class="line">		<span class="keyword">if</span>(map2[face2[i].order[<span class="number">0</span>]][face2[i].order[<span class="number">2</span>]]!=<span class="number">-1</span>)</span><br><span class="line">		&#123;</span><br><span class="line">			tmpe=edge2[map2[face2[i].order[<span class="number">0</span>]][face2[i].order[<span class="number">2</span>]]];</span><br><span class="line">			edge6-&gt;opposite=tmpe;</span><br><span class="line">			tmpe-&gt;opposite=edge6;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">else</span></span><br><span class="line">		&#123;</span><br><span class="line">			edge6-&gt;opposite=<span class="literal">NULL</span>;</span><br><span class="line">			map2[face2[i].order[<span class="number">2</span>]][face2[i].order[<span class="number">0</span>]]=e_num;</span><br><span class="line">		&#125;</span><br><span class="line">		e_num++;</span><br><span class="line"></span><br><span class="line">		edge4-&gt;IncFace=i;</span><br><span class="line">		edge5-&gt;IncFace=i;</span><br><span class="line">		edge6-&gt;IncFace=i;</span><br><span class="line"></span><br><span class="line">		edge2.push_back(edge4);</span><br><span class="line">		edge2.push_back(edge5);</span><br><span class="line">		edge2.push_back(edge6);</span><br><span class="line">	&#125;</span><br><span class="line">	n_edge=edge2.size();</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">cout&lt;&lt;&quot; 新边 &quot;&lt;&lt;endl;</span></span><br><span class="line"><span class="comment">for(int i=0;i&lt;edge2.size();i++)</span></span><br><span class="line"><span class="comment">&#123;</span></span><br><span class="line"><span class="comment">	cout&lt;&lt;edge2[i]-&gt;origin&lt;&lt;&quot; &quot;&lt;&lt;edge2[i]-&gt;next-&gt;origin&lt;&lt;&quot; &quot;&lt;&lt;edge2[i]-&gt;IncFace&lt;&lt;endl;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">	vertex.assign(vertex2.begin(),vertex2.end());</span><br><span class="line"></span><br><span class="line">	face.assign(face2.begin(),face2.end());</span><br><span class="line"></span><br><span class="line">	edge.assign(edge2.begin(),edge2.end());</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">&quot; 完成一次细分 &quot;</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;n_node&lt;&lt;<span class="string">&quot; &quot;</span>&lt;&lt;n_edge&lt;&lt;<span class="string">&quot; &quot;</span>&lt;&lt;n_face&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">display</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);<span class="comment">// 清除颜色和深度缓存  </span></span><br><span class="line">	glMatrixMode(GL_MODELVIEW);</span><br><span class="line">    	glLoadIdentity();<span class="comment">// 重置当前的模型观察矩阵</span></span><br><span class="line">	glPushMatrix();</span><br><span class="line">	glTranslatef(<span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">-5.0f</span>);</span><br><span class="line">	</span><br><span class="line">    	glRotatef(<span class="number">30</span>, <span class="number">0.0f</span>, <span class="number">1.0f</span>, <span class="number">0.0f</span>); <span class="comment">// 饶轴旋转</span></span><br><span class="line">	glRotatef(<span class="number">30</span>, <span class="number">1.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>);  </span><br><span class="line">    	glColor3f(<span class="number">0.5f</span>, <span class="number">0.5f</span>, <span class="number">0.5f</span>); <span class="comment">// 灰色</span></span><br><span class="line"></span><br><span class="line">	glBegin(GL_TRIANGLES);</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n_face;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		glVertex3f(vertex[face[i].order[<span class="number">0</span>]].x,vertex[face[i].order[<span class="number">0</span>]].y,vertex[face[i].order[<span class="number">0</span>]].z);</span><br><span class="line">		glVertex3f(vertex[face[i].order[<span class="number">1</span>]].x,vertex[face[i].order[<span class="number">1</span>]].y,vertex[face[i].order[<span class="number">1</span>]].z);</span><br><span class="line">		glVertex3f(vertex[face[i].order[<span class="number">2</span>]].x,vertex[face[i].order[<span class="number">2</span>]].y,vertex[face[i].order[<span class="number">2</span>]].z);</span><br><span class="line">	&#125;</span><br><span class="line">	glEnd();</span><br><span class="line">	glPopMatrix();</span><br><span class="line">	glutSwapBuffers();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">keyboard</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">char</span> key, <span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">switch</span>(key)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">case</span> <span class="string">&#x27;1&#x27;</span>:</span><br><span class="line">			glPolygonMode(GL_FRONT_AND_BACK, GL_FILL);</span><br><span class="line">			<span class="keyword">break</span>;</span><br><span class="line">	  	<span class="keyword">case</span> <span class="string">&#x27;2&#x27;</span>:</span><br><span class="line">			glPolygonMode(GL_FRONT_AND_BACK, GL_LINE);</span><br><span class="line">			<span class="keyword">break</span>;</span><br><span class="line">	  	<span class="keyword">case</span> <span class="string">&#x27;3&#x27;</span>:</span><br><span class="line">			glPolygonMode(GL_FRONT_AND_BACK, GL_POINT);</span><br><span class="line">			<span class="keyword">break</span>;</span><br><span class="line">		<span class="keyword">case</span> <span class="string">&#x27;w&#x27;</span>:</span><br><span class="line">			subdivide();</span><br><span class="line">			<span class="keyword">break</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	glutPostRedisplay();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">reshape</span><span class="params">(<span class="keyword">int</span> w, <span class="keyword">int</span> h)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 定义视口大小</span></span><br><span class="line">    glViewport(<span class="number">0</span>, <span class="number">0</span>, (GLsizei) w, (GLsizei) h); </span><br><span class="line">    <span class="comment">// 投影显示</span></span><br><span class="line">    glMatrixMode(GL_PROJECTION);</span><br><span class="line">    <span class="comment">// 坐标原点在屏幕中心</span></span><br><span class="line">    glLoadIdentity();</span><br><span class="line">    <span class="comment">// 操作模型视景</span></span><br><span class="line">    gluPerspective(<span class="number">60.0</span>, (GLfloat) w/(GLfloat) h, <span class="number">1.0</span>, <span class="number">20.0</span>);</span><br><span class="line">    glMatrixMode(GL_MODELVIEW);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	readoff(filename);</span><br><span class="line">	initEdge();</span><br><span class="line"></span><br><span class="line">	glutInit(&amp;argc,argv);</span><br><span class="line">	glutInitDisplayMode(GLUT_DOUBLE|GLUT_RGB|GLUT_DEPTH);</span><br><span class="line">	glutInitWindowSize(width,height);</span><br><span class="line">	glutInitWindowPosition(<span class="number">100</span>,<span class="number">100</span>);</span><br><span class="line">	glutCreateWindow(<span class="string">&quot;loop&quot;</span>);</span><br><span class="line">	glutReshapeFunc(reshape);</span><br><span class="line">	glutDisplayFunc(display);</span><br><span class="line">	glutIdleFunc(display);</span><br><span class="line">	glutKeyboardFunc(keyboard);</span><br><span class="line">	glutMainLoop();</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>图形文件 cube.off</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">OFF</span><br><span class="line">8 12 0</span><br><span class="line">-0.5 -0.5 -0.5</span><br><span class="line">0.5 -0.5 -0.5</span><br><span class="line">-0.5 0.5 -0.5</span><br><span class="line">0.5 0.5 -0.5</span><br><span class="line">-0.5 -0.5 0.5</span><br><span class="line">0.5 -0.5 0.5</span><br><span class="line">-0.5 0.5 0.5 </span><br><span class="line">0.5 0.5 0.5 </span><br><span class="line">3 4 5 6</span><br><span class="line">3 6 5 7</span><br><span class="line">3 5 1 7</span><br><span class="line">3 7 1 3</span><br><span class="line">3 0 3 1</span><br><span class="line">3 0 2 3</span><br><span class="line">3 4 6 2</span><br><span class="line">3 4 2 0</span><br><span class="line">3 6 7 2</span><br><span class="line">3 3 2 7</span><br><span class="line">3 5 4 0</span><br><span class="line">3 5 0 1</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Graphic</category>
      </categories>
      <tags>
        <tag>graphic</tag>
        <tag>loop subdivision</tag>
      </tags>
  </entry>
  <entry>
    <title>朴素贝叶斯 Naive-Bayes</title>
    <url>/2020/10/30/Naive-Bayes/</url>
    <content><![CDATA[<h6 id="参考文献"> 参考文献 </h6>
<p>[1] 《统计学习方法（第二版）》李航 </p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>bayes</tag>
      </tags>
  </entry>
  <entry>
    <title>感知机 Perceptron</title>
    <url>/2020/10/23/Perceptron/</url>
    <content><![CDATA[<p>感知机是二类分类的线性分类模型，即输出为 <img src="https://math.now.sh?inline=%2B1" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=-1" style="display:inline-block;margin: 0;"> 二值，属于判别模型。感知机学习旨在将训练数据进行线性划分，有简单易实现的优点。感知机算法分为原始形式和对偶形式。</p>
<a id="more"></a>
<p>感知机模型的函数表示为</p>
<p><img src="https://math.now.sh?inline=f%28x%29%3Dsign(w%20%5Ccdot%20x%2Bb)%20%5Ctag%7B1%7D" style="display:inline-block;margin: 0;"></p>
<p>其中 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"> 为输入的实例的特征向量，<img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;"> 为输出的实例的类别。<img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=b" style="display:inline-block;margin: 0;"> 为感知机模型参数，<img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;"> 叫做权值（weight）或权值向量，<img src="https://math.now.sh?inline=b" style="display:inline-block;margin: 0;"> 叫做偏值（bias），<img src="https://math.now.sh?inline=w%20%5Ccdot%20x" style="display:inline-block;margin: 0;"> 表示 <img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"> 的内积。</p>
<p>sign 是符号函数，即</p>
<p><img src="https://math.now.sh?inline=sign%28x%29%3D%5Cleft%5C%7B%5Cbegin%7Baligned%7D%20%2B1%2C%5C%20%26%20x%20%5Cgeq%200%20%5C%5C%20%20-1%2C%5C%20%26%20x%20%3C0%20%5Cend%7Baligned%7D%20%5Cright.%20%5Ctag%7B2%7D" style="display:inline-block;margin: 0;"></p>
<p>感知机模型的假设空间是定义在特征空间的所有线性分类模型，即函数集合 <img src="https://math.now.sh?inline=%5C%7Bf%7Cf%28x%29%3Dw%20%5Ccdot%20x%2Bb%5C%7D" style="display:inline-block;margin: 0;">。</p>
<p>（假设空间是指所有可能的模型的集合）</p>
<p>感知机可以理解为：所有训练样本在一个特征空间中，<img src="https://math.now.sh?inline=w%20%5Ccdot%20x%2Bb" style="display:inline-block;margin: 0;"> 是特征空间中的一个超平面 <img src="https://math.now.sh?inline=S" style="display:inline-block;margin: 0;">，它将特征空间划分为两个部分（正、负两类）。这个超平面 <img src="https://math.now.sh?inline=S" style="display:inline-block;margin: 0;"> 称为分离超平面（separating hyperplane）。</p>
<p><img src="/2020/10/23/Perceptron/image-20201023030026330.png" alt="image-20201023030026330"></p>
<p>假设训练数据集是线性可分的（即可以找到超平面 <img src="https://math.now.sh?inline=S" style="display:inline-block;margin: 0;"> 将正负实例点完全分开），为了找到超平面 <img src="https://math.now.sh?inline=S" style="display:inline-block;margin: 0;">，我们可以定义一个（经验）损失函数并将损失函数极小化。（经验损失函数表示由所有训练样本的误差计算得到的损失函数）</p>
<p>感知机采用的损失函数是误分类点到超平面 <img src="https://math.now.sh?inline=S" style="display:inline-block;margin: 0;"> 的总距离。首先，特征空间中任意一点 <img src="https://math.now.sh?inline=x_0" style="display:inline-block;margin: 0;"> 到超平面 <img src="https://math.now.sh?inline=S" style="display:inline-block;margin: 0;"> 的距离为：</p>
<p><img src="https://math.now.sh?inline=%5Cfrac1%7B%7C%7Cw%7C%7C%7D%7Cw%20%5Ccdot%20x_0%3Db%7C" style="display:inline-block;margin: 0;"></p>
<p>其中，<img src="https://math.now.sh?inline=%7C%7Cw%7C%7C" style="display:inline-block;margin: 0;"> 是 <img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;"> 的 <img src="https://math.now.sh?inline=L_2" style="display:inline-block;margin: 0;"> 范式。</p>
<p>其次，对误分类的数据 <img src="https://math.now.sh?inline=%28x_i%2Cy_i%29" style="display:inline-block;margin: 0;">，<img src="https://math.now.sh?inline=-y_i%28w%20%5Ccdot%20x_i%2Bb%29%3E0" style="display:inline-block;margin: 0;"> 成立。</p>
<p>因此，误差分类点 <img src="https://math.now.sh?inline=x_i" style="display:inline-block;margin: 0;"> 到超平面 <img src="https://math.now.sh?inline=S" style="display:inline-block;margin: 0;"> 的距离是</p>
<p><img src="https://math.now.sh?inline=-%5Cfrac1%7B%7C%7Cw%7C%7C%7D%20y_i%28w%20%5Ccdot%20x_i%2Bb%29" style="display:inline-block;margin: 0;"></p>
<p>于是，假设超平面 <img src="https://math.now.sh?inline=S" style="display:inline-block;margin: 0;"> 的误分类点集合为 <img src="https://math.now.sh?inline=M" style="display:inline-block;margin: 0;">，那么所有误分类点到超平面 <img src="https://math.now.sh?inline=S" style="display:inline-block;margin: 0;"> 的总距离为</p>
<p><img src="https://math.now.sh?inline=-%5Cfrac1%7B%7C%7Cw%7C%7C%7D%20%5Cdisplaystyle%20%5Csum_%7Bx_i%20%5Cin%20M%7D%20y_i%28w%20%5Ccdot%20x_i%2Bb%29" style="display:inline-block;margin: 0;"></p>
<p>不考虑 <img src="https://math.now.sh?inline=%5Cfrac1%7B%7C%7Cw%7C%7C%7D" style="display:inline-block;margin: 0;">，就得到感知机学习的 <strong> 损失函数</strong>。</p>
<p>所以，感知机学习的损失函数为</p>
<p><img src="https://math.now.sh?inline=L%28w%2Cb%29%3D-%5Cdisplaystyle%20%5Csum_%7Bx_i%20%5Cin%20M%7D%20y_i(w%20%5Ccdot%20x_i%2Bb)%20%5Ctag%7B3%7D" style="display:inline-block;margin: 0;"></p>
<p>显然，损失函数 <img src="https://math.now.sh?inline=L%28w%2Cb%29%20%5Cgeq%200" style="display:inline-block;margin: 0;">。如果没有误分类点，损失函数为 0。误分类点越少，损失函数越小。于是我们需要选取使损失函数最小的模型参数 <img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=b" style="display:inline-block;margin: 0;">。</p>
<p>接下来是之前提到的两种感知机学习的算法：原始形式和对偶形式。</p>
<p><strong>原始形式</strong></p>
<p>已知感知机学习是找到使损失函数极小化的模型参数，即</p>
<p><img src="https://math.now.sh?inline=%5Cdisplaystyle%20%5Cmin_%7Bw%2Cb%7D%20L%28w%2Cb%29%3D-%5Csum_%7Bx_i%20%5Cin%20M%7D%20y_i(w%20%5Ccdot%20x_i%2Bb)%20%5Ctag%7B4%7D" style="display:inline-block;margin: 0;"></p>
<p>所以感知机学习算法是由误分类驱动的，具体采用 <strong> 随机梯度下降法</strong>：首先，任意选取一个超平面（即任意选取参数 <img src="https://math.now.sh?inline=w_0" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=b_0" style="display:inline-block;margin: 0;">），然后用梯度下降法不断地极小化目标函数（即损失函数）。在极小化过程中，每次随机选取一个误分类点使其梯度下降。</p>
<p>损失函数的梯度为</p>
<p><img src="https://math.now.sh?inline=%5Cnabla_wL%28w%2Cb%29%3D%5Cdisplaystyle%20-%5Csum_%7Bx_i%20%5Cin%20M%7Dy_ix_i" style="display:inline-block;margin: 0;"></p>
<p><img src="https://math.now.sh?inline=%5Cnabla_bL%28w%2Cb%29%3D%5Cdisplaystyle%20-%5Csum_%7Bx_i%20%5Cin%20M%7Dy_i" style="display:inline-block;margin: 0;"></p>
<p>随机选取一个误分类点 <img src="https://math.now.sh?inline=%28x_i%2Cy_i%29" style="display:inline-block;margin: 0;">，对 <img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=b" style="display:inline-block;margin: 0;"> 进行更新：</p>
<p><img src="https://math.now.sh?inline=w%3A%3Dw%2B%5Calpha%20y_ix_i%20%5Ctag%7B5%7D" style="display:inline-block;margin: 0;"></p>
<p><img src="https://math.now.sh?inline=b%3A%3Db%2B%5Calpha%20y_i%20%5Ctag%7B6%7D" style="display:inline-block;margin: 0;"></p>
<p>其中 <img src="https://math.now.sh?inline=%5Calpha%20%5C%20%280%3C%5Calpha%20%5Cleq%201%29" style="display:inline-block;margin: 0;"> 是步长，又称为 <strong> 学习率</strong>（learning rate）。</p>
<p>于是，感知机学习算法的 <strong> 原始形式 </strong> 为：</p>
<ul>
<li>Input：训练数据集 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;">，学习率 <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;">；</li>
<li>Output：<img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;">，<img src="https://math.now.sh?inline=b" style="display:inline-block;margin: 0;">，感知机模型 <img src="https://math.now.sh?inline=f%28x%29%3Dsign(w%20%5Ccdot%20x%2Bb)" style="display:inline-block;margin: 0;">。</li>
</ul>
<ol>
<li>
<p>选取随机初始参数 <img src="https://math.now.sh?inline=w_0" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=b_0" style="display:inline-block;margin: 0;">；</p>
</li>
<li>
<p>选取训练集中样本 <img src="https://math.now.sh?inline=%28x_i%2Cy_i%29" style="display:inline-block;margin: 0;">；</p>
</li>
<li>
<p>如果 <img src="https://math.now.sh?inline=y_i%28w%20%5Ccdot%20x_i%2Bb%29%20%5Cleq%200" style="display:inline-block;margin: 0;">，</p>
<p><img src="https://math.now.sh?inline=w%3A%3Dw%2B%5Calpha%20y_ix_i" style="display:inline-block;margin: 0;"></p>
<p><img src="https://math.now.sh?inline=b%3A%3Db%2B%5Calpha%20y_i" style="display:inline-block;margin: 0;"></p>
</li>
<li>
<p>重复 2、3 步，直到训练集中没有误分类点。</p>
</li>
</ol>
<p>选择不同的初始参数，或者以不同的顺序选取误分类点，会得到不同的解。</p>
<p>根据定理 <strong>Novikoff</strong> 可知，误分类的次数是有上限的，所以经过有限次搜索可以找到将训练集完全正确分开的超平面。但为了得到唯一的超平面，需要对超平面增加约束条件，这就是线性支持向量机的想法。</p>
<p><strong>对偶形式</strong></p>
<p>对偶形式的基本想法是，将 <img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=b" style="display:inline-block;margin: 0;"> 表示为实例 <img src="https://math.now.sh?inline=x_i" style="display:inline-block;margin: 0;"> 和标记 <img src="https://math.now.sh?inline=y_i" style="display:inline-block;margin: 0;"> 的线性组合的形式，通过求解其系数从而求得 <img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=b" style="display:inline-block;margin: 0;">。</p>
<p>还是取初始值 <img src="https://math.now.sh?inline=w_0" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=b_0" style="display:inline-block;margin: 0;">，误分类点通过</p>
<p><img src="https://math.now.sh?inline=w%3A%3Dw%2B%5Calpha%20y_ix_i" style="display:inline-block;margin: 0;"></p>
<p><img src="https://math.now.sh?inline=b%3A%3Db%2B%5Calpha%20y_i" style="display:inline-block;margin: 0;"></p>
<p>修改 <img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=b" style="display:inline-block;margin: 0;">。</p>
<p>假设修改 <img src="https://math.now.sh?inline=n" style="display:inline-block;margin: 0;"> 次，则 <img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=b" style="display:inline-block;margin: 0;"> 关于 <img src="https://math.now.sh?inline=%28x_i%2Cy_i%29" style="display:inline-block;margin: 0;"> 的增量分别是 <img src="https://math.now.sh?inline=%5Ceta_iy_ix_i" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=%5Ceta_iy_i" style="display:inline-block;margin: 0;">，这里 <img src="https://math.now.sh?inline=%5Ceta_i%3Dn_i%5Calpha" style="display:inline-block;margin: 0;">。于是，最后学习得到的模型参数可表示为</p>
<p><img src="https://math.now.sh?inline=w%3D%5Cdisplaystyle%20%5Csum_%7Bi%3D1%7D%5EN%20%5Ceta_iy_ix_i%20%5Ctag%7B7%7D" style="display:inline-block;margin: 0;"></p>
<p><img src="https://math.now.sh?inline=b%3D%5Cdisplaystyle%20%5Csum_%7Bi%3D1%7D%5EN%20%5Ceta_iy_i%20%5Ctag%7B8%7D" style="display:inline-block;margin: 0;"></p>
<p>这里 <img src="https://math.now.sh?inline=%5Ceta_i%20%5Cgeq%200%2C%5C%20i%3D1%2C2%2C...%2CN" style="display:inline-block;margin: 0;">。当 <img src="https://math.now.sh?inline=%5Calpha%3D1" style="display:inline-block;margin: 0;"> 时，表示第 <img src="https://math.now.sh?inline=i" style="display:inline-block;margin: 0;"> 个实例由于误分而进行更新的次数。实例更新次数越多，意味着它离超平面越近，也就越难正确分类。这样的实例对学习结构影响最大。</p>
<p>感知机学习算法的 <strong> 对偶形式 </strong> 为：</p>
<ul>
<li>Input：训练数据集 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;">，学习率 <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;">；</li>
<li>Output：<img src="https://math.now.sh?inline=%5Ceta" style="display:inline-block;margin: 0;">，<img src="https://math.now.sh?inline=b" style="display:inline-block;margin: 0;">，感知机模型 <img src="https://math.now.sh?inline=f%28x%29%3D%5Cdisplaystyle%20sign(%5Csum_%7Bj%3D1%7D%5EN%20%5Ceta_jy_jx_j%20%5Ccdot%20x%2Bb)" style="display:inline-block;margin: 0;">，其中 <img src="https://math.now.sh?inline=%5Ceta%3D%28%5Ceta_1%2C%5Ceta_2%2C...%2C%5Ceta_N%29%5ET" style="display:inline-block;margin: 0;">。</li>
</ul>
<ol>
<li>
<p>令 <img src="https://math.now.sh?inline=%5Ceta%3D0" style="display:inline-block;margin: 0;">，<img src="https://math.now.sh?inline=b%3D0" style="display:inline-block;margin: 0;">；</p>
</li>
<li>
<p>选取训练集中样本 <img src="https://math.now.sh?inline=%28x_i%2Cy_i%29" style="display:inline-block;margin: 0;">；</p>
</li>
<li>
<p>如果 <img src="https://math.now.sh?inline=%5Cdisplaystyle%20y_i%28%5Csum_%7Bj%3D1%7D%5EN%20%5Ceta_jy_jx_j%20%5Ccdot%20x_i%2Bb%29%20%5Cleq%200" style="display:inline-block;margin: 0;">，</p>
<p><img src="https://math.now.sh?inline=%5Ceta_i%3A%3D%5Ceta_i%2B%5Calpha" style="display:inline-block;margin: 0;"></p>
<p><img src="https://math.now.sh?inline=b%3A%3Db%2B%5Calpha%20y_i" style="display:inline-block;margin: 0;"></p>
<p>（** 注意！** 这里的判断公式可以进行变形：</p>
<p>​	将上面的公式 <img src="https://math.now.sh?inline=%288%29" style="display:inline-block;margin: 0;"> 带入 <img src="https://math.now.sh?inline=%5Cdisplaystyle%20y_i%28%5Csum_%7Bj%3D1%7D%5EN%20%5Ceta_jy_jx_j%20%5Ccdot%20x_i%2Bb%29%20%5Cleq%200" style="display:inline-block;margin: 0;"> 得到</p>
<p>​	<img src="https://math.now.sh?inline=%5Cdisplaystyle%20y_i%28%5Csum_%7Bj%3D1%7D%5EN%20%5Ceta_jy_jx_j%20%5Ccdot%20x_i%2B%5Csum_%7Bj%3D1%7D%5EN%20%5Ceta_jy_j%29%20%5Cleq%200" style="display:inline-block;margin: 0;"></p>
<p>​	合并后变为</p>
<p>​	<img src="https://math.now.sh?inline=%5Cdisplaystyle%20y_i%28%5Csum_%7Bj%3D1%7D%5EN%20%5Ceta_jy_j(x_j%20%5Ccdot%20x_i%2B%201%29)%20%5Cleq%200" style="display:inline-block;margin: 0;"></p>
<p>​	在代码中我使用这个公式进行判断，因为如果直接用 <img src="https://math.now.sh?inline=b" style="display:inline-block;margin: 0;"> 会导致结果错误，这点我也不太清楚原因）</p>
</li>
<li>
<p>重复 2、3 步，直到训练集中没有误分类点。</p>
</li>
</ol>
<p>对偶形式中训练实例只以内积的形式出现，可以先将所以训练数据的内积计算出来以矩阵形式储存，这就是 Gram 矩阵</p>
<p><img src="https://math.now.sh?inline=G%3D%5Bx_i%20%5Ccdot%20x_j%5D_%7BN%20%5Ctimes%20N%7D" style="display:inline-block;margin: 0;"></p>
<p>（在对偶形式的应用中，<img src="https://math.now.sh?inline=G%3D%5Bx_j%20%5Ccdot%20x_i%5D_%7BN%20%5Ctimes%20N%7D" style="display:inline-block;margin: 0;">，其中 <img src="https://math.now.sh?inline=i%20%5Cin%20%5B1%2CN%5D%2C%5C%20j%20%5Cin%20%5B1%2CN%5D" style="display:inline-block;margin: 0;">）</p>
<p>在选择误分类点顺序一致时，对偶形式的解与原始形式一样。并且，对偶形式也存在多个解。</p>
<h5 id="代码实现">代码实现</h5>
<p>已知正样本点 <img src="https://math.now.sh?inline=x_1%3D%283%2C3%29%5ET" style="display:inline-block;margin: 0;">，<img src="https://math.now.sh?inline=x_2%3D%284%2C3%29%5ET" style="display:inline-block;margin: 0;">，负样本点 <img src="https://math.now.sh?inline=x_3%3D%281%2C1%29%5ET" style="display:inline-block;margin: 0;">，求感知机模型。</p>
<p><strong>原始形式</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PerceptronOrigin</span>():</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    感知机算法的原始形式</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, x, y, a=<span class="number">1</span></span>):</span></span><br><span class="line">        self.x=x</span><br><span class="line">        self.y=y</span><br><span class="line">        <span class="comment"># 初始化模型参数 w 和 b （这里将它们都设为 0）</span></span><br><span class="line">        self.w=np.zeros(x.shape[<span class="number">1</span>])</span><br><span class="line">        self.b=<span class="number">0</span></span><br><span class="line">        <span class="comment"># 学习率</span></span><br><span class="line">        self.a=a</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        找出不会误分类任何训练样本的超平面</span></span><br><span class="line"><span class="string">        :return: w, b</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        error_exist=<span class="literal">True</span></span><br><span class="line">        current=<span class="number">0</span></span><br><span class="line">        index_point=<span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> error_exist:</span><br><span class="line">            <span class="comment"># 如果存在误分类点，更新参数</span></span><br><span class="line">            <span class="keyword">if</span> (np.dot(self.x[current],self.w)+self.b)*self.y[current]&lt;=<span class="number">0</span>:</span><br><span class="line">                index_point=current</span><br><span class="line">                self.w=np.add(self.w,self.a*self.y[current]*self.x[current])</span><br><span class="line">                self.b+=self.a*self.y[current]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 当前误分类点被正确分类后，从该点开始遍历所有点</span></span><br><span class="line">                current=(current+<span class="number">1</span>)%self.x.shape[<span class="number">0</span>]</span><br><span class="line">                <span class="comment"># 直到索引再次回到该点，表示已无误分类点</span></span><br><span class="line">                <span class="keyword">if</span> current==index_point:</span><br><span class="line">                    error_exist=<span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> self.w,self.b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        对测试样本进行预测</span></span><br><span class="line"><span class="string">        :param x: input(feature) space</span></span><br><span class="line"><span class="string">        :return: output label (+1 or -1)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        y=np.dot(self.w,x)+self.b</span><br><span class="line">        <span class="keyword">return</span> int(y)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/10/23/Perceptron/Figure_1.png" alt="Figure_1.png"></p>
<p><strong>对偶形式</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PerceptronDual</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, x, y, a=<span class="number">1</span></span>):</span></span><br><span class="line">        self.x=x</span><br><span class="line">        self.y=y</span><br><span class="line">        self.w=np.zeros(x.shape[<span class="number">1</span>])</span><br><span class="line">        self.eta=np.zeros(x.shape[<span class="number">0</span>])</span><br><span class="line">        self.b=<span class="number">0</span></span><br><span class="line">        self.a=a</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_Gram</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        计算 Gram 矩阵</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        x=self.x.copy()</span><br><span class="line">        G=x.dot(x.T)</span><br><span class="line">        <span class="keyword">return</span> G</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        G=self.get_Gram()</span><br><span class="line">        error_exist = <span class="literal">True</span></span><br><span class="line">        current = <span class="number">0</span></span><br><span class="line">        index_point = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> error_exist:</span><br><span class="line">            <span class="keyword">if</span> self.y[current]*(sum((self.eta[i]*self.y[i]*(G[current][i]+<span class="number">1</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> range(self.y.shape[<span class="number">0</span>]))) &lt;=<span class="number">0</span>:</span><br><span class="line">                <span class="string">&#x27;&#x27;&#x27; 注意这个判断公式 &#x27;&#x27;&#x27;</span></span><br><span class="line">                index_point = current</span><br><span class="line">                self.eta[current]+=self.a</span><br><span class="line">                self.b+=self.a*self.y[current]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                current=(current+<span class="number">1</span>)%self.x.shape[<span class="number">0</span>]</span><br><span class="line">                <span class="keyword">if</span> current==index_point:</span><br><span class="line">                    error_exist=<span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.y.shape[<span class="number">0</span>]):</span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27; 用公式计算出参数 w &#x27;&#x27;&#x27;</span></span><br><span class="line">            self.w=np.add(self.w,(self.eta[i]*self.x[i]*self.y[i]))</span><br><span class="line">        <span class="keyword">return</span> self.w,self.b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        y=np.dot(self.w,x)+self.b</span><br><span class="line">        <span class="keyword">return</span> int(y)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/10/23/Perceptron/Figure_2.png" alt="Figure_2.png"></p>
<p><a href="https://github.com/faple-ml/machine-learning/blob/master/perceptron.py">完整代码</a></p>
<h6 id="参考文献">参考文献</h6>
<p>[1] 《统计学习方法（第二版）》李航</p>
<p>[2] <a href="https://blog.csdn.net/huanyingzhizai/article/details/93525995">【机器学习笔记】——感知机（Perceptron）</a></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>perceptron</tag>
      </tags>
  </entry>
  <entry>
    <title>回归算法 Regression Algorithm</title>
    <url>/2020/09/22/Regression-Algorithms/</url>
    <content><![CDATA[<p>机器学习的一般步骤是：训练样本 - 特征抽取 - 学习函数 <img src="https://math.now.sh?inline=y%5E*%20%3D%20argmax%5C%20f%28X_i%29" style="display:inline-block;margin: 0;"> - 预测。其中，为了使模型型获得更多训练，预测和学习函数是在不断循环的。</p>
<p>机器学习的问题可以分为两种类型：<strong>分类问题 </strong> 和<strong>预测问题</strong>。分类模型是将数据最终归于某一个类型，而预测模型的结果更多的是一个具体数值。这里先讲一讲机器学习中经典的回归算法。</p>
<a id="more"></a>
<h2 id="线性回归">线性回归</h2>
<p>我们用一个例子来展现线性回归的建模过程。下表是一个银行判断用户贷款额度的数据集，其中，银行给用户的贷款额度取决于两个因素：工资和年龄，这两个因素就是这个数据集的_特征_。</p>
<table>
<thead>
<tr>
<th>工资（<img src="https://math.now.sh?inline=x_1" style="display:inline-block;margin: 0;">）</th>
<th>年龄（<img src="https://math.now.sh?inline=x_2" style="display:inline-block;margin: 0;">）</th>
<th>额度（<img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;">）</th>
</tr>
</thead>
<tbody>
<tr>
<td>4000</td>
<td>25</td>
<td>20000</td>
</tr>
<tr>
<td>8000</td>
<td>30</td>
<td>70000</td>
</tr>
<tr>
<td>5000</td>
<td>28</td>
<td>35000</td>
</tr>
<tr>
<td>7500</td>
<td>33</td>
<td>50000</td>
</tr>
<tr>
<td>12000</td>
<td>40</td>
<td>85000</td>
</tr>
</tbody>
</table>
<h3 id="建立模型">建立模型</h3>
<p>根据上表，给每个特征 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"> 一个参数 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;">，构成根据特征预测出的额度 <img src="https://math.now.sh?inline=h_%5Ctheta%28x%29" style="display:inline-block;margin: 0;">：</p>
<p><img src="https://math.now.sh?inline=%5Cbegin%7Baligned%7D%20h_%7B%5Ctheta%7D%28x%29%20%26%20%3D%20%5Ctheta_0%20%2B%20%5Ctheta_1x_1%20%2B%20%5Ctheta_2x_2%20%5C%5C%20%26%20%3D%20%5Csum_%7Bi%3D0%7D%5E%7B2%7D%5Ctheta_ix_i%20%5C%5C%20%26%20%3D%20%5Ctheta%5ETx%20%5Cend%7Baligned%7D" style="display:inline-block;margin: 0;"></p>
<p>但是预测的额度与真实值之间可能会有一定的误差，于是真实的额度可以表示为</p>
<p><img src="https://math.now.sh?inline=%5Cunderbrace%7By%5E%7B%28i%29%7D%7D_%7B%E7%9C%9F%E5%AE%9E%E5%80%BC%7D%20%3D%20%5Cunderbrace%7B%5Ctheta%5ETx%5E%7B(i)%7D%7D_%7B%E9%A2%84%E6%B5%8B%E5%80%BC%7D%20%2B%20%5Cunderbrace%7B%5Cvarepsilon%5E%7B(i)%7D%7D_%7B%E8%AF%AF%E5%B7%AE%7D%20%5Ctag%7B1%7D" style="display:inline-block;margin: 0;"></p>
<p>其中误差 <img src="https://math.now.sh?inline=%5Cvarepsilon%5E%7B%28i%29%7D" style="display:inline-block;margin: 0;"> 是_独立_（样本直接无关联）且_具有相同分布_（大部分误差在一个范围中）的，通常被认为服从均值为 0、方差为 <img src="https://math.now.sh?inline=%5Ctheta%5E2" style="display:inline-block;margin: 0;"> 的 <strong> 高斯分布</strong></p>
<ul>
<li>
<p>假设误差分布为 <img src="https://math.now.sh?inline=p" style="display:inline-block;margin: 0;"> （即 <img src="https://math.now.sh?inline=p" style="display:inline-block;margin: 0;"> 为高斯分布（均值为 0））</p>
<p><img src="https://math.now.sh?inline=p%28%5Cvarepsilon%5E%7B(i%29%7D)%20%3D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20%5Cexp(-%5Cfrac%7B(%5Cvarepsilon%5E%7B(i)%7D)%5E2%7D%7B2%5Csigma%5E2%7D)" style="display:inline-block;margin: 0;"></p>
<p>其中 <img src="https://math.now.sh?inline=%5Cexp%28n%29%20%3D%20e%5En" style="display:inline-block;margin: 0;"></p>
<p>用公式 <img src="https://math.now.sh?inline=%281%29" style="display:inline-block;margin: 0;"> 替换误差 <img src="https://math.now.sh?inline=%5Cvarepsilon%5E%7B%28i%29%7D" style="display:inline-block;margin: 0;">，得到</p>
<p><img src="https://math.now.sh?inline=p%28y%5E%7B(i%29%7D%7Cx%5E%7B(i)%7D%3B%5Ctheta)%20%3D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20%5Cexp(-%5Cfrac%7B(y%5E%7B(i)%7D-%5Ctheta%5ETx%5E%7B(i)%7D)%5E2%7D%7B2%5Csigma%5E2%7D)%20%5Ctag%7B2%7D" style="display:inline-block;margin: 0;"></p>
<p>这里 <img src="https://math.now.sh?inline=p%28y%5E%7B(i%29%7D%7Cx%5E%7B(i)%7D%3B%5Ctheta)%20%5Cin%20%5B0%2C1%5D" style="display:inline-block;margin: 0;">，即 <img src="https://math.now.sh?inline=p" style="display:inline-block;margin: 0;"> 为概率值，它越接近 1，概率越大，预测值越接近真实值。</p>
</li>
</ul>
<h4 id="似然函数和目标函数">似然函数和目标函数</h4>
<p>我们希望每个预测值都尽量接近真实值，所以构造了 <strong> 似然函数</strong>：</p>
<p><img src="https://math.now.sh?inline=%5Cbegin%7Baligned%7D%20L%28%5Ctheta%29%20%26%20%3D%20%5Cprod_%7Bi%3D1%7D%5E%7Bm%7Dp(y%5E%7B(i)%7D%7Cx%5E%7B(i)%7D%3B%5Ctheta)%20%5C%5C%20%26%20%3D%20%5Cprod_%7Bi%3D1%7D%5E%7Bm%7D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20%5Cexp(-%5Cfrac%7B(y%5E%7B(i)%7D-%5Ctheta%5ETx%5E%7B(i)%7D)%5E2%7D%7B2%5Csigma%5E2%7D)%20%5Cend%7Baligned%7D%20%5Ctag%7B3%7D" style="display:inline-block;margin: 0;"></p>
<p>这里_累乘所有样本的概率值，目标是使 <img src="https://math.now.sh?inline=L%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 尽可能大_。（因为如果每个训练样本的预测值都接近真实值，即误差很小，那么每个概率 <img src="https://math.now.sh?inline=p" style="display:inline-block;margin: 0;"> 都会接近 1，于是它们的累乘也会接近 1。）</p>
<p>为了方便计算，我们将 <img src="https://math.now.sh?inline=L%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 中的乘法转换成加法运算，即对 <img src="https://math.now.sh?inline=L%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 做对数运算，得到 <strong> 对数似然函数</strong>：</p>
<p><img src="https://math.now.sh?inline=%5Cbegin%7Baligned%7D%20l%28%5Ctheta%29%20%26%20%3D%20%5Clog%20L(%5Ctheta)%20%5C%5C%20%26%20%3D%20%5Clog%20%5Cprod_%7Bi%3D1%7D%5E%7Bm%7D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20%5Cexp(-%5Cfrac%7B(y%5E%7B(i)%7D-%5Ctheta%5ETx%5E%7B(i)%7D)%5E2%7D%7B2%5Csigma%5E2%7D)%20%5C%5C%20%26%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%5Clog%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20%5Cexp(-%5Cfrac%7B(y%5E%7B(i)%7D-%5Ctheta%5ETx%5E%7B(i)%7D)%5E2%7D%7B2%5Csigma%5E2%7D)%20%5C%5C%20%26%20%3D%20m%20%5Clog%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20-%20%5Cfrac%7B1%7D%7B%5Csigma%5E2%7D%20%5Ccdot%20%5Cfrac12%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%20(y%5E%7B(i)%7D-%5Ctheta%5ETx%5E%7B(i)%7D)%5E2%20%5Cend%7Baligned%7D%20%5Ctag%7B4%7D" style="display:inline-block;margin: 0;"></p>
<p>因为 <img src="https://math.now.sh?inline=l%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 的前半部分 <img src="https://math.now.sh?inline=m%20%5Clog%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D" style="display:inline-block;margin: 0;"> 为常数，不会被 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> 改变，所以要使 <img src="https://math.now.sh?inline=l%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 尽可能大，它的后半部分 <img src="https://math.now.sh?inline=%5Cfrac%7B1%7D%7B%5Csigma%5E2%7D%20%5Ccdot%20%5Cfrac12%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%20%28y%5E%7B(i%29%7D-%5Ctheta%5ETx%5E%7B(i)%7D)%5E2" style="display:inline-block;margin: 0;"> 需要尽可能小（这里后半部分一定大于 0，所以需要使其尽可能接近 0）。最后得到 <strong> 目标函数</strong></p>
<p><img src="https://math.now.sh?inline=J%28%5Ctheta%29%20%3D%20%5Cfrac12%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D(h_%7B%5Ctheta%7D(x%5E%7B(i)%7D)-y%5E%7B(i)%7D)%5E2%20%5Ctag%7B5%7D" style="display:inline-block;margin: 0;"></p>
<p>（留下 <img src="https://math.now.sh?inline=%5Cfrac12" style="display:inline-block;margin: 0;"> 方便求导）</p>
<h4 id="求导">求导</h4>
<p>我们的目标是找到_使 <img src="https://math.now.sh?inline=J%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 尽可能小的 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> 值_。对一个函数取极值，可以对其进行求导。</p>
<p>我们用矩阵形式表示 <img src="https://math.now.sh?inline=%285%29" style="display:inline-block;margin: 0;"> 中 <img src="https://math.now.sh?inline=%28h_%7B%5Ctheta%7D(x%5E%7B(i%29%7D)-y%5E%7B(i)%7D)%5E2" style="display:inline-block;margin: 0;">：</p>
<p><img src="https://math.now.sh?inline=J%28%5Ctheta%29%20%3D%20%5Cfrac12%20(X%5Ctheta-y)%5ET(X%5Ctheta-y)" style="display:inline-block;margin: 0;"></p>
<p>为了求 <img src="https://math.now.sh?inline=J%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 的极值，需要对 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> <em>求偏导</em></p>
<p><img src="https://math.now.sh?inline=%5Cbegin%7Baligned%7D%20%5Cnabla_%5Ctheta%20J%28%5Ctheta%29%20%26%20%3D%20%5Cnabla_%5Ctheta%20(%5Cfrac12%20(X%5Ctheta-y)%5ET(X%5Ctheta-y))%20%5C%5C%20%26%20%3D%20%5Cnabla_%5Ctheta%20(%5Cfrac12%20(%5Ctheta%5ETX%5ET-y%5ET)(X%5Ctheta-y))%20%5C%5C%20%26%20%3D%20%5Cnabla_%5Ctheta%20(%5Cfrac12%20(%5Ctheta%5ETX%5ETX%5Ctheta%20-%20%5Ctheta%5ETX%5ETy%20-%20y%5ETX%5Ctheta%20%2B%20y%5ETy))%20%5C%5C%20%26%20%3D%20%5Cfrac12%20(2X%5ETX%5Ctheta%20-%20X%5ETy%20-%20(y%5ETX)%5ET)%20%5C%5C%20%26%20%3D%20X%5ETX%5Ctheta%20-%20X%5ETy%20%5Cend%7Baligned%7D" style="display:inline-block;margin: 0;"></p>
<p>令 <img src="https://math.now.sh?inline=%5Cnabla_%5Ctheta%20J%28%5Ctheta%29%3D0" style="display:inline-block;margin: 0;">，即 <img src="https://math.now.sh?inline=X%5ETX%5Ctheta%20-%20X%5ETy%3D0" style="display:inline-block;margin: 0;">，则</p>
<p><img src="https://math.now.sh?inline=%5Ctheta%20%3D%20%28X%5ETX%29%5E%7B-1%7DX%5ETy%20%5Ctag%7B6%7D" style="display:inline-block;margin: 0;"></p>
<h2 id="Logistic- 回归">Logistic 回归</h2>
<p>逻辑回归是一个分类算法。</p>
<h3 id="S- 函数（Sigmoid- 函数）">S 函数（Sigmoid 函数）</h3>
<p><img src="/2020/09/22/Regression-Algorithms/790418-20181107181130984-1052306153.png" alt="img"></p>
<ul>
<li>
<p>函数表达式为 <img src="https://math.now.sh?inline=g%28z%29%20%3D%20%5Cfrac%7B1%7D%7B1%2Be%5E%7B(-z)%7D%7D" style="display:inline-block;margin: 0;"></p>
</li>
<li>
<p>取值范围 <img src="https://math.now.sh?inline=x%20%5Cin%20%28-%5Cinfty%2C%2B%5Cinfty%29%2C%5C%20y%20%5Cin%20(0%2C1)" style="display:inline-block;margin: 0;"></p>
</li>
</ul>
<h4 id="建模">建模</h4>
<p>由 S 函数可以看出，逻辑回归就是把模型的计算结果映射在 S 函数上，使最终结果在 <img src="https://math.now.sh?inline=%280%2C1%29" style="display:inline-block;margin: 0;"> 区间上。我们可以设置 0.5 为分界，结果小于 0.5 的数据归于一类，大于 0.5 的归于一类。</p>
<p>令<br>
<img src="https://math.now.sh?inline=h_%5Ctheta%28x%29%20%3D%20g(%5Ctheta%5ETx)%20%3D%20%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Ctheta%5ETx%7D%7D" style="display:inline-block;margin: 0;"><br>
这里符号含义同上，即 <img src="https://math.now.sh?inline=h_%5Ctheta%28x%29" style="display:inline-block;margin: 0;"> 是预测值。</p>
<p>之后建立目标函数的步骤与线性回归一致。</p>
<h3 id="梯度下降">梯度下降</h3>
<p>参数 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> 是无法一步优化到位的，我们需要一步一步使 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> 向函数的“坡下”走，这个向下走的“方向”就是函数在当前位置对 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> 的偏导数。</p>
<p>假设数据集只有一个特征，则预测值为 <img src="https://math.now.sh?inline=h_%5Ctheta%28x%29%3D%5Ctheta_1x%2B%5Ctheta_0" style="display:inline-block;margin: 0;"></p>
<p>目标函数为</p>
<p><img src="https://math.now.sh?inline=J%28%5Ctheta_0%2C%5Ctheta_1%29%20%3D%20%5Cfrac%7B1%7D%7B2m%7D%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D(h_%7B%5Ctheta%7D(x%5E%7B(i)%7D)-y%5E%7B(i)%7D)%5E2" style="display:inline-block;margin: 0;"></p>
<p>（这里如果不乘 <img src="https://math.now.sh?inline=%5Cfrac1m" style="display:inline-block;margin: 0;">，意味着训练样本越多 <img src="https://math.now.sh?inline=J%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 越大）</p>
<p>对 <img src="https://math.now.sh?inline=%5Ctheta_0" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=%5Ctheta_1" style="display:inline-block;margin: 0;"> 分别求偏导</p>
<p><img src="https://math.now.sh?inline=%5Cfrac%7B%5Cpartial%20J%28%5Ctheta_0%2C%5Ctheta_1%29%7D%7B%5Cpartial%20%5Ctheta_0%7D%20%3D%20%5Cfrac1m%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D(h_%7B%5Ctheta%7D(x%5E%7B(i)%7D)-y%5E%7B(i)%7D)" style="display:inline-block;margin: 0;"></p>
<p><img src="https://math.now.sh?inline=%5Cfrac%7B%5Cpartial%20J%28%5Ctheta_0%2C%5Ctheta_1%29%7D%7B%5Cpartial%20%5Ctheta_1%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D(h_%7B%5Ctheta%7D(x%5E%7B(i)%7D)-y%5E%7B(i)%7D)%20*%20x%5E%7B(i)%7D" style="display:inline-block;margin: 0;"></p>
<p>使每个参数沿梯度下降方向前进 <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"> 步</p>
<p><img src="https://math.now.sh?inline=%5Ctheta_0%20%3A%3D%20%5Ctheta_0%20-%20%5Calpha%20*%20%5Cfrac%7B%5Cpartial%20J%28%5Ctheta_0%2C%5Ctheta_1%29%7D%7B%5Cpartial%20%5Ctheta_0%7D" style="display:inline-block;margin: 0;"></p>
<p><img src="https://math.now.sh?inline=%5Ctheta_1%20%3A%3D%20%5Ctheta_1%20-%20%5Calpha%20*%20%5Cfrac%7B%5Cpartial%20J%28%5Ctheta_0%2C%5Ctheta_1%29%7D%7B%5Cpartial%20%5Ctheta_1%7D" style="display:inline-block;margin: 0;"></p>
<p>这里 <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"> 称为 <strong> 学习率</strong>（或步长），步长过大可能略过最优点，步长太小迭代速度太慢。</p>
<p>当参数经过迭代后改变很小时，迭代结束，我们认为当前 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> 是（或近似）最优解。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>linear regression</tag>
        <tag>logistic regression</tag>
      </tags>
  </entry>
  <entry>
    <title>Xgboost</title>
    <url>/2020/09/25/Xgboost/</url>
    <content><![CDATA[<p>Xgboost 是一套提升树可扩展性的机器学习系统，它的算法原理就是将多棵树的预测结果相加得到最终预测结果。之前看了一些博客和视频教程，看完还是一头雾水，最后还是看了原论文才终于有了点头绪。</p>
<a id="more"></a>
<h3 id="正则化目标函数">正则化目标函数</h3>
<p>首先，给定一个数据集 <img src="https://math.now.sh?inline=%5Ccal%7BD%7D" style="display:inline-block;margin: 0;"> 包含 <img src="https://math.now.sh?inline=n" style="display:inline-block;margin: 0;"> 个数据，其中每个数据有 <img src="https://math.now.sh?inline=m" style="display:inline-block;margin: 0;"> 个特征，即</p>
<p><img src="https://math.now.sh?inline=%5Ccal%7BD%7D%3D%5C%7B%28x_i%2Cy_i%29%5C%7D(%7CD%7C%3Dn%2C%5C%20x_i%20%5Cin%20%5CBbb%7BR%7D%5Em%2C%5C%20y_i%20%5Cin%20%5CBbb%7BR%7D)" style="display:inline-block;margin: 0;"></p>
<p>Xgboost 是一个树集成模型，它将 <img src="https://math.now.sh?inline=K" style="display:inline-block;margin: 0;"> 个树的预测结果进行求和得到最终预测值。即每一个样本的预测值为</p>
<p><img src="https://math.now.sh?inline=%5Chat%7By%7D_i%3D%5Cphi%28x_i%29%3D%5Csum_%7Bk%3D1%7D%5E%7BK%7Df_k(x_i)%2C%5C%20f_k%20%5Cin%20%5Ccal%7BF%7D%20%5Ctag%7B1%7D" style="display:inline-block;margin: 0;"></p>
<p>其中，</p>
<ul>
<li>
<p><img src="https://math.now.sh?inline=%5Ccal%7BF%7D%3D%5C%7Bf%28x%29%3Dw_%7Bq(x)%7D%5C%7D(q%3A%5CBbb%7BR%7D%5Em%20%5Cto%20%5Cmit%7BT%7D%2C%5C%20w%20%5Cin%20%5CBbb%7BR%7D%5ET)" style="display:inline-block;margin: 0;"> 是回归树（CART 树）的空间；</p>
</li>
<li>
<p><img src="https://math.now.sh?inline=q" style="display:inline-block;margin: 0;"> 代表每棵树的结构，它把样本映射到树中对应的叶子节点，比如一个样本 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"> 被树划分到一个叶子节点，这个叶子节点的编号为 <img src="https://math.now.sh?inline=q%28x%29" style="display:inline-block;margin: 0;">。</p>
<p>（所以 <img src="https://math.now.sh?inline=%5Ccal%7BF%7D" style="display:inline-block;margin: 0;"> 就是样本 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"> 被每棵树结构 <img src="https://math.now.sh?inline=q" style="display:inline-block;margin: 0;"> 映射到的叶节点 <img src="https://math.now.sh?inline=q%28x%29" style="display:inline-block;margin: 0;"> 的权重的集合）</p>
</li>
<li>
<p><img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"> 是树的叶子节点个数。</p>
</li>
<li>
<p>每一个 <img src="https://math.now.sh?inline=f_k" style="display:inline-block;margin: 0;"> 对应一个独立的树结构 <img src="https://math.now.sh?inline=q" style="display:inline-block;margin: 0;"> 和叶子节点的权重 <img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;">。</p>
</li>
</ul>
<p>这里的树是回归树，也就是树的每个叶子节点都包含一个权重 <img src="https://math.now.sh?inline=w_i" style="display:inline-block;margin: 0;"> （第 <img src="https://math.now.sh?inline=i" style="display:inline-block;margin: 0;"> 个叶子节点的权重）。对于每一个样本，我们根据每一棵回归树把它划分到每棵树的叶子节点中，然后对这些叶子节点的权重求和得到该样本的预测结果。</p>
<p>于是我们的目标就是最小化 <strong> 目标函数</strong>：</p>
<p><img src="https://math.now.sh?inline=%5Ccal%7BL%7D%28%5Cphi%29%3D%5Csum_%7Bi%7Dl(%5Chat%7By%7D_i%2Cy_i)%2B%5Csum_%7Bk%7D%5COmega(f_k)%20%5Ctag%7B2%7D" style="display:inline-block;margin: 0;"></p>
<p>其中</p>
<ul>
<li>
<p><img src="https://math.now.sh?inline=%5COmega%28f%29%3D%5Cgamma%20T%20%2B%20%5Cfrac12%20%5Clambda%20%7C%7Cw%7C%7C%5E2" style="display:inline-block;margin: 0;"> （<img src="https://math.now.sh?inline=%7C%7Cw%7C%7C%5E2%20%3D%20%5Csum_%7Bj%3D1%7D%5E%7BT%7Dw_j%5E2" style="display:inline-block;margin: 0;">）</p>
</li>
<li>
<p><img src="https://math.now.sh?inline=l" style="display:inline-block;margin: 0;"> 是 <strong> 损失函数</strong>，表示预测值 <img src="https://math.now.sh?inline=%5Chat%7By%7D_i" style="display:inline-block;margin: 0;"> 和真实值 <img src="https://math.now.sh?inline=y_i" style="display:inline-block;margin: 0;"> 之间的误差。</p>
</li>
<li>
<p><img src="https://math.now.sh?inline=%5COmega" style="display:inline-block;margin: 0;"> 用来控制（回归树）模型的复杂度，避免过拟合。</p>
</li>
</ul>
<p><img src="https://math.now.sh?inline=%282%29" style="display:inline-block;margin: 0;"> 中目标函数的优化参数是模型（即 <img src="https://math.now.sh?inline=f_k" style="display:inline-block;margin: 0;">）。因为整个 Xgboost 模型是以求和回归树的方式训练的，所以相当于训练中每一轮都加入一个回归树模型，即</p>
<ul>
<li>
<p>一开始，预测值 <img src="https://math.now.sh?inline=%5Chat%7By%7D_i%5E%7B%280%29%7D%3D0" style="display:inline-block;margin: 0;"></p>
</li>
<li>
<p>之后每一轮都是前一轮的预测值加上一个新的回归树模型，即在第 <img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"> 轮，模型的预测值为</p>
<p><img src="https://math.now.sh?inline=%5Chat%7By%7D_i%5E%7B%28t%29%7D%3D%5Chat%7By%7D_i%5E%7B(t-1)%7D%2Bf_t(x_i)" style="display:inline-block;margin: 0;"></p>
</li>
</ul>
<p>于是目标函数变为</p>
<p><img src="https://math.now.sh?inline=%5Ccal%7BL%7D%5E%7B%28t%29%7D%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dl(y_i%2C%5Chat%7By%7D_i%5E%7B(t-1)%7D%2Bf_t(x_i))%2B%5COmega(f_t)" style="display:inline-block;margin: 0;"></p>
<p>为了能快速优化目标函数，我们将目标函数进行二阶泰勒展开，得到近似目标函数（也就是说这个展开后的函数近似原来的目标函数，但并不完全相等）：</p>
<p><img src="https://math.now.sh?inline=%5Ccal%7BL%7D%5E%7B%28t%29%7D%20%5Csimeq%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Bl(y_i%2C%5Chat%7By%7D_i%5E%7B(t-1)%7D)%2Bg_if_t(x_i)%2B%5Cfrac12h_if_t%5E2(x_i)%5D%2B%5COmega(f_t)" style="display:inline-block;margin: 0;"></p>
<p>其中 <img src="https://math.now.sh?inline=g_i%3D%5Cpartial_%7B%5Chat%7By%7D%5E%7B%28t-1%29%7D%7Dl(y_i%2C%5Chat%7By%7D%5E%7B(t-1)%7D)" style="display:inline-block;margin: 0;">，<img src="https://math.now.sh?inline=h_i%3D%5Cpartial%5E2_%7B%5Chat%7By%7D%5E%7B%28t-1%29%7D%7Dl(y_i%2C%5Chat%7By%7D%5E%7B(t-1)%7D)" style="display:inline-block;margin: 0;"> 分别是损失函数的一阶和二阶偏导。</p>
<p>然后，我们可以移除目标函数中的常量，得到</p>
<p><img src="https://math.now.sh?inline=%5Cbar%7B%5Ccal%7BL%7D%7D%5E%7B%28t%29%7D%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Bg_if_t(x_i)%2B%5Cfrac12h_if_t%5E2(x_i)%5D%2B%5COmega(f_t)%20%5Ctag%7B3%7D" style="display:inline-block;margin: 0;"></p>
<p>令 <img src="https://math.now.sh?inline=I_j%3D%5C%7Bi%7Cq%28x_i%29%3Dj%5C%7D" style="display:inline-block;margin: 0;"> 为叶子节点 <img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"> 中样本的集合（即被划分到该叶子节点的所有样本），我们可以将 <img src="https://math.now.sh?inline=%283%29" style="display:inline-block;margin: 0;"> 写为</p>
<p><img src="https://math.now.sh?inline=%5Cbegin%7Baligned%7D%20%5Cbar%7B%5Ccal%7BL%7D%7D%5E%7B%28t%29%7D%20%26%20%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Bg_if_t(x_i)%2B%5Cfrac12h_if_t%5E2(x_i)%5D%2B%5Cgamma%20T%20%2B%20%5Cfrac12%5Clambda%5Csum_%7Bj%3D1%7D%5E%7BT%7Dw_j%5E2%20%5C%5C%20%26%20%3D%5Csum_%7Bj%3D1%7D%5E%7BT%7D%5B(%5Csum_%7Bi%20%5Cin%20I_j%7Dg_i)w_j%2B%5Cfrac12(%5Csum_%7Bi%20%5Cin%20I_j%7Dh_i%2B%5Clambda)w_j%5E2%5D%2B%5Cgamma%20T%20%5Cend%7Baligned%7D%20%5Ctag%7B4%7D" style="display:inline-block;margin: 0;"></p>
<p>求这个目标函数的最优解</p>
<p><img src="https://math.now.sh?inline=w_j%5E*%3D-%5Cfrac%7B%5Csum_%7Bi%20%5Cin%20I_j%7Dg_i%7D%7B%5Csum_%7Bi%20%5Cin%20I_j%7Dh_i%2B%5Clambda%7D%20%5Ctag%7B5%7D" style="display:inline-block;margin: 0;"></p>
<p>带入 <img src="https://math.now.sh?inline=w_j%5E*" style="display:inline-block;margin: 0;"> 得到最优目标函数</p>
<p><img src="https://math.now.sh?inline=%5Cbar%7B%5Ccal%7BL%7D%7D%5E%7B%28t%29%7D(q)%3D-%5Cfrac12%5Csum_%7Bj%3D1%7D%5E%7BT%7D%5Cfrac%7B(%5Csum_%7Bi%20%5Cin%20I_j%7Dg_i)%5E2%7D%7B%5Csum_%7Bi%20%5Cin%20I_j%7Dh_i%2B%5Clambda%7D%2B%5Cgamma%20T%20%5Ctag%7B6%7D" style="display:inline-block;margin: 0;"></p>
<p><img src="https://math.now.sh?inline=%5Cbar%7B%5Ccal%7BL%7D%7D%5E%7B%28t%29%7D(q)" style="display:inline-block;margin: 0;"> 可以用来衡量一个树结构 <img src="https://math.now.sh?inline=q" style="display:inline-block;margin: 0;"> 的好坏（类似决策树中的纯度），<img src="https://math.now.sh?inline=%5Cbar%7B%5Ccal%7BL%7D%7D%5E%7B%28t%29%7D(q)" style="display:inline-block;margin: 0;"> 越小，树结构越好。</p>
<h3 id="贪婪算法">贪婪算法</h3>
<p>通常我们无法枚举所有树结构，所以我们用一种贪婪算法：从单个叶节点开始，迭代地给树添加分支。</p>
<p>假设 <img src="https://math.now.sh?inline=I_L" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=I_R" style="display:inline-block;margin: 0;"> 是节点分裂后的左节点的样本集和右节点的样本集，令 <img src="https://math.now.sh?inline=I%3DI_L%20%5Ccup%20I_R" style="display:inline-block;margin: 0;">，节点切分后的损失函数为</p>
<p><img src="https://math.now.sh?inline=%5Ccal%7BL%7D_%7Bsplit%7D%3D%5Cfrac12%5B%5Cfrac%7B%28%5Csum_%7Bi%20%5Cin%20%5Cmit%7BI%7D_L%7Dg_i%29%5E2%7D%7B%5Csum_%7Bi%20%5Cin%20%5Cmit%7BI%7D_L%7Dh_i%2B%5Clambda%7D%2B%5Cfrac%7B(%5Csum_%7Bi%20%5Cin%20%5Cmit%7BI%7D_R%7Dg_i)%5E2%7D%7B%5Csum_%7Bi%20%5Cin%20%5Cmit%7BI%7D_R%7Dh_i%2B%5Clambda%7D-%5Cfrac%7B(%5Csum_%7Bi%20%5Cin%20%5Cmit%7BI%7D%7Dg_i)%5E2%7D%7B%5Csum_%7Bi%20%5Cin%20%5Cmit%7BI%7D%7Dh_i%2B%5Clambda%7D%5D-%5Cgamma%20%5Ctag%7B7%7D" style="display:inline-block;margin: 0;"></p>
<p>我们的目标是找到一个特征，使切分后的损失减少最大。<img src="https://math.now.sh?inline=%5Cgamma" style="display:inline-block;margin: 0;"> 除了控制树的复杂度，也作为阈值，当分裂后增益大于 <img src="https://math.now.sh?inline=%5Cgamma" style="display:inline-block;margin: 0;"> 时才分裂，起到了预剪枝作用。</p>
<h3 id="缩减和列采样">缩减和列采样</h3>
<p>缩减和列采样也用来防止过拟合。</p>
<ul>
<li>缩减是在每一步 tree boosting （增加树）后引入缩减系数 <img src="https://math.now.sh?inline=%5Ceta" style="display:inline-block;margin: 0;"> ，对新增加的权重进行收缩，减少每棵树的影响，为以后加入的树留下改进模型的空间。</li>
<li>列采样通常用在随机森林中，它有时比传统的行采样更能防止过拟合，它也加速了并行计算。</li>
</ul>
<p>之后还有一部分算法，比如寻找最佳分割点等，下次补充。</p>
<p>参考：</p>
<p><a href="https://arxiv.org/pdf/1603.02754v1.pdf">XGBoost: A Scalable Tree Boosting System</a>（这是 Xgboost 的论文）</p>
<p><a href="https://www.jianshu.com/p/a62f4dce3ce8">XGBoost 原理</a>（这篇基本是翻译了 Xgboost 的论文，有些英文看不明白的可以看看中文，不过我看到一半发现一处翻译错误，已在我的这篇订正）</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>xgboost</tag>
      </tags>
  </entry>
  <entry>
    <title>k 近邻法 k-NN</title>
    <url>/2020/10/26/k-NN/</url>
    <content><![CDATA[<p><img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 近邻法是一种基本分类与回归方法（这里只讨论分类问题）。<img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 近邻法通过训练样本对特征空间进行划分，形成模型，之后对新的实例，根据与其最近的 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 个实例的类别，通过多数表决等方式进行预测，即这 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 个实例的多数属于某个类，那么这个新的实例就被分为这个类。</p>
<a id="more"></a>
<p><img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 近邻法的 <strong> 算法 </strong> 为：</p>
<ul>
<li>Input：训练数据集 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;">，实例的特征向量 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;">；</li>
<li>Output：实例 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"> 所属的类 <img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;">；</li>
</ul>
<ol>
<li>
<p>根据距离找出训练集 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"> 中与 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"> 最邻近的 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 个点，涵盖这 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 个点的邻域记作 <img src="https://math.now.sh?inline=N_k%28x%29" style="display:inline-block;margin: 0;">；</p>
</li>
<li>
<p>在 <img src="https://math.now.sh?inline=N_k%28x%29" style="display:inline-block;margin: 0;"> 中根据_分类决策规则_（比如多数表决）决定 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"> 的类别 <img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;">：</p>
<p><img src="https://math.now.sh?inline=y%3D%5Cdisplaystyle%20%5Carg%20%5Cmax_%7Bc_j%7D%20%5Csum_%7Bx_i%20%5Cin%20N_k%28x%29%7D%20I(y_i%3Dc_j)%2C%5C%20i%3D1%2C2%2C...%2CK%20%5Ctag%7B1%7D" style="display:inline-block;margin: 0;"></p>
<p>其中，<img src="https://math.now.sh?inline=I" style="display:inline-block;margin: 0;"> 为指示函数，即当 <img src="https://math.now.sh?inline=y_i%3Dc_j" style="display:inline-block;margin: 0;"> 时 <img src="https://math.now.sh?inline=I" style="display:inline-block;margin: 0;"> 为 1，否则 <img src="https://math.now.sh?inline=I" style="display:inline-block;margin: 0;"> 为 0。</p>
</li>
</ol>
<p><img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 近邻法有特殊情况 <img src="https://math.now.sh?inline=k%3D1" style="display:inline-block;margin: 0;">，称为 <strong> 最近邻算法</strong>。对于输入的实例 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;">，最近邻法将训练集中与 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"> 最近的点的类作为 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"> 的类。</p>
<p>这里分类决策规则一般为多数表决规则，要使其误分类概率最小化，则相当于经验风险最小化。</p>
<h4 id="k- 近邻模型"><img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 近邻模型</h4>
<p><img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 近邻模型由三个基本要素决定：距离度量、<img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 值的选择和分类决策规则。当这三个要素确定后，每一个实例都可以确定唯一的分类。</p>
<p>特征空间中，对每一个训练样本 <img src="https://math.now.sh?inline=x_i" style="display:inline-block;margin: 0;">，距离该点比其他点更近的所有点组成一个区域，叫做单元（cell）。每个样本（只）拥有一个单元，所有训练样本的单元构成对特征空间的一个划分。每个单元中所有点的类 <img src="https://math.now.sh?inline=y_i" style="display:inline-block;margin: 0;"> 称为类标记（class label）。</p>
<h5 id="距离度量">距离度量</h5>
<p><img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 近邻法的距离度量一般为_欧氏距离_，但也可以是 <img src="https://math.now.sh?inline=L_p" style="display:inline-block;margin: 0;"> 距离或 Minkowski 距离。</p>
<p><strong><img src="https://math.now.sh?inline=L_p" style="display:inline-block;margin: 0;"> 距离</strong></p>
<p>设特征空间中两个实例 <img src="https://math.now.sh?inline=x_i%3D%28x_i%5E%7B(1%29%7D%2Cx_i%5E%7B(2)%7D%2C...%2Cx_i%5E%7B(n)%7D)%5ET" style="display:inline-block;margin: 0;">，<img src="https://math.now.sh?inline=x_j%3D%28x_i%5E%7B(1%29%7D%2Cx_i%5E%7B(2)%7D%2C...%2Cx_i%5E%7B(n)%7D)%5ET" style="display:inline-block;margin: 0;">，则 <img src="https://math.now.sh?inline=x_i" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=x_j" style="display:inline-block;margin: 0;"> 的 <img src="https://math.now.sh?inline=L_p" style="display:inline-block;margin: 0;"> 距离为</p>
<p><img src="https://math.now.sh?inline=L_p%28x_i%2Cx_j%29%3D%5Cdisplaystyle%20(%5Csum_%7Bl%3D1%7D%5En%20%7Cx_i%5E%7B(l)%7D-x_j%5E%7B(l)%7D%7C%5Ep)%5E%7B%5Cfrac1p%7D%20%5Ctag%7B2%7D" style="display:inline-block;margin: 0;"></p>
<p>这里 <img src="https://math.now.sh?inline=p%20%5Cgeq%201" style="display:inline-block;margin: 0;">。当 <img src="https://math.now.sh?inline=p%3D2" style="display:inline-block;margin: 0;"> 时，称为 <strong> 欧氏距离</strong>。</p>
<p>当 <img src="https://math.now.sh?inline=p%3D1" style="display:inline-block;margin: 0;"> 时，称为曼哈顿距离。</p>
<p>当 <img src="https://math.now.sh?inline=p%3D%5Cinfty" style="display:inline-block;margin: 0;"> 时，<img src="https://math.now.sh?inline=L_p" style="display:inline-block;margin: 0;"> 距离是 <img src="https://math.now.sh?inline=x_i" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=x_j" style="display:inline-block;margin: 0;"> 各个坐标距离的最大值，即</p>
<p><img src="https://math.now.sh?inline=L_%5Cinfty%28x_i%2Cx_j%29%3D%5Cdisplaystyle%20%5Cmax_l%20%7Cx_i%5E%7B(l)%7D-x_j%5E%7B(l)%7D%7C%20%5Ctag%7B3%7D" style="display:inline-block;margin: 0;"></p>
<p>由不同的距离度量所确定的最近邻点是不同的。</p>
<h5 id="k- 值的选择"><img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 值的选择</h5>
<p><img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 值的选择对 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 近邻法的结果影响很大。</p>
<p>如果 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 值较小，学习的近似误差会减小，估计误差会增大，即与输入实例比较近的点才会起比较大的作用。如果比较近的点是噪声，预测就会出错。也就是说，<img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 值较小的模型容易过拟合。</p>
<p>如果 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 值较大，学习的近似误差会增大，估计误差会减小，即与输入实例距离较远的点也会影响预测结果，使预测发生错误。</p>
<p>如果 <img src="https://math.now.sh?inline=k%3DN" style="display:inline-block;margin: 0;">（<img src="https://math.now.sh?inline=N" style="display:inline-block;margin: 0;"> 是训练样本的数量），那么输入实例会被预测为训练样本中最多点的类，这个模型过于简单，不可取。</p>
<p><img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 值一般取一个较小的数，通常用_交叉验证法_选取。</p>
<h4 id="kd- 树"><img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树</h4>
<p>为提高 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 近邻搜索的效率 ，我们需要用特殊的结构存储训练数据，以减少计算距离的次数，这就是 <img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树方法。</p>
<h5 id="构造 -kd- 树">构造 <img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树</h5>
<p><img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树是一种对 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 维空间中的点进行存储以便对其进行快速检索的树形数据结构。<img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树是二叉树，树种每一个节点对应于一个 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 维超矩形区域。</p>
<p><strong>构造 <img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树的方法</strong>：</p>
<ol>
<li>
<p>构造根节点（根节点对应 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 维空间中包含所有实例点的超矩形区域）；</p>
</li>
<li>
<p>递归切分 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 维空间，生成子节点：</p>
<ol>
<li>在超矩形区域（节点）上选择一个坐标轴和在此坐标轴上的一个切分点；</li>
<li>确定一个超平面通过切分点并垂直于坐标轴；</li>
<li>将当前超矩形区域切分为左右两个子区域（子节点）</li>
</ol>
<p>直到子区域内没有实例时终止。在这个过程中，将实例保存在相应的节点上。</p>
</li>
</ol>
<p>通常依次选择坐标轴，切分点选择坐标轴的中位数，这样得到的 <img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树是平衡的（但效率未必是最优的）。</p>
<p><strong>构造平衡 <img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树的算法 </strong> 为：</p>
<ul>
<li>Input：数据集 <img src="https://math.now.sh?inline=T%3D%5C%7Bx_1%2Cx_2%2C...%2Cx_N%5C%7D" style="display:inline-block;margin: 0;">（在 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 维空间中），其中 <img src="https://math.now.sh?inline=x_i%3D%28x_i%5E%7B(1%29%7D%2Cx_i%5E%7B(2)%7D%2C...%2Cx_i%5E%7B(k)%7D)%5ET" style="display:inline-block;margin: 0;">；</li>
<li>Output：<img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树。</li>
</ul>
<ol>
<li>
<p>开始：</p>
<p>构造根节点（根节点对应 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 维空间中包含所有实例点的超矩形区域）。</p>
<p>选择 <img src="https://math.now.sh?inline=x%5E%7B%281%29%7D" style="display:inline-block;margin: 0;"> 为坐标轴，以 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"> 中所有实例的 <img src="https://math.now.sh?inline=x%5E%7B%281%29%7D" style="display:inline-block;margin: 0;"> 坐标的中位数为切分点，得到两个子区域。</p>
<p>由根节点生成深度为 1 的左右子节点：左子节点对应坐标 <img src="https://math.now.sh?inline=x%5E%7B%281%29%7D" style="display:inline-block;margin: 0;"> 小于切分点的子区域，右子节点对应大于的子区域。</p>
<p>将落在切分超平面上的实例点保存在根节点。</p>
</li>
<li>
<p>重复：</p>
<p>对深度为 <img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"> 的节点，选择 <img src="https://math.now.sh?inline=x%5E%7B%28l%29%7D" style="display:inline-block;margin: 0;"> 为切分坐标轴，<img src="https://math.now.sh?inline=l%3Dj%28%5Cmod%20k%29%2B1" style="display:inline-block;margin: 0;">，以该节点的区域中所有实例的 <img src="https://math.now.sh?inline=x%5E%7B%28l%29%7D" style="display:inline-block;margin: 0;"> 坐标的中位数为切分点，将该节点对应的超矩形区域切分为两个子区域。</p>
<p>由该节点生成深度为 <img src="https://math.now.sh?inline=j%2B1" style="display:inline-block;margin: 0;"> 的左右子节点：左子节点对应坐标 <img src="https://math.now.sh?inline=x%5E%7B%28l%29%7D" style="display:inline-block;margin: 0;"> 小于切分点的子区域，右子节点对应大于的子区域。</p>
<p>将落在切分超平面上的实例点保存在该节点。</p>
</li>
<li>
<p>直到两个子区域没有实例存在时停止。完成 <img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树的区域划分。</p>
</li>
</ol>
<h5 id="搜索 -kd- 树">搜索 <img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树</h5>
<p>给定一个目标点，搜索其最近邻：</p>
<ol>
<li>找到包含目标点的叶节点；</li>
<li>从该叶节点出发，依次回退到父节点；</li>
<li>不断查找与目标点最邻近的节点，当确定不可能存在更近的节点时终止。</li>
</ol>
<p><strong><img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树的最近邻搜索算法</strong>：</p>
<ul>
<li>Input：<img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树，目标点 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;">；</li>
<li>Output：<img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"> 的最近邻。</li>
</ul>
<ol>
<li>
<p>在 <img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树中找出包含目标点 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"> 的叶节点：</p>
<p>从根节点出发，递归向下访问 <img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树。</p>
<p>若在当前维度目标点 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"> 的坐标小于切分点，则去左子树，否则去右子树。直到子节点为叶节点为止。</p>
</li>
<li>
<p>以此叶节点为“当前最近点”。</p>
</li>
<li>
<p>递归向上回退，在每个节点：</p>
<ol>
<li>如果该节点保存的实例点比“当前最近点”离目标点更近，则以该实例点为“当前最近点”；</li>
<li>检查该节点的兄弟节点对应的区域是否有更近的点，即，检查兄弟节点对应区域是否与以目标点为中心、以目标点与“当前最近点”间的距离为半径的超球体相较：
<ul>
<li>若相交，则可能存在更近的点，去该兄弟节点，递归进行近邻搜索；</li>
<li>若不相交，向上回退。</li>
</ul>
</li>
</ol>
</li>
<li>
<p>当回退到根节点时，搜索结束。最后的“当前最近节点”即为 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"> 的最近邻点。</p>
</li>
</ol>
<p>如果实例随机分布，<img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树搜索的平均计算复杂度为 <img src="https://math.now.sh?inline=O%28%5Clog%20N%29" style="display:inline-block;margin: 0;">（这里 <img src="https://math.now.sh?inline=N" style="display:inline-block;margin: 0;"> 是训练实例数）。</p>
<p><img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树更适用于训练实例数远大于空间维度时的 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"> 近邻搜索。</p>
<p>我用 Python(3.8) 实现了构造平衡 <img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树和 <img src="https://math.now.sh?inline=kd" style="display:inline-block;margin: 0;"> 树的最近邻搜索算法，完整代码在 <a href="https://github.com/faple-ml/machine-learning/blob/master/kd_tree.py"> 我的 Github</a>。kNN 算法比较简单，先不做实现。</p>
<h6 id="参考文献">参考文献</h6>
<p>[1] 《统计学习方法（第二版）》李航</p>
<p>[2] <a href="https://github.com/fengdu78/lihang-code/blob/master/%E7%AC%AC03%E7%AB%A0%20k%E8%BF%91%E9%82%BB%E6%B3%95/3.KNearestNeighbors.ipynb">李航《统计学习方法》的代码实现</a> （from：CSDN 博主 <a href="https://me.csdn.net/xue_csdn">Ooo。</a>）</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>kNN</tag>
      </tags>
  </entry>
</search>
