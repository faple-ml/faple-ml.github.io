<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>决策树 Decision Tree</title>
    <url>/2020/09/23/Decision-Tree/</url>
    <content><![CDATA[<p>决策树是机器学习中的一个预测模型，它表示对象属性和对象值之间的一种映射。树中的每一个非叶子节点（决策点）表示对象属性的判断条件，其分支表示符合节点条件的对象。树的叶子节点表示对象所属的预测结果。</p>
<a id="more"></a>
<h2 id="决策树">决策树</h2>
<h3 id="构建决策树">构建决策树</h3>
<p>决策树的构建分为两个阶段：</p>
<ol>
<li>
<p>训练阶段（构造决策树）</p>
<p><img src="https://math.now.sh?inline=class%20%3D%20DecisionTree%28Data%29" style="display:inline-block;margin: 0;"></p>
</li>
<li>
<p>分类阶段（获得分类 / 决策结果）</p>
<p><img src="https://math.now.sh?inline=y%20%3D%20DecisionTree%28x%29" style="display:inline-block;margin: 0;"></p>
</li>
</ol>
<p>构建决策树的基本思想是，随着树深度的增加，节点的熵迅速地降低；熵降低的速度越快越好，越快树的高度越矮。</p>
<h4 id="构建决策树的基本步骤">构建决策树的基本步骤</h4>
<ol>
<li>将所有数据看作一个节点；</li>
<li>遍历每个特征的每一种分割方式，找到最好的分割点；</li>
<li>分割成多个节点 <img src="https://math.now.sh?inline=N_1" style="display:inline-block;margin: 0;"> 到 <img src="https://math.now.sh?inline=N_i" style="display:inline-block;margin: 0;">；</li>
<li>对 <img src="https://math.now.sh?inline=N_1" style="display:inline-block;margin: 0;"> 到 <img src="https://math.now.sh?inline=N_i" style="display:inline-block;margin: 0;"> 分别执行 2、3 步，直到每个节点足够 <strong> 纯粹</strong>。</li>
</ol>
<h5 id="评价分割点的好坏">评价分割点的好坏</h5>
<p>如果一个分割点可以将当前的所有节点分为两类，使得每一类都 <strong> 纯度 </strong> 很高，也就是同一类的数据较多，那么就是一个好分割点。</p>
<p>于是，我们需要量化“纯度”这个指标。</p>
<h5 id="量化纯度">量化纯度</h5>
<p>如果一个特征被分为 <img src="https://math.now.sh?inline=n" style="display:inline-block;margin: 0;"> 类，则每一类的比例 <img src="https://math.now.sh?inline=P%28i%29%3D%E7%AC%AC%20i%20%E7%B1%BB%E7%9A%84%E6%95%B0%E7%9B%AE%2F%E6%80%BB%E6%95%B0%E7%9B%AE" style="display:inline-block;margin: 0;">。这个特征节点会有 <img src="https://math.now.sh?inline=n" style="display:inline-block;margin: 0;"> 个分支。</p>
<p>有三种方法来度量纯度：</p>
<ul>
<li>
<p>Gini 系数</p>
<p><img src="https://math.now.sh?inline=Gini%20%3D%201%20-%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7DP%28i%29%5E2" style="display:inline-block;margin: 0;"></p>
</li>
<li>
<p>熵（一般选择）</p>
<p><img src="https://math.now.sh?inline=Entropy%20%3D%20-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%28P(i%29%20%5Ctimes%20%5Clog_2P(i))" style="display:inline-block;margin: 0;"></p>
<p>这里 <img src="https://math.now.sh?inline=%5Clog" style="display:inline-block;margin: 0;"> 底数可以取 <img src="https://math.now.sh?inline=2" style="display:inline-block;margin: 0;"> 或 <img src="https://math.now.sh?inline=e" style="display:inline-block;margin: 0;">。</p>
<p>熵代表_混乱程度_，混乱程度越大，熵越大（即越不纯）。</p>
</li>
<li>
<p>错误率</p>
<p><img src="https://math.now.sh?inline=Error%20%3D%201%20-%20%5Cmax%28P(i%29%7Ci%20%5Cin%20%5B1%2Cn%5D)" style="display:inline-block;margin: 0;"></p>
</li>
</ul>
<p>这三个_值越大，表示纯度越低_。</p>
<p>然后有纯度差的概念，也就是 <strong> 信息增益 Information Gain</strong>。信息增益表示当以一个属性作为节点进行分裂后，它未分裂时的纯度与各个分支的纯度之和的差值。</p>
<p><img src="https://math.now.sh?inline=%5CDelta%20%3D%20I%28parent%29%20-%20%5Csum_%7Bj%3D1%7D%5E%7BK%7D(%5Cfrac%7BN(v_j)%7D%7BN%7D%20%5Ctimes%20I(v_j))" style="display:inline-block;margin: 0;"></p>
<p>其中，<img src="https://math.now.sh?inline=I" style="display:inline-block;margin: 0;"> 代表不纯度（i.e. 上面三个公式之一），<img src="https://math.now.sh?inline=K" style="display:inline-block;margin: 0;"> 代表分割的节点数（一般 <img src="https://math.now.sh?inline=K%3D2" style="display:inline-block;margin: 0;">），<img src="https://math.now.sh?inline=v_j" style="display:inline-block;margin: 0;"> 表示子节点中的记录数。</p>
<p>即，当前节点的不纯度减去子节点不纯度的加权平均数，权重由子节点记录数与当前节点记录数的比例决定。</p>
<h3 id="常用算法">常用算法</h3>
<h4 id="ID3- 算法">ID3 算法</h4>
<p>ID3 算法的核心思想是以_信息增益_度量属性选择，选择分裂后信息增益最大的属性进行分裂。</p>
<h5 id="算法步骤">算法步骤</h5>
<ol>
<li>
<p>设 D 为父节点，则 D 的熵为</p>
<p><img src="https://math.now.sh?inline=Entropy%28D%29%20%3D%20-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D(P(i)%20%5Ctimes%20%5Clog_2P(i))" style="display:inline-block;margin: 0;"></p>
<p>（即，数据集中 y 的各个分类的占比为 P）</p>
</li>
<li>
<p>对 D 的每个属性 A，计算 A 对 D 划分的期望信息</p>
<p><img src="https://math.now.sh?inline=Entropy_A%28D%29%20%3D%20-%5Csum_%7Bj%3D1%7D%5E%7Bv%7D(%5Cfrac%7B%7CD_j%7C%7D%7B%7CD%7C%7DEntropy(D_j))" style="display:inline-block;margin: 0;"></p>
<p>（即，一个属性中各个分类的熵乘以这个属性的占比）</p>
</li>
<li>
<p>信息增益即为两者差值</p>
<p><img src="https://math.now.sh?inline=Gain%28A%29%20%3D%20Entropy(D)%20-%20Entropy_A(D)" style="display:inline-block;margin: 0;"></p>
<p>（这里信息增益即，以 A 属性为节点，熵值下降了多少）</p>
</li>
</ol>
<p>ID3 算法就是在每次需要分裂时，计算每个属性的增益，然后选择增益最大的属性进行分裂。但 ID3 算法倾向于选择多属性，这种属性可能对分类没有太大作用，比如，假设一个属性有 10 种分类，每个分类中都只有一个数据，那么它的每个分类都很纯，信息增益会很大，但这个属性本身对决策没有帮助。C4.5 算法可以避免这个问题。</p>
<h4 id="C4-5- 算法">C4.5 算法</h4>
<p>C4.5 算法是 ID3 算法的优化，引入_信息增益率_的概念，用它替代信息增益作为选择属性的度量依据。</p>
<h5 id="算法步骤 -2">算法步骤</h5>
<ol>
<li>
<p>定义分裂信息</p>
<p><img src="https://math.now.sh?inline=split%5C_Entropy_A%28D%29%20%3D%20-%5Csum_%7Bj%3D1%7D%5E%7Bv%7D(%5Cfrac%7B%7CD_j%7C%7D%7B%7CD%7C%7D%5Clog_2(%7B%5Cfrac%7B%7CD_j%7C%7D%7B%7CD%7C%7D%7D))" style="display:inline-block;margin: 0;"></p>
</li>
<li>
<p>计算属性的信息增益（这一步与 ID3 中一致）</p>
</li>
<li>
<p>增益率为</p>
<p><img src="https://math.now.sh?inline=GainRatio%28A%29%20%3D%20%5Cfrac%7BGain(A)%7D%7Bsplit%5C_Entropy_A(D)%7D" style="display:inline-block;margin: 0;"></p>
</li>
</ol>
<p>C4.5 算法选择增益率最大的属性。</p>
<h4 id="CART- 算法">CART 算法</h4>
<p>区别于前两个算法，CART 算法基于 Gini 系数。</p>
<p>CART 算法计算以一个属性为节点的分支的_评价函数_（类似损失函数，越小越好）：</p>
<p><img src="https://math.now.sh?inline=C%28T%29%20%3D%20%5Csum_%7Bt%20%5Cin%20leaf%7DN_t%20%5Ccdot%20H(t)" style="display:inline-block;margin: 0;"></p>
<p>其中 <img src="https://math.now.sh?inline=N_t" style="display:inline-block;margin: 0;"> 为叶子节点中分类出的数据个数，<img src="https://math.now.sh?inline=H%28t%29" style="display:inline-block;margin: 0;"> 为熵值（这里是 Gini 系数，即 <img src="https://math.now.sh?inline=H%28t%29%3DGini" style="display:inline-block;margin: 0;">）</p>
<h3 id="连续值离散化">连续值离散化</h3>
<p>对于连续值（比如 <img src="https://math.now.sh?inline=%5B0%2C100%5D" style="display:inline-block;margin: 0;">），需要把它划分成离散的范围（比如 <img src="https://math.now.sh?inline=%5B0%2C20%5D%2C%5B21%2C40%5D%2C...%2C%5B81%2C100%5D" style="display:inline-block;margin: 0;">）。可以用 <strong> 贪婪算法 </strong> 选取分界点。</p>
<h3 id="剪枝">剪枝</h3>
<p>决策树_过度拟合_是因为节点过多（即决策树太高，分支太多），所以需要裁剪枝叶。</p>
<h4 id="裁剪策略">裁剪策略</h4>
<ul>
<li>
<p>预剪枝：在决策树构造时进行剪枝。当一个节点包含的样本数小于阈值，则不再划分。</p>
</li>
<li>
<p>后剪枝（常用方案）：生成决策树后再剪枝</p>
<p><img src="https://math.now.sh?inline=C_%5Calpha%28T%29%20%3D%20C(T)%20%2B%20%5Calpha%20%5Ccdot%20%7CT_%7Bleaf%7D%7C" style="display:inline-block;margin: 0;"></p>
<p>其中，<img src="https://math.now.sh?inline=C%28T%29" style="display:inline-block;margin: 0;"> 是评价函数（见 CART 算法），<img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"> 决定了叶子节点个数对最终的评价函数 <img src="https://math.now.sh?inline=C_%5Calpha%28T%29" style="display:inline-block;margin: 0;"> 的影响大小。叶子节点个数越多，损失越大。</p>
<p>当一个分支需要被裁剪时，或者用单一叶子节点代替整个子树，叶节点的分类为子树中的主要分类；或者用一个子树替代另一个子树。</p>
<p>但是后剪枝有一个主要问题，就是计算效率比较低（毕竟被裁剪的树枝也都被计算了一遍）。</p>
</li>
</ul>
<h2 id="随机森林">随机森林</h2>
<p>随机森林有两个步骤：</p>
<ol>
<li>Bootstraping 有放回采样（随机样本，随机特征）</li>
<li>Bagging 有放回采样 <img src="https://math.now.sh?inline=n" style="display:inline-block;margin: 0;"> 个样本共同建立分类器</li>
</ol>
<p>即，随机森林需要构造多颗决策树，将测试样本放入这些决策树得到结果，如果是分类，则取众数；如果是回归，则取平均值。其中需要选择随机的样本和特征构造决策树（避免干扰样本每次都被使用）。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>decision tree</tag>
        <tag>random forest</tag>
      </tags>
  </entry>
  <entry>
    <title>回归算法 Regression Algorithm</title>
    <url>/2020/09/22/Regression-Algorithms/</url>
    <content><![CDATA[<p>机器学习的一般步骤是：训练样本 - 特征抽取 - 学习函数 <img src="https://math.now.sh?inline=y%5E*%20%3D%20argmax%5C%20f%28X_i%29" style="display:inline-block;margin: 0;"> - 预测。其中，为了使模型型获得更多训练，预测和学习函数是在不断循环的。</p>
<p>机器学习的问题可以分为两种类型：<strong>分类问题 </strong> 和<strong>预测问题</strong>。分类模型是将数据最终归于某一个类型，而预测模型的结果更多的是一个具体数值。这里先讲一讲机器学习中经典的回归算法。</p>
<a id="more"></a>
<h2 id="线性回归">线性回归</h2>
<p>我们用一个例子来展现线性回归的建模过程。下表是一个银行判断用户贷款额度的数据集，其中，银行给用户的贷款额度取决于两个因素：工资和年龄，这两个因素就是这个数据集的_特征_。</p>
<table>
<thead>
<tr>
<th>工资（<img src="https://math.now.sh?inline=x_1" style="display:inline-block;margin: 0;">）</th>
<th>年龄（<img src="https://math.now.sh?inline=x_2" style="display:inline-block;margin: 0;">）</th>
<th>额度（<img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;">）</th>
</tr>
</thead>
<tbody>
<tr>
<td>4000</td>
<td>25</td>
<td>20000</td>
</tr>
<tr>
<td>8000</td>
<td>30</td>
<td>70000</td>
</tr>
<tr>
<td>5000</td>
<td>28</td>
<td>35000</td>
</tr>
<tr>
<td>7500</td>
<td>33</td>
<td>50000</td>
</tr>
<tr>
<td>12000</td>
<td>40</td>
<td>85000</td>
</tr>
</tbody>
</table>
<h3 id="建立模型">建立模型</h3>
<p>根据上表，给每个特征 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"> 一个参数 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;">，构成根据特征预测出的额度 <img src="https://math.now.sh?inline=h_%5Ctheta%28x%29" style="display:inline-block;margin: 0;">：</p>
<p><img src="https://math.now.sh?inline=%5Cbegin%7Baligned%7D%20h_%7B%5Ctheta%7D%28x%29%20%26%20%3D%20%5Ctheta_0%20%2B%20%5Ctheta_1x_1%20%2B%20%5Ctheta_2x_2%20%5C%5C%20%26%20%3D%20%5Csum_%7Bi%3D0%7D%5E%7B2%7D%5Ctheta_ix_i%20%5C%5C%20%26%20%3D%20%5Ctheta%5ETx%20%5Cend%7Baligned%7D" style="display:inline-block;margin: 0;"></p>
<p>但是预测的额度与真实值之间可能会有一定的误差，于是真实的额度可以表示为</p>
<p><img src="https://math.now.sh?inline=%5Cunderbrace%7By%5E%7B%28i%29%7D%7D_%7B%E7%9C%9F%E5%AE%9E%E5%80%BC%7D%20%3D%20%5Cunderbrace%7B%5Ctheta%5ETx%5E%7B(i)%7D%7D_%7B%E9%A2%84%E6%B5%8B%E5%80%BC%7D%20%2B%20%5Cunderbrace%7B%5Cvarepsilon%5E%7B(i)%7D%7D_%7B%E8%AF%AF%E5%B7%AE%7D%20%5Ctag%7B1%7D" style="display:inline-block;margin: 0;"></p>
<p>其中误差 <img src="https://math.now.sh?inline=%5Cvarepsilon%5E%7B%28i%29%7D" style="display:inline-block;margin: 0;"> 是_独立_（样本直接无关联）且_具有相同分布_（大部分误差在一个范围中）的，通常被认为服从均值为 0、方差为 <img src="https://math.now.sh?inline=%5Ctheta%5E2" style="display:inline-block;margin: 0;"> 的 <strong> 高斯分布</strong></p>
<ul>
<li>
<p>假设误差分布为 <img src="https://math.now.sh?inline=p" style="display:inline-block;margin: 0;"> （即 <img src="https://math.now.sh?inline=p" style="display:inline-block;margin: 0;"> 为高斯分布（均值为 0））</p>
<p><img src="https://math.now.sh?inline=p%28%5Cvarepsilon%5E%7B(i%29%7D)%20%3D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20%5Cexp(-%5Cfrac%7B(%5Cvarepsilon%5E%7B(i)%7D)%5E2%7D%7B2%5Csigma%5E2%7D)" style="display:inline-block;margin: 0;"></p>
<p>其中 <img src="https://math.now.sh?inline=%5Cexp%28n%29%20%3D%20e%5En" style="display:inline-block;margin: 0;"></p>
<p>用公式 <img src="https://math.now.sh?inline=%281%29" style="display:inline-block;margin: 0;"> 替换误差 <img src="https://math.now.sh?inline=%5Cvarepsilon%5E%7B%28i%29%7D" style="display:inline-block;margin: 0;">，得到</p>
<p><img src="https://math.now.sh?inline=p%28y%5E%7B(i%29%7D%7Cx%5E%7B(i)%7D%3B%5Ctheta)%20%3D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20%5Cexp(-%5Cfrac%7B(y%5E%7B(i)%7D-%5Ctheta%5ETx%5E%7B(i)%7D)%5E2%7D%7B2%5Csigma%5E2%7D)%20%5Ctag%7B2%7D" style="display:inline-block;margin: 0;"></p>
<p>这里 <img src="https://math.now.sh?inline=p%28y%5E%7B(i%29%7D%7Cx%5E%7B(i)%7D%3B%5Ctheta)%20%5Cin%20%5B0%2C1%5D" style="display:inline-block;margin: 0;">，即 <img src="https://math.now.sh?inline=p" style="display:inline-block;margin: 0;"> 为概率值，它越接近 1，概率越大，预测值越接近真实值。</p>
</li>
</ul>
<h4 id="似然函数和目标函数">似然函数和目标函数</h4>
<p>我们希望每个预测值都尽量接近真实值，所以构造了 <strong> 似然函数</strong>：</p>
<p><img src="https://math.now.sh?inline=%5Cbegin%7Baligned%7D%20L%28%5Ctheta%29%20%26%20%3D%20%5Cprod_%7Bi%3D1%7D%5E%7Bm%7Dp(y%5E%7B(i)%7D%7Cx%5E%7B(i)%7D%3B%5Ctheta)%20%5C%5C%20%26%20%3D%20%5Cprod_%7Bi%3D1%7D%5E%7Bm%7D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20%5Cexp(-%5Cfrac%7B(y%5E%7B(i)%7D-%5Ctheta%5ETx%5E%7B(i)%7D)%5E2%7D%7B2%5Csigma%5E2%7D)%20%5Cend%7Baligned%7D%20%5Ctag%7B3%7D" style="display:inline-block;margin: 0;"></p>
<p>这里_累乘所有样本的概率值，目标是使 <img src="https://math.now.sh?inline=L%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 尽可能大_。（因为如果每个训练样本的预测值都接近真实值，即误差很小，那么每个概率 <img src="https://math.now.sh?inline=p" style="display:inline-block;margin: 0;"> 都会接近 1，于是它们的累乘也会接近 1。）</p>
<p>为了方便计算，我们将 <img src="https://math.now.sh?inline=L%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 中的乘法转换成加法运算，即对 <img src="https://math.now.sh?inline=L%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 做对数运算，得到 <strong> 对数似然函数</strong>：</p>
<p><img src="https://math.now.sh?inline=%5Cbegin%7Baligned%7D%20l%28%5Ctheta%29%20%26%20%3D%20%5Clog%20L(%5Ctheta)%20%5C%5C%20%26%20%3D%20%5Clog%20%5Cprod_%7Bi%3D1%7D%5E%7Bm%7D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20%5Cexp(-%5Cfrac%7B(y%5E%7B(i)%7D-%5Ctheta%5ETx%5E%7B(i)%7D)%5E2%7D%7B2%5Csigma%5E2%7D)%20%5C%5C%20%26%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%5Clog%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20%5Cexp(-%5Cfrac%7B(y%5E%7B(i)%7D-%5Ctheta%5ETx%5E%7B(i)%7D)%5E2%7D%7B2%5Csigma%5E2%7D)%20%5C%5C%20%26%20%3D%20m%20%5Clog%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20-%20%5Cfrac%7B1%7D%7B%5Csigma%5E2%7D%20%5Ccdot%20%5Cfrac12%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%20(y%5E%7B(i)%7D-%5Ctheta%5ETx%5E%7B(i)%7D)%5E2%20%5Cend%7Baligned%7D%20%5Ctag%7B4%7D" style="display:inline-block;margin: 0;"></p>
<p>因为 <img src="https://math.now.sh?inline=l%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 的前半部分 <img src="https://math.now.sh?inline=m%20%5Clog%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D" style="display:inline-block;margin: 0;"> 为常数，不会被 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> 改变，所以要使 <img src="https://math.now.sh?inline=l%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 尽可能大，它的后半部分 <img src="https://math.now.sh?inline=%5Cfrac%7B1%7D%7B%5Csigma%5E2%7D%20%5Ccdot%20%5Cfrac12%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%20%28y%5E%7B(i%29%7D-%5Ctheta%5ETx%5E%7B(i)%7D)%5E2" style="display:inline-block;margin: 0;"> 需要尽可能小（这里后半部分一定大于 0，所以需要使其尽可能接近 0）。最后得到 <strong> 目标函数</strong></p>
<p><img src="https://math.now.sh?inline=J%28%5Ctheta%29%20%3D%20%5Cfrac12%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D(h_%7B%5Ctheta%7D(x%5E%7B(i)%7D)-y%5E%7B(i)%7D)%5E2%20%5Ctag%7B5%7D" style="display:inline-block;margin: 0;"></p>
<p>（留下 <img src="https://math.now.sh?inline=%5Cfrac12" style="display:inline-block;margin: 0;"> 方便求导）</p>
<h4 id="求导">求导</h4>
<p>我们的目标是找到_使 <img src="https://math.now.sh?inline=J%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 尽可能小的 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> 值_。对一个函数取极值，可以对其进行求导。</p>
<p>我们用矩阵形式表示 <img src="https://math.now.sh?inline=%285%29" style="display:inline-block;margin: 0;"> 中 <img src="https://math.now.sh?inline=%28h_%7B%5Ctheta%7D(x%5E%7B(i%29%7D)-y%5E%7B(i)%7D)%5E2" style="display:inline-block;margin: 0;">：</p>
<p><img src="https://math.now.sh?inline=J%28%5Ctheta%29%20%3D%20%5Cfrac12%20(X%5Ctheta-y)%5ET(X%5Ctheta-y)" style="display:inline-block;margin: 0;"></p>
<p>为了求 <img src="https://math.now.sh?inline=J%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 的极值，需要对 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> <em>求偏导</em></p>
<p><img src="https://math.now.sh?inline=%5Cbegin%7Baligned%7D%20%5Cnabla_%5Ctheta%20J%28%5Ctheta%29%20%26%20%3D%20%5Cnabla_%5Ctheta%20(%5Cfrac12%20(X%5Ctheta-y)%5ET(X%5Ctheta-y))%20%5C%5C%20%26%20%3D%20%5Cnabla_%5Ctheta%20(%5Cfrac12%20(%5Ctheta%5ETX%5ET-y%5ET)(X%5Ctheta-y))%20%5C%5C%20%26%20%3D%20%5Cnabla_%5Ctheta%20(%5Cfrac12%20(%5Ctheta%5ETX%5ETX%5Ctheta%20-%20%5Ctheta%5ETX%5ETy%20-%20y%5ETX%5Ctheta%20%2B%20y%5ETy))%20%5C%5C%20%26%20%3D%20%5Cfrac12%20(2X%5ETX%5Ctheta%20-%20X%5ETy%20-%20(y%5ETX)%5ET)%20%5C%5C%20%26%20%3D%20X%5ETX%5Ctheta%20-%20X%5ETy%20%5Cend%7Baligned%7D" style="display:inline-block;margin: 0;"></p>
<p>令 <img src="https://math.now.sh?inline=%5Cnabla_%5Ctheta%20J%28%5Ctheta%29%3D0" style="display:inline-block;margin: 0;">，即 <img src="https://math.now.sh?inline=X%5ETX%5Ctheta%20-%20X%5ETy%3D0" style="display:inline-block;margin: 0;">，则</p>
<p><img src="https://math.now.sh?inline=%5Ctheta%20%3D%20%28X%5ETX%29%5E%7B-1%7DX%5ETy%20%5Ctag%7B6%7D" style="display:inline-block;margin: 0;"></p>
<h2 id="Logistic- 回归">Logistic 回归</h2>
<p>逻辑回归是一个分类算法。</p>
<h3 id="S- 函数（Sigmoid- 函数）">S 函数（Sigmoid 函数）</h3>
<p><img src="/2020/09/22/Regression-Algorithms/790418-20181107181130984-1052306153.png" alt="img"></p>
<ul>
<li>
<p>函数表达式为 <img src="https://math.now.sh?inline=g%28z%29%20%3D%20%5Cfrac%7B1%7D%7B1%2Be%5E%7B(-z)%7D%7D" style="display:inline-block;margin: 0;"></p>
</li>
<li>
<p>取值范围 <img src="https://math.now.sh?inline=x%20%5Cin%20%28-%5Cinfty%2C%2B%5Cinfty%29%2C%5C%20y%20%5Cin%20(0%2C1)" style="display:inline-block;margin: 0;"></p>
</li>
</ul>
<h4 id="建模">建模</h4>
<p>由 S 函数可以看出，逻辑回归就是把模型的计算结果映射在 S 函数上，使最终结果在 <img src="https://math.now.sh?inline=%280%2C1%29" style="display:inline-block;margin: 0;"> 区间上。我们可以设置 0.5 为分界，结果小于 0.5 的数据归于一类，大于 0.5 的归于一类。</p>
<p>令<br>
<img src="https://math.now.sh?inline=h_%5Ctheta%28x%29%20%3D%20g(%5Ctheta%5ETx)%20%3D%20%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Ctheta%5ETx%7D%7D" style="display:inline-block;margin: 0;"><br>
这里符号含义同上，即 <img src="https://math.now.sh?inline=h_%5Ctheta%28x%29" style="display:inline-block;margin: 0;"> 是预测值。</p>
<p>之后建立目标函数的步骤与线性回归一致。</p>
<h3 id="梯度下降">梯度下降</h3>
<p>参数 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> 是无法一步优化到位的，我们需要一步一步使 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> 向函数的“坡下”走，这个向下走的“方向”就是函数在当前位置对 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> 的偏导数。</p>
<p>假设数据集只有一个特征，则预测值为 <img src="https://math.now.sh?inline=h_%5Ctheta%28x%29%3D%5Ctheta_1x%2B%5Ctheta_0" style="display:inline-block;margin: 0;"></p>
<p>目标函数为</p>
<p><img src="https://math.now.sh?inline=J%28%5Ctheta_0%2C%5Ctheta_1%29%20%3D%20%5Cfrac%7B1%7D%7B2m%7D%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D(h_%7B%5Ctheta%7D(x%5E%7B(i)%7D)-y%5E%7B(i)%7D)%5E2" style="display:inline-block;margin: 0;"></p>
<p>（这里如果不乘 <img src="https://math.now.sh?inline=%5Cfrac1m" style="display:inline-block;margin: 0;">，意味着训练样本越多 <img src="https://math.now.sh?inline=J%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 越大）</p>
<p>对 <img src="https://math.now.sh?inline=%5Ctheta_0" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=%5Ctheta_1" style="display:inline-block;margin: 0;"> 分别求偏导</p>
<p><img src="https://math.now.sh?inline=%5Cfrac%7B%5Cpartial%20J%28%5Ctheta_0%2C%5Ctheta_1%29%7D%7B%5Cpartial%20%5Ctheta_0%7D%20%3D%20%5Cfrac1m%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D(h_%7B%5Ctheta%7D(x%5E%7B(i)%7D)-y%5E%7B(i)%7D)" style="display:inline-block;margin: 0;"></p>
<p><img src="https://math.now.sh?inline=%5Cfrac%7B%5Cpartial%20J%28%5Ctheta_0%2C%5Ctheta_1%29%7D%7B%5Cpartial%20%5Ctheta_1%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D(h_%7B%5Ctheta%7D(x%5E%7B(i)%7D)-y%5E%7B(i)%7D)%20*%20x%5E%7B(i)%7D" style="display:inline-block;margin: 0;"></p>
<p>使每个参数沿梯度下降方向前进 <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"> 步</p>
<p><img src="https://math.now.sh?inline=%5Ctheta_0%20%3A%3D%20%5Ctheta_0%20-%20%5Calpha%20*%20%5Cfrac%7B%5Cpartial%20J%28%5Ctheta_0%2C%5Ctheta_1%29%7D%7B%5Cpartial%20%5Ctheta_0%7D" style="display:inline-block;margin: 0;"></p>
<p><img src="https://math.now.sh?inline=%5Ctheta_1%20%3A%3D%20%5Ctheta_1%20-%20%5Calpha%20*%20%5Cfrac%7B%5Cpartial%20J%28%5Ctheta_0%2C%5Ctheta_1%29%7D%7B%5Cpartial%20%5Ctheta_1%7D" style="display:inline-block;margin: 0;"></p>
<p>这里 <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"> 称为 <strong> 学习率</strong>（或步长），步长过大可能略过最优点，步长太小迭代速度太慢。</p>
<p>当参数经过迭代后改变很小时，迭代结束，我们认为当前 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> 是（或近似）最优解。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>linear regression</tag>
        <tag>logistic regression</tag>
      </tags>
  </entry>
</search>
