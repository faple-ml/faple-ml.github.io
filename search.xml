<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>贝叶斯算法 Bayesian Classfier</title>
    <url>/2020/09/24/Bayesian-Classfier/</url>
    <content><![CDATA[<p>贝叶斯算法是一种分类算法，用于解决逆向概率（逆概）的问题。</p>
<a id="more"></a>
<p>首先我们需要了解正向概率和逆向概率的区别。</p>
<ul>
<li><strong>正向概率</strong>：假设袋子里有 <img src="https://math.now.sh?inline=N" style="display:inline-block;margin: 0;"> 个白球，<img src="https://math.now.sh?inline=M" style="display:inline-block;margin: 0;"> 个黑球，摸一次出黑球的概率。</li>
<li><strong>逆向概率</strong>：袋子中黑白球的比例未知，摸出一个（或几个）球，根据取出球的颜色，推测袋子中黑白球的比例。</li>
</ul>
<p>然后我们先用一个例子来引入和介绍贝叶斯算法。</p>
<h3 id="学生校服问题">学生校服问题</h3>
<p>假设学校中有 60% 的男生和 40% 的女生，其中男生总穿长裤，女生一半穿长裤一半穿裙子。</p>
<p>一个正向概率的问题可能是：随机选一个学生，TA 穿长裤的概率或穿裙子的概率是多大。而一个逆向概率的问题为：一个学生穿着长裤，TA 是女生的概率是多大。</p>
<h4 id="公式推导">公式推导</h4>
<p>假设学校里总人数为 <img src="https://math.now.sh?inline=U" style="display:inline-block;margin: 0;">。</p>
<p>则穿长裤的男生人数为</p>
<p><img src="https://math.now.sh?inline=U*P%28Boy%29*P(Pants%7CBoy)" style="display:inline-block;margin: 0;"></p>
<p>其中，</p>
<ul>
<li><img src="https://math.now.sh?inline=P%28Boy%29" style="display:inline-block;margin: 0;"> 表示男生的概率 = 60%</li>
<li><img src="https://math.now.sh?inline=P%28Pants%7CBoy%29" style="display:inline-block;margin: 0;"> 表示男生穿长裤的概率 = 100%</li>
</ul>
<p>穿长裤的女生人数为</p>
<p><img src="https://math.now.sh?inline=U*P%28Girl%29*P(Pants%7CGirl)" style="display:inline-block;margin: 0;"></p>
<p>我们想要知道穿长裤的学生中女生的概率，则需要用穿长裤的女生的人数除以穿长裤的学生的总人数。</p>
<p>穿长裤的学生的总人数为</p>
<p><img src="https://math.now.sh?inline=U*P%28Boy%29*P(Pants%7CBoy)%2BU*P(Girl)*P(Pants%7CGirl)" style="display:inline-block;margin: 0;"></p>
<p>所以穿长裤的学生中女生的比例为</p>
<p><img src="https://math.now.sh?inline=%5Cbegin%7Baligned%7D%20P%28Girl%7CPants%29%20%26%20%3D%5Cfrac%7BU*P(Girl)*P(Pants%7CGirl)%7D%7B%E7%A9%BF%E9%95%BF%E8%A3%A4%E6%80%BB%E4%BA%BA%E6%95%B0%7D%20%5C%5C%20%26%20%3D%5Cfrac%7BU*P(Girl)*P(Pants%7CGirl)%7D%7BU*P(Boy)*P(Pants%7CBoy)%2BU*P(Girl)*P(Pants%7CGirl)%7D%20%5C%5C%20%26%20%3D%20%5Cfrac%7BP(Girl)*P(Pants%7CGirl)%7D%7B%5Cunderbrace%7BP(Boy)*P(Pants%7CBoy)%2BP(Girl)*P(Pants%7CGirl)%7D_%7B%E5%AD%A6%E6%A0%A1%E4%B8%AD%E7%A9%BF%E8%A3%A4%E5%AD%90%E5%AD%A6%E7%94%9F%E7%9A%84%E6%AF%94%E4%BE%8B%7D%7D%20%5C%5C%20%26%20%3D%5Cfrac%7BP(Pants%2CGirl)%7D%7BP(Pants)%7D%20%5Cend%7Baligned%7D" style="display:inline-block;margin: 0;"></p>
<h3 id="贝叶斯公式">贝叶斯公式</h3>
<p>由上面的例子，我们可以看出贝叶斯公式的一般形式为</p>
<p><img src="https://math.now.sh?inline=P%28A%7CB%29%3D%5Cfrac%7BP(B%7CA)P(A)%7D%7BP(B)%7D" style="display:inline-block;margin: 0;"></p>
<p>即求在 B 的条件下 A 的概率，可以根据在 A 的条件下 B 的概率进行计算。</p>
<p>其中 <img src="https://math.now.sh?inline=P%28A%29" style="display:inline-block;margin: 0;"> 是 <strong> 先验概率</strong>。</p>
<p>这里我们再用另一个实例来解释和应用贝叶斯。</p>
<h3 id="拼写纠正实例">拼写纠正实例</h3>
<p>一个用户输入了一个不在词典中的单词，我们需要猜测他想输入的单词是什么，即计算 <img src="https://math.now.sh?inline=P%28%E7%8C%9C%E6%B5%8B%E4%BB%96%E6%83%B3%E8%BE%93%E5%85%A5%E7%9A%84%E5%8D%95%E8%AF%8D%7C%E4%BB%96%E5%AE%9E%E9%99%85%E8%BE%93%E5%85%A5%E7%9A%84%E5%8D%95%E8%AF%8D%29" style="display:inline-block;margin: 0;">。</p>
<p>我们设用户实际输入单词为 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"> (Data，即观测数据)，猜测是某个单词的概率为 <img src="https://math.now.sh?inline=P%28h%7CD%29" style="display:inline-block;margin: 0;">，即 <img src="https://math.now.sh?inline=h" style="display:inline-block;margin: 0;"> 是我们猜测的单词，该输入单词为 <img src="https://math.now.sh?inline=h" style="display:inline-block;margin: 0;"> 的概率为 <img src="https://math.now.sh?inline=P%28h%7CD%29" style="display:inline-block;margin: 0;">。</p>
<p>根据贝叶斯公式，</p>
<p><img src="https://math.now.sh?inline=P%28h%7CD%29%3D%5Cfrac%7BP(D%7Ch)P(h)%7D%7BP(D)%7D" style="display:inline-block;margin: 0;"></p>
<p>其中，</p>
<ul>
<li>因为对于每个猜测 h1,h2,h3…，<img src="https://math.now.sh?inline=P%28D%29" style="display:inline-block;margin: 0;"> 的值不变，所以可以忽略它</li>
<li>所以 <img src="https://math.now.sh?inline=P%28h%7CD%29%20%5Cpropto%20P(h)*P(D%7Ch)" style="display:inline-block;margin: 0;">，即它俩成正比
<ul>
<li>这里 <img src="https://math.now.sh?inline=P%28h%29" style="display:inline-block;margin: 0;"> 表示猜测 <img src="https://math.now.sh?inline=h" style="display:inline-block;margin: 0;"> 本身独立的可能性大小。
<ul>
<li>比如在词库中有 1w 个单词，其中 <img src="https://math.now.sh?inline=h" style="display:inline-block;margin: 0;"> 有 5000 个，那么 <img src="https://math.now.sh?inline=P%28h%29%3D%5Cfrac%7B5000%7D%7B10000%7D%3D%5Cfrac12" style="display:inline-block;margin: 0;">。</li>
<li>即猜测的单词的_词频_会影响它的可能性的概率。比如，用户输入单词 thaw，我们猜测他可能想输入 that 或者 than，这两个单词生成输入单词的可能性都很大，那么因为 that 的词频比较高，所以我们最后会猜测用户想输入的是 that。</li>
</ul>
</li>
<li><img src="https://math.now.sh?inline=P%28D%7Ch%29" style="display:inline-block;margin: 0;"> 表示猜测 <img src="https://math.now.sh?inline=h" style="display:inline-block;margin: 0;"> 生成输入单词的可能性大小。</li>
</ul>
</li>
</ul>
<h3 id="模型比较">模型比较</h3>
<ul>
<li><strong>最大似然估计</strong>：最符合观测数据的最有优势（即 <img src="https://math.now.sh?inline=P%28D%7Ch%29" style="display:inline-block;margin: 0;"> 最大的最有优势）。
<ul>
<li>比如，我们先投掷硬币 10w 次，观察每次硬币的正反，得出有 60% 的概率为正，40% 的概率为反，那么在第 10w+1 次投掷硬币时，我们就根据前 10w 次观察的概率判断这次硬币可能为正。</li>
</ul>
</li>
<li><strong>奥卡姆剃刀</strong>：<img src="https://math.now.sh?inline=P%28h%29" style="display:inline-block;margin: 0;"> 较大的模型有较大的优势（即越常见越好）。</li>
</ul>
<p>最大似然估计应该比较常用。</p>
<h3 id="垃圾邮件过滤">垃圾邮件过滤</h3>
<p>这个应该是贝叶斯分类的经典例子。</p>
<p>我们要判断一封邮件是否为垃圾邮件。</p>
<p>首先，假设这封邮件为 D，D 由 <img src="https://math.now.sh?inline=N" style="display:inline-block;margin: 0;"> 个单词组成。我们用 <img src="https://math.now.sh?inline=h%2B" style="display:inline-block;margin: 0;"> 表示垃圾邮件，<img src="https://math.now.sh?inline=h-" style="display:inline-block;margin: 0;"> 表示正常邮件。</p>
<p>那么 D 是垃圾邮件的概率为</p>
<p><img src="https://math.now.sh?inline=P%28h%2B%7CD%29%3D%5Cfrac%7BP(h%2B)P(D%7Ch%2B)%7D%7BP(D)%7D" style="display:inline-block;margin: 0;"></p>
<p>D 不是垃圾邮件的概率为</p>
<p><img src="https://math.now.sh?inline=P%28h-%7CD%29%3D%5Cfrac%7BP(h-)P(D%7Ch-)%7D%7BP(D)%7D" style="display:inline-block;margin: 0;"></p>
<p>这里_先验概率_ <img src="https://math.now.sh?inline=P%28h%2B%29" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=P%28h-%29" style="display:inline-block;margin: 0;"> 可以由一个邮件库中垃圾邮件和正常邮件的比例求得。</p>
<p>因为 D 中包含 <img src="https://math.now.sh?inline=N" style="display:inline-block;margin: 0;"> 个单词 d1,d2,…,dn，则 <img src="https://math.now.sh?inline=P%28D%7Ch%2B%29%3DP(d1%2Cd2%2C...%2Cdn%7Ch%2B)" style="display:inline-block;margin: 0;"> 表示在垃圾邮件中出现与邮件 D 一样的邮件的概率。即</p>
<p><img src="https://math.now.sh?inline=P%28d1%2Cd2%2C...%2Cdn%7Ch%2B%29%3DP(d1%7Ch%2B)*P(d2%7Cd1%2Ch%2B)*P(d3%7Cd2%2Cd1%2Ch%2B)*..." style="display:inline-block;margin: 0;"></p>
<p>其中，<img src="https://math.now.sh?inline=P%28d1%7Ch%2B%29" style="display:inline-block;margin: 0;"> 表示一个垃圾邮件中第一个词为 d1 的概率，<img src="https://math.now.sh?inline=P%28d2%7Cd1%2Ch%2B%29" style="display:inline-block;margin: 0;"> 表示一个垃圾邮件中第一个词为 d1 后第二个词恰好为 d2 的概率（其他类推）。</p>
<p>为了方便计算，我们假设 di 与 di-1 相互独立（即完全无关，互不影响），则将其转换为 <strong> 朴素贝叶斯</strong>（Naive Bayes，NB）：</p>
<p><img src="https://math.now.sh?inline=P%28d1%2Cd2%2C...%2Cdn%7Ch%2B%29%3DP(d1%7Ch%2B)*P(d2%7Ch%2B)*P(d3%7Ch%2B)*..." style="display:inline-block;margin: 0;"></p>
<p>于是只需要统计单词 di 在垃圾邮件中出现的频率即可。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>bayes algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树 Decision Tree</title>
    <url>/2020/09/23/Decision-Tree/</url>
    <content><![CDATA[<p>决策树是机器学习中的一个预测模型，它表示对象属性和对象值之间的一种映射。树中的每一个非叶子节点（决策点）表示对象属性的判断条件，其分支表示符合节点条件的对象。树的叶子节点表示对象所属的预测结果。</p>
<a id="more"></a>
<h2 id="决策树">决策树</h2>
<h3 id="构建决策树">构建决策树</h3>
<p>决策树的构建分为两个阶段：</p>
<ol>
<li>
<p>训练阶段（构造决策树）</p>
<p><img src="https://math.now.sh?inline=class%20%3D%20DecisionTree%28Data%29" style="display:inline-block;margin: 0;"></p>
</li>
<li>
<p>分类阶段（获得分类 / 决策结果）</p>
<p><img src="https://math.now.sh?inline=y%20%3D%20DecisionTree%28x%29" style="display:inline-block;margin: 0;"></p>
</li>
</ol>
<p>构建决策树的基本思想是，随着树深度的增加，节点的熵迅速地降低；熵降低的速度越快越好，越快树的高度越矮。</p>
<h4 id="构建决策树的基本步骤">构建决策树的基本步骤</h4>
<ol>
<li>将所有数据看作一个节点；</li>
<li>遍历每个特征的每一种分割方式，找到最好的分割点；</li>
<li>分割成多个节点 <img src="https://math.now.sh?inline=N_1" style="display:inline-block;margin: 0;"> 到 <img src="https://math.now.sh?inline=N_i" style="display:inline-block;margin: 0;">；</li>
<li>对 <img src="https://math.now.sh?inline=N_1" style="display:inline-block;margin: 0;"> 到 <img src="https://math.now.sh?inline=N_i" style="display:inline-block;margin: 0;"> 分别执行 2、3 步，直到每个节点足够 <strong> 纯粹</strong>。</li>
</ol>
<h5 id="评价分割点的好坏">评价分割点的好坏</h5>
<p>如果一个分割点可以将当前的所有节点分为两类，使得每一类都 <strong> 纯度 </strong> 很高，也就是同一类的数据较多，那么就是一个好分割点。</p>
<p>于是，我们需要量化“纯度”这个指标。</p>
<h5 id="量化纯度">量化纯度</h5>
<p>如果一个特征被分为 <img src="https://math.now.sh?inline=n" style="display:inline-block;margin: 0;"> 类，则每一类的比例 <img src="https://math.now.sh?inline=P%28i%29%3D%E7%AC%AC%20i%20%E7%B1%BB%E7%9A%84%E6%95%B0%E7%9B%AE%2F%E6%80%BB%E6%95%B0%E7%9B%AE" style="display:inline-block;margin: 0;">。这个特征节点会有 <img src="https://math.now.sh?inline=n" style="display:inline-block;margin: 0;"> 个分支。</p>
<p>有三种方法来度量纯度：</p>
<ul>
<li>
<p>Gini 系数</p>
<p><img src="https://math.now.sh?inline=Gini%20%3D%201%20-%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7DP%28i%29%5E2" style="display:inline-block;margin: 0;"></p>
</li>
<li>
<p>熵（一般选择）</p>
<p><img src="https://math.now.sh?inline=Entropy%20%3D%20-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%28P(i%29%20%5Ctimes%20%5Clog_2P(i))" style="display:inline-block;margin: 0;"></p>
<p>这里 <img src="https://math.now.sh?inline=%5Clog" style="display:inline-block;margin: 0;"> 底数可以取 <img src="https://math.now.sh?inline=2" style="display:inline-block;margin: 0;"> 或 <img src="https://math.now.sh?inline=e" style="display:inline-block;margin: 0;">。</p>
<p>熵代表_混乱程度_，混乱程度越大，熵越大（即越不纯）。</p>
</li>
<li>
<p>错误率</p>
<p><img src="https://math.now.sh?inline=Error%20%3D%201%20-%20%5Cmax%28P(i%29%7Ci%20%5Cin%20%5B1%2Cn%5D)" style="display:inline-block;margin: 0;"></p>
</li>
</ul>
<p>这三个_值越大，表示纯度越低_。</p>
<p>然后有纯度差的概念，也就是 <strong> 信息增益 Information Gain</strong>。信息增益表示当以一个属性作为节点进行分裂后，它未分裂时的纯度与各个分支的纯度之和的差值。</p>
<p><img src="https://math.now.sh?inline=%5CDelta%20%3D%20I%28parent%29%20-%20%5Csum_%7Bj%3D1%7D%5E%7BK%7D(%5Cfrac%7BN(v_j)%7D%7BN%7D%20%5Ctimes%20I(v_j))" style="display:inline-block;margin: 0;"></p>
<p>其中，<img src="https://math.now.sh?inline=I" style="display:inline-block;margin: 0;"> 代表不纯度（i.e. 上面三个公式之一），<img src="https://math.now.sh?inline=K" style="display:inline-block;margin: 0;"> 代表分割的节点数（一般 <img src="https://math.now.sh?inline=K%3D2" style="display:inline-block;margin: 0;">），<img src="https://math.now.sh?inline=v_j" style="display:inline-block;margin: 0;"> 表示子节点中的记录数。</p>
<p>即，当前节点的不纯度减去子节点不纯度的加权平均数，权重由子节点记录数与当前节点记录数的比例决定。</p>
<h3 id="常用算法">常用算法</h3>
<h4 id="ID3- 算法">ID3 算法</h4>
<p>ID3 算法的核心思想是以_信息增益_度量属性选择，选择分裂后信息增益最大的属性进行分裂。</p>
<h5 id="算法步骤">算法步骤</h5>
<ol>
<li>
<p>设 D 为父节点，则 D 的熵为</p>
<p><img src="https://math.now.sh?inline=Entropy%28D%29%20%3D%20-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D(P(i)%20%5Ctimes%20%5Clog_2P(i))" style="display:inline-block;margin: 0;"></p>
<p>（即，数据集中 y 的各个分类的占比为 P）</p>
</li>
<li>
<p>对 D 的每个属性 A，计算 A 对 D 划分的期望信息</p>
<p><img src="https://math.now.sh?inline=Entropy_A%28D%29%20%3D%20-%5Csum_%7Bj%3D1%7D%5E%7Bv%7D(%5Cfrac%7B%7CD_j%7C%7D%7B%7CD%7C%7DEntropy(D_j))" style="display:inline-block;margin: 0;"></p>
<p>（即，一个属性中各个分类的熵乘以这个属性的占比）</p>
</li>
<li>
<p>信息增益即为两者差值</p>
<p><img src="https://math.now.sh?inline=Gain%28A%29%20%3D%20Entropy(D)%20-%20Entropy_A(D)" style="display:inline-block;margin: 0;"></p>
<p>（这里信息增益即，以 A 属性为节点，熵值下降了多少）</p>
</li>
</ol>
<p>ID3 算法就是在每次需要分裂时，计算每个属性的增益，然后选择增益最大的属性进行分裂。但 ID3 算法倾向于选择多属性，这种属性可能对分类没有太大作用，比如，假设一个属性有 10 种分类，每个分类中都只有一个数据，那么它的每个分类都很纯，信息增益会很大，但这个属性本身对决策没有帮助。C4.5 算法可以避免这个问题。</p>
<h4 id="C4-5- 算法">C4.5 算法</h4>
<p>C4.5 算法是 ID3 算法的优化，引入_信息增益率_的概念，用它替代信息增益作为选择属性的度量依据。</p>
<h5 id="算法步骤 -2">算法步骤</h5>
<ol>
<li>
<p>定义分裂信息</p>
<p><img src="https://math.now.sh?inline=split%5C_Entropy_A%28D%29%20%3D%20-%5Csum_%7Bj%3D1%7D%5E%7Bv%7D(%5Cfrac%7B%7CD_j%7C%7D%7B%7CD%7C%7D%5Clog_2(%7B%5Cfrac%7B%7CD_j%7C%7D%7B%7CD%7C%7D%7D))" style="display:inline-block;margin: 0;"></p>
</li>
<li>
<p>计算属性的信息增益（这一步与 ID3 中一致）</p>
</li>
<li>
<p>增益率为</p>
<p><img src="https://math.now.sh?inline=GainRatio%28A%29%20%3D%20%5Cfrac%7BGain(A)%7D%7Bsplit%5C_Entropy_A(D)%7D" style="display:inline-block;margin: 0;"></p>
</li>
</ol>
<p>C4.5 算法选择增益率最大的属性。</p>
<h4 id="CART- 算法">CART 算法</h4>
<p>区别于前两个算法，CART 算法基于 Gini 系数。</p>
<p>CART 算法计算以一个属性为节点的分支的_评价函数_（类似损失函数，越小越好）：</p>
<p><img src="https://math.now.sh?inline=C%28T%29%20%3D%20%5Csum_%7Bt%20%5Cin%20leaf%7DN_t%20%5Ccdot%20H(t)" style="display:inline-block;margin: 0;"></p>
<p>其中 <img src="https://math.now.sh?inline=N_t" style="display:inline-block;margin: 0;"> 为叶子节点中分类出的数据个数，<img src="https://math.now.sh?inline=H%28t%29" style="display:inline-block;margin: 0;"> 为熵值（这里是 Gini 系数，即 <img src="https://math.now.sh?inline=H%28t%29%3DGini" style="display:inline-block;margin: 0;">）</p>
<h3 id="连续值离散化">连续值离散化</h3>
<p>对于连续值（比如 <img src="https://math.now.sh?inline=%5B0%2C100%5D" style="display:inline-block;margin: 0;">），需要把它划分成离散的范围（比如 <img src="https://math.now.sh?inline=%5B0%2C20%5D%2C%5B21%2C40%5D%2C...%2C%5B81%2C100%5D" style="display:inline-block;margin: 0;">）。可以用 <strong> 贪婪算法 </strong> 选取分界点。</p>
<h3 id="剪枝">剪枝</h3>
<p>决策树_过度拟合_是因为节点过多（即决策树太高，分支太多），所以需要裁剪枝叶。</p>
<h4 id="裁剪策略">裁剪策略</h4>
<ul>
<li>
<p>预剪枝：在决策树构造时进行剪枝。当一个节点包含的样本数小于阈值，则不再划分。</p>
</li>
<li>
<p>后剪枝（常用方案）：生成决策树后再剪枝</p>
<p><img src="https://math.now.sh?inline=C_%5Calpha%28T%29%20%3D%20C(T)%20%2B%20%5Calpha%20%5Ccdot%20%7CT_%7Bleaf%7D%7C" style="display:inline-block;margin: 0;"></p>
<p>其中，<img src="https://math.now.sh?inline=C%28T%29" style="display:inline-block;margin: 0;"> 是评价函数（见 CART 算法），<img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"> 决定了叶子节点个数对最终的评价函数 <img src="https://math.now.sh?inline=C_%5Calpha%28T%29" style="display:inline-block;margin: 0;"> 的影响大小。叶子节点个数越多，损失越大。</p>
<p>当一个分支需要被裁剪时，或者用单一叶子节点代替整个子树，叶节点的分类为子树中的主要分类；或者用一个子树替代另一个子树。</p>
<p>但是后剪枝有一个主要问题，就是计算效率比较低（毕竟被裁剪的树枝也都被计算了一遍）。</p>
</li>
</ul>
<h2 id="随机森林">随机森林</h2>
<p>随机森林有两个步骤：</p>
<ol>
<li>Bootstraping 有放回采样（随机样本，随机特征）</li>
<li>Bagging 有放回采样 <img src="https://math.now.sh?inline=n" style="display:inline-block;margin: 0;"> 个样本共同建立分类器</li>
</ol>
<p>即，随机森林需要构造多颗决策树，将测试样本放入这些决策树得到结果，如果是分类，则取众数；如果是回归，则取平均值。其中需要选择随机的样本和特征构造决策树（避免干扰样本每次都被使用）。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>decision tree</tag>
        <tag>random forest</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown 数学公式和符号</title>
    <url>/2020/09/25/Markdown-Math/</url>
    <content><![CDATA[<p> 因为经常需要查一些数学符号的写法，有点麻烦，也没有找到一个比较全面的博客，所以决定自己整理一份，方便查找。</p>
<a id="more"></a>
<h4 id="显示数学公式或符号"> 显示数学公式或符号 </h4>
<p>$ 数学公式或符号 $</p>
<h4 id="上 - 下标"> 上 / 下标 </h4>
<ul>
<li> 上标：<code>x^2</code> <img src="https://math.now.sh?inline=x%5E2" style="display:inline-block;margin: 0;"></li>
<li> 下标：<code>x_2</code> <img src="https://math.now.sh?inline=x_2" style="display:inline-block;margin: 0;"></li>
</ul>
<h4 id="占位符"> 占位符 </h4>
<ul>
<li> 两个 quad 空格：<code>\qquad</code> <img src="https://math.now.sh?inline=x%20%5Cqquad%20y" style="display:inline-block;margin: 0;"></li>
<li>quad 空格：<code>\quad</code> <img src="https://math.now.sh?inline=x%20%5Cquad%20y" style="display:inline-block;margin: 0;"></li>
<li> 空格：<code>\</code> <img src="https://math.now.sh?inline=x%20%5C%20y" style="display:inline-block;margin: 0;"></li>
<li> 紧贴：<code>\!</code> <img src="https://math.now.sh?inline=x%20%5C!%20y" style="display:inline-block;margin: 0;"></li>
</ul>
<h4 id="四则运算"> 四则运算 </h4>
<ul>
<li> 加：<code>+</code> <img src="https://math.now.sh?inline=x%2By" style="display:inline-block;margin: 0;"></li>
<li> 减：<code>-</code> <img src="https://math.now.sh?inline=x-y" style="display:inline-block;margin: 0;"></li>
<li> 乘：<code>\times</code> <img src="https://math.now.sh?inline=x%20%5Ctimes%20y" style="display:inline-block;margin: 0;"></li>
<li> 点乘：<code>\cdot</code> <img src="https://math.now.sh?inline=x%20%5Ccdot%20y" style="display:inline-block;margin: 0;"></li>
<li> 星乘：<code>\ast</code> 或 <code>*</code> <img src="https://math.now.sh?inline=x%20%5Cast%20y" style="display:inline-block;margin: 0;"></li>
<li> 除：<code>\div</code> <img src="https://math.now.sh?inline=x%20%5Cdiv%20y" style="display:inline-block;margin: 0;"> 或 <code>/</code> <img src="https://math.now.sh?inline=x%2Fy" style="display:inline-block;margin: 0;"></li>
<li> 加减：<code>\pm</code> <img src="https://math.now.sh?inline=x%20%5Cpm%20y" style="display:inline-block;margin: 0;"></li>
<li> 减加：<code>\mp</code> <img src="https://math.now.sh?inline=x%20%5Cmp%20y" style="display:inline-block;margin: 0;"></li>
</ul>
<h4 id="高级运算"> 高级运算 </h4>
<ul>
<li> 分式：<code>\frac&#123; 分子 &#125;&#123; 分母 &#125;</code> <img src="https://math.now.sh?inline=%5Cfrac%7Bx%7D%7By%7D" style="display:inline-block;margin: 0;"> 或 <code>&#123; 分子 &#125;\over&#123; 分母 &#125;</code> <img src="https://math.now.sh?inline=%7Bx%7D%20%5Cover%20%7By%7D" style="display:inline-block;margin: 0;"></li>
<li> 绝对值：<code>| |</code> <img src="https://math.now.sh?inline=%7Cx-y%7C" style="display:inline-block;margin: 0;"></li>
<li> 平均数：<code>\overline&#123; 算式 &#125;</code> <img src="https://math.now.sh?inline=%5Coverline%7Bxyz%7D" style="display:inline-block;margin: 0;"> (\overline{xyz})</li>
<li> 开方：
<ul>
<li> 二次：<code>\sqrt</code> <img src="https://math.now.sh?inline=%5Csqrt%7Bx%7D" style="display:inline-block;margin: 0;"> (\sqrt{x})</li>
<li> 多次：<code>\sqrt[开方数]&#123; 被开方数 &#125;</code> <img src="https://math.now.sh?inline=%5Csqrt%5B3%5D%7Bx%2By%7D" style="display:inline-block;margin: 0;"> (\sqrt[3]{x+y})</li>
</ul>
</li>
<li> 对数：<code>\log</code> <img src="https://math.now.sh?inline=%5Clog%28x%29" style="display:inline-block;margin: 0;"> (\log(x))</li>
<li> 极限：<code>\lim</code> <img src="https://math.now.sh?inline=%5Clim%5E%7Bx%20%5Cto%20%5Cinfty%7D_%7By%20%5Cto%200%7D%5Cfrac%7Bx%7D%7By%7D" style="display:inline-block;margin: 0;"> (\lim^{x \to \infty}_{y \to 0}\frac{x}{y})</li>
<li> 求和：</li>
<li> 积分：</li>
<li> 微分：</li>
<li> 矩阵：</li>
</ul>
]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>回归算法 Regression Algorithm</title>
    <url>/2020/09/22/Regression-Algorithms/</url>
    <content><![CDATA[<p>机器学习的一般步骤是：训练样本 - 特征抽取 - 学习函数 <img src="https://math.now.sh?inline=y%5E*%20%3D%20argmax%5C%20f%28X_i%29" style="display:inline-block;margin: 0;"> - 预测。其中，为了使模型型获得更多训练，预测和学习函数是在不断循环的。</p>
<p>机器学习的问题可以分为两种类型：<strong>分类问题 </strong> 和<strong>预测问题</strong>。分类模型是将数据最终归于某一个类型，而预测模型的结果更多的是一个具体数值。这里先讲一讲机器学习中经典的回归算法。</p>
<a id="more"></a>
<h2 id="线性回归">线性回归</h2>
<p>我们用一个例子来展现线性回归的建模过程。下表是一个银行判断用户贷款额度的数据集，其中，银行给用户的贷款额度取决于两个因素：工资和年龄，这两个因素就是这个数据集的_特征_。</p>
<table>
<thead>
<tr>
<th>工资（<img src="https://math.now.sh?inline=x_1" style="display:inline-block;margin: 0;">）</th>
<th>年龄（<img src="https://math.now.sh?inline=x_2" style="display:inline-block;margin: 0;">）</th>
<th>额度（<img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;">）</th>
</tr>
</thead>
<tbody>
<tr>
<td>4000</td>
<td>25</td>
<td>20000</td>
</tr>
<tr>
<td>8000</td>
<td>30</td>
<td>70000</td>
</tr>
<tr>
<td>5000</td>
<td>28</td>
<td>35000</td>
</tr>
<tr>
<td>7500</td>
<td>33</td>
<td>50000</td>
</tr>
<tr>
<td>12000</td>
<td>40</td>
<td>85000</td>
</tr>
</tbody>
</table>
<h3 id="建立模型">建立模型</h3>
<p>根据上表，给每个特征 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"> 一个参数 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;">，构成根据特征预测出的额度 <img src="https://math.now.sh?inline=h_%5Ctheta%28x%29" style="display:inline-block;margin: 0;">：</p>
<p><img src="https://math.now.sh?inline=%5Cbegin%7Baligned%7D%20h_%7B%5Ctheta%7D%28x%29%20%26%20%3D%20%5Ctheta_0%20%2B%20%5Ctheta_1x_1%20%2B%20%5Ctheta_2x_2%20%5C%5C%20%26%20%3D%20%5Csum_%7Bi%3D0%7D%5E%7B2%7D%5Ctheta_ix_i%20%5C%5C%20%26%20%3D%20%5Ctheta%5ETx%20%5Cend%7Baligned%7D" style="display:inline-block;margin: 0;"></p>
<p>但是预测的额度与真实值之间可能会有一定的误差，于是真实的额度可以表示为</p>
<p><img src="https://math.now.sh?inline=%5Cunderbrace%7By%5E%7B%28i%29%7D%7D_%7B%E7%9C%9F%E5%AE%9E%E5%80%BC%7D%20%3D%20%5Cunderbrace%7B%5Ctheta%5ETx%5E%7B(i)%7D%7D_%7B%E9%A2%84%E6%B5%8B%E5%80%BC%7D%20%2B%20%5Cunderbrace%7B%5Cvarepsilon%5E%7B(i)%7D%7D_%7B%E8%AF%AF%E5%B7%AE%7D%20%5Ctag%7B1%7D" style="display:inline-block;margin: 0;"></p>
<p>其中误差 <img src="https://math.now.sh?inline=%5Cvarepsilon%5E%7B%28i%29%7D" style="display:inline-block;margin: 0;"> 是_独立_（样本直接无关联）且_具有相同分布_（大部分误差在一个范围中）的，通常被认为服从均值为 0、方差为 <img src="https://math.now.sh?inline=%5Ctheta%5E2" style="display:inline-block;margin: 0;"> 的 <strong> 高斯分布</strong></p>
<ul>
<li>
<p>假设误差分布为 <img src="https://math.now.sh?inline=p" style="display:inline-block;margin: 0;"> （即 <img src="https://math.now.sh?inline=p" style="display:inline-block;margin: 0;"> 为高斯分布（均值为 0））</p>
<p><img src="https://math.now.sh?inline=p%28%5Cvarepsilon%5E%7B(i%29%7D)%20%3D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20%5Cexp(-%5Cfrac%7B(%5Cvarepsilon%5E%7B(i)%7D)%5E2%7D%7B2%5Csigma%5E2%7D)" style="display:inline-block;margin: 0;"></p>
<p>其中 <img src="https://math.now.sh?inline=%5Cexp%28n%29%20%3D%20e%5En" style="display:inline-block;margin: 0;"></p>
<p>用公式 <img src="https://math.now.sh?inline=%281%29" style="display:inline-block;margin: 0;"> 替换误差 <img src="https://math.now.sh?inline=%5Cvarepsilon%5E%7B%28i%29%7D" style="display:inline-block;margin: 0;">，得到</p>
<p><img src="https://math.now.sh?inline=p%28y%5E%7B(i%29%7D%7Cx%5E%7B(i)%7D%3B%5Ctheta)%20%3D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20%5Cexp(-%5Cfrac%7B(y%5E%7B(i)%7D-%5Ctheta%5ETx%5E%7B(i)%7D)%5E2%7D%7B2%5Csigma%5E2%7D)%20%5Ctag%7B2%7D" style="display:inline-block;margin: 0;"></p>
<p>这里 <img src="https://math.now.sh?inline=p%28y%5E%7B(i%29%7D%7Cx%5E%7B(i)%7D%3B%5Ctheta)%20%5Cin%20%5B0%2C1%5D" style="display:inline-block;margin: 0;">，即 <img src="https://math.now.sh?inline=p" style="display:inline-block;margin: 0;"> 为概率值，它越接近 1，概率越大，预测值越接近真实值。</p>
</li>
</ul>
<h4 id="似然函数和目标函数">似然函数和目标函数</h4>
<p>我们希望每个预测值都尽量接近真实值，所以构造了 <strong> 似然函数</strong>：</p>
<p><img src="https://math.now.sh?inline=%5Cbegin%7Baligned%7D%20L%28%5Ctheta%29%20%26%20%3D%20%5Cprod_%7Bi%3D1%7D%5E%7Bm%7Dp(y%5E%7B(i)%7D%7Cx%5E%7B(i)%7D%3B%5Ctheta)%20%5C%5C%20%26%20%3D%20%5Cprod_%7Bi%3D1%7D%5E%7Bm%7D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20%5Cexp(-%5Cfrac%7B(y%5E%7B(i)%7D-%5Ctheta%5ETx%5E%7B(i)%7D)%5E2%7D%7B2%5Csigma%5E2%7D)%20%5Cend%7Baligned%7D%20%5Ctag%7B3%7D" style="display:inline-block;margin: 0;"></p>
<p>这里_累乘所有样本的概率值，目标是使 <img src="https://math.now.sh?inline=L%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 尽可能大_。（因为如果每个训练样本的预测值都接近真实值，即误差很小，那么每个概率 <img src="https://math.now.sh?inline=p" style="display:inline-block;margin: 0;"> 都会接近 1，于是它们的累乘也会接近 1。）</p>
<p>为了方便计算，我们将 <img src="https://math.now.sh?inline=L%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 中的乘法转换成加法运算，即对 <img src="https://math.now.sh?inline=L%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 做对数运算，得到 <strong> 对数似然函数</strong>：</p>
<p><img src="https://math.now.sh?inline=%5Cbegin%7Baligned%7D%20l%28%5Ctheta%29%20%26%20%3D%20%5Clog%20L(%5Ctheta)%20%5C%5C%20%26%20%3D%20%5Clog%20%5Cprod_%7Bi%3D1%7D%5E%7Bm%7D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20%5Cexp(-%5Cfrac%7B(y%5E%7B(i)%7D-%5Ctheta%5ETx%5E%7B(i)%7D)%5E2%7D%7B2%5Csigma%5E2%7D)%20%5C%5C%20%26%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%5Clog%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20%5Cexp(-%5Cfrac%7B(y%5E%7B(i)%7D-%5Ctheta%5ETx%5E%7B(i)%7D)%5E2%7D%7B2%5Csigma%5E2%7D)%20%5C%5C%20%26%20%3D%20m%20%5Clog%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20-%20%5Cfrac%7B1%7D%7B%5Csigma%5E2%7D%20%5Ccdot%20%5Cfrac12%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%20(y%5E%7B(i)%7D-%5Ctheta%5ETx%5E%7B(i)%7D)%5E2%20%5Cend%7Baligned%7D%20%5Ctag%7B4%7D" style="display:inline-block;margin: 0;"></p>
<p>因为 <img src="https://math.now.sh?inline=l%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 的前半部分 <img src="https://math.now.sh?inline=m%20%5Clog%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D" style="display:inline-block;margin: 0;"> 为常数，不会被 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> 改变，所以要使 <img src="https://math.now.sh?inline=l%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 尽可能大，它的后半部分 <img src="https://math.now.sh?inline=%5Cfrac%7B1%7D%7B%5Csigma%5E2%7D%20%5Ccdot%20%5Cfrac12%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%20%28y%5E%7B(i%29%7D-%5Ctheta%5ETx%5E%7B(i)%7D)%5E2" style="display:inline-block;margin: 0;"> 需要尽可能小（这里后半部分一定大于 0，所以需要使其尽可能接近 0）。最后得到 <strong> 目标函数</strong></p>
<p><img src="https://math.now.sh?inline=J%28%5Ctheta%29%20%3D%20%5Cfrac12%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D(h_%7B%5Ctheta%7D(x%5E%7B(i)%7D)-y%5E%7B(i)%7D)%5E2%20%5Ctag%7B5%7D" style="display:inline-block;margin: 0;"></p>
<p>（留下 <img src="https://math.now.sh?inline=%5Cfrac12" style="display:inline-block;margin: 0;"> 方便求导）</p>
<h4 id="求导">求导</h4>
<p>我们的目标是找到_使 <img src="https://math.now.sh?inline=J%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 尽可能小的 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> 值_。对一个函数取极值，可以对其进行求导。</p>
<p>我们用矩阵形式表示 <img src="https://math.now.sh?inline=%285%29" style="display:inline-block;margin: 0;"> 中 <img src="https://math.now.sh?inline=%28h_%7B%5Ctheta%7D(x%5E%7B(i%29%7D)-y%5E%7B(i)%7D)%5E2" style="display:inline-block;margin: 0;">：</p>
<p><img src="https://math.now.sh?inline=J%28%5Ctheta%29%20%3D%20%5Cfrac12%20(X%5Ctheta-y)%5ET(X%5Ctheta-y)" style="display:inline-block;margin: 0;"></p>
<p>为了求 <img src="https://math.now.sh?inline=J%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 的极值，需要对 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> <em>求偏导</em></p>
<p><img src="https://math.now.sh?inline=%5Cbegin%7Baligned%7D%20%5Cnabla_%5Ctheta%20J%28%5Ctheta%29%20%26%20%3D%20%5Cnabla_%5Ctheta%20(%5Cfrac12%20(X%5Ctheta-y)%5ET(X%5Ctheta-y))%20%5C%5C%20%26%20%3D%20%5Cnabla_%5Ctheta%20(%5Cfrac12%20(%5Ctheta%5ETX%5ET-y%5ET)(X%5Ctheta-y))%20%5C%5C%20%26%20%3D%20%5Cnabla_%5Ctheta%20(%5Cfrac12%20(%5Ctheta%5ETX%5ETX%5Ctheta%20-%20%5Ctheta%5ETX%5ETy%20-%20y%5ETX%5Ctheta%20%2B%20y%5ETy))%20%5C%5C%20%26%20%3D%20%5Cfrac12%20(2X%5ETX%5Ctheta%20-%20X%5ETy%20-%20(y%5ETX)%5ET)%20%5C%5C%20%26%20%3D%20X%5ETX%5Ctheta%20-%20X%5ETy%20%5Cend%7Baligned%7D" style="display:inline-block;margin: 0;"></p>
<p>令 <img src="https://math.now.sh?inline=%5Cnabla_%5Ctheta%20J%28%5Ctheta%29%3D0" style="display:inline-block;margin: 0;">，即 <img src="https://math.now.sh?inline=X%5ETX%5Ctheta%20-%20X%5ETy%3D0" style="display:inline-block;margin: 0;">，则</p>
<p><img src="https://math.now.sh?inline=%5Ctheta%20%3D%20%28X%5ETX%29%5E%7B-1%7DX%5ETy%20%5Ctag%7B6%7D" style="display:inline-block;margin: 0;"></p>
<h2 id="Logistic- 回归">Logistic 回归</h2>
<p>逻辑回归是一个分类算法。</p>
<h3 id="S- 函数（Sigmoid- 函数）">S 函数（Sigmoid 函数）</h3>
<p><img src="/2020/09/22/Regression-Algorithms/790418-20181107181130984-1052306153.png" alt="img"></p>
<ul>
<li>
<p>函数表达式为 <img src="https://math.now.sh?inline=g%28z%29%20%3D%20%5Cfrac%7B1%7D%7B1%2Be%5E%7B(-z)%7D%7D" style="display:inline-block;margin: 0;"></p>
</li>
<li>
<p>取值范围 <img src="https://math.now.sh?inline=x%20%5Cin%20%28-%5Cinfty%2C%2B%5Cinfty%29%2C%5C%20y%20%5Cin%20(0%2C1)" style="display:inline-block;margin: 0;"></p>
</li>
</ul>
<h4 id="建模">建模</h4>
<p>由 S 函数可以看出，逻辑回归就是把模型的计算结果映射在 S 函数上，使最终结果在 <img src="https://math.now.sh?inline=%280%2C1%29" style="display:inline-block;margin: 0;"> 区间上。我们可以设置 0.5 为分界，结果小于 0.5 的数据归于一类，大于 0.5 的归于一类。</p>
<p>令<br>
<img src="https://math.now.sh?inline=h_%5Ctheta%28x%29%20%3D%20g(%5Ctheta%5ETx)%20%3D%20%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Ctheta%5ETx%7D%7D" style="display:inline-block;margin: 0;"><br>
这里符号含义同上，即 <img src="https://math.now.sh?inline=h_%5Ctheta%28x%29" style="display:inline-block;margin: 0;"> 是预测值。</p>
<p>之后建立目标函数的步骤与线性回归一致。</p>
<h3 id="梯度下降">梯度下降</h3>
<p>参数 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> 是无法一步优化到位的，我们需要一步一步使 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> 向函数的“坡下”走，这个向下走的“方向”就是函数在当前位置对 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> 的偏导数。</p>
<p>假设数据集只有一个特征，则预测值为 <img src="https://math.now.sh?inline=h_%5Ctheta%28x%29%3D%5Ctheta_1x%2B%5Ctheta_0" style="display:inline-block;margin: 0;"></p>
<p>目标函数为</p>
<p><img src="https://math.now.sh?inline=J%28%5Ctheta_0%2C%5Ctheta_1%29%20%3D%20%5Cfrac%7B1%7D%7B2m%7D%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D(h_%7B%5Ctheta%7D(x%5E%7B(i)%7D)-y%5E%7B(i)%7D)%5E2" style="display:inline-block;margin: 0;"></p>
<p>（这里如果不乘 <img src="https://math.now.sh?inline=%5Cfrac1m" style="display:inline-block;margin: 0;">，意味着训练样本越多 <img src="https://math.now.sh?inline=J%28%5Ctheta%29" style="display:inline-block;margin: 0;"> 越大）</p>
<p>对 <img src="https://math.now.sh?inline=%5Ctheta_0" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=%5Ctheta_1" style="display:inline-block;margin: 0;"> 分别求偏导</p>
<p><img src="https://math.now.sh?inline=%5Cfrac%7B%5Cpartial%20J%28%5Ctheta_0%2C%5Ctheta_1%29%7D%7B%5Cpartial%20%5Ctheta_0%7D%20%3D%20%5Cfrac1m%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D(h_%7B%5Ctheta%7D(x%5E%7B(i)%7D)-y%5E%7B(i)%7D)" style="display:inline-block;margin: 0;"></p>
<p><img src="https://math.now.sh?inline=%5Cfrac%7B%5Cpartial%20J%28%5Ctheta_0%2C%5Ctheta_1%29%7D%7B%5Cpartial%20%5Ctheta_1%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D(h_%7B%5Ctheta%7D(x%5E%7B(i)%7D)-y%5E%7B(i)%7D)%20*%20x%5E%7B(i)%7D" style="display:inline-block;margin: 0;"></p>
<p>使每个参数沿梯度下降方向前进 <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"> 步</p>
<p><img src="https://math.now.sh?inline=%5Ctheta_0%20%3A%3D%20%5Ctheta_0%20-%20%5Calpha%20*%20%5Cfrac%7B%5Cpartial%20J%28%5Ctheta_0%2C%5Ctheta_1%29%7D%7B%5Cpartial%20%5Ctheta_0%7D" style="display:inline-block;margin: 0;"></p>
<p><img src="https://math.now.sh?inline=%5Ctheta_1%20%3A%3D%20%5Ctheta_1%20-%20%5Calpha%20*%20%5Cfrac%7B%5Cpartial%20J%28%5Ctheta_0%2C%5Ctheta_1%29%7D%7B%5Cpartial%20%5Ctheta_1%7D" style="display:inline-block;margin: 0;"></p>
<p>这里 <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"> 称为 <strong> 学习率</strong>（或步长），步长过大可能略过最优点，步长太小迭代速度太慢。</p>
<p>当参数经过迭代后改变很小时，迭代结束，我们认为当前 <img src="https://math.now.sh?inline=%5Ctheta" style="display:inline-block;margin: 0;"> 是（或近似）最优解。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>linear regression</tag>
        <tag>logistic regression</tag>
      </tags>
  </entry>
  <entry>
    <title>Xgboost</title>
    <url>/2020/09/25/Xgboost/</url>
    <content><![CDATA[<p>Xgboost 是一套提升树可扩展性的机器学习系统，它的算法原理就是将多棵树的预测结果相加得到最终预测结果。之前看了一些博客和视频教程，看完还是一头雾水，最后还是看了原论文才终于有了头绪。</p>
<a id="more"></a>
<h3 id="正则化目标函数">正则化目标函数</h3>
<p>首先，给定一个数据集 <img src="https://math.now.sh?inline=%5Ccal%7BD%7D" style="display:inline-block;margin: 0;"> 包含 <img src="https://math.now.sh?inline=n" style="display:inline-block;margin: 0;"> 个数据，其中每个数据有 <img src="https://math.now.sh?inline=m" style="display:inline-block;margin: 0;"> 个特征，即</p>
<p><img src="https://math.now.sh?inline=%5Ccal%7BD%7D%3D%5C%7B%28x_i%2Cy_i%29%5C%7D(%7CD%7C%3Dn%2C%5C%20x_i%20%5Cin%20%5CBbb%7BR%7D%5Em%2C%5C%20y_i%20%5Cin%20%5CBbb%7BR%7D)" style="display:inline-block;margin: 0;"></p>
<p>Xgboost 是一个树集成模型，它将 <img src="https://math.now.sh?inline=K" style="display:inline-block;margin: 0;"> 个树的预测结果进行求和得到最终预测值。即每一个样本的预测值为</p>
<p><img src="https://math.now.sh?inline=%5Chat%7By%7D_i%3D%5Cphi%28x_i%29%3D%5Csum_%7Bk%3D1%7D%5E%7BK%7Df_k(x_i)%2C%5C%20f_k%20%5Cin%20%5Ccal%7BF%7D%20%5Ctag%7B1%7D" style="display:inline-block;margin: 0;"></p>
<p>其中，</p>
<ul>
<li>
<p><img src="https://math.now.sh?inline=%5Ccal%7BF%7D%3D%5C%7Bf%28x%29%3Dw_%7Bq(x)%7D%5C%7D(q%3A%5CBbb%7BR%7D%5Em%20%5Cto%20%5Cmit%7BT%7D%2C%5C%20w%20%5Cin%20%5CBbb%7BR%7D%5ET)" style="display:inline-block;margin: 0;"> 是回归树（CART 树）的空间；</p>
</li>
<li>
<p><img src="https://math.now.sh?inline=q" style="display:inline-block;margin: 0;"> 代表每棵树的结构，它把样本映射到树中对应的叶子节点，比如一个样本 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"> 被树划分到一个叶子节点，这个叶子节点的编号为 <img src="https://math.now.sh?inline=q%28x%29" style="display:inline-block;margin: 0;">。</p>
<p>（所以 <img src="https://math.now.sh?inline=%5Ccal%7BF%7D" style="display:inline-block;margin: 0;"> 就是样本 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"> 被每棵树结构 <img src="https://math.now.sh?inline=q" style="display:inline-block;margin: 0;"> 映射到的叶节点 <img src="https://math.now.sh?inline=q%28x%29" style="display:inline-block;margin: 0;"> 的权重的集合）</p>
</li>
<li>
<p><img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"> 是树的叶子节点个数。</p>
</li>
<li>
<p>每一个 <img src="https://math.now.sh?inline=f_k" style="display:inline-block;margin: 0;"> 对应一个独立的树结构 <img src="https://math.now.sh?inline=q" style="display:inline-block;margin: 0;"> 和叶子节点的权重 <img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;">。</p>
</li>
</ul>
<p>这里的树是回归树，也就是树的每个叶子节点都包含一个权重 <img src="https://math.now.sh?inline=w_i" style="display:inline-block;margin: 0;"> （第 <img src="https://math.now.sh?inline=i" style="display:inline-block;margin: 0;"> 个叶子节点的权重）。对于每一个样本，我们根据每一棵回归树把它划分到每棵树的叶子节点中，然后对这些叶子节点的权重求和得到该样本的预测结果。</p>
<p>于是我们的目标就是最小化 <strong> 目标函数</strong>：</p>
<p><img src="https://math.now.sh?inline=%5Ccal%7BL%7D%28%5Cphi%29%3D%5Csum_%7Bi%7Dl(%5Chat%7By%7D_i%2Cy_i)%2B%5Csum_%7Bk%7D%5COmega(f_k)%20%5Ctag%7B2%7D" style="display:inline-block;margin: 0;"></p>
<p>其中</p>
<ul>
<li>
<p><img src="https://math.now.sh?inline=%5COmega%28f%29%3D%5Cgamma%20T%20%2B%20%5Cfrac12%20%5Clambda%20%7C%7Cw%7C%7C%5E2" style="display:inline-block;margin: 0;"> （<img src="https://math.now.sh?inline=%7C%7Cw%7C%7C%5E2%20%3D%20%5Csum_%7Bj%3D1%7D%5E%7BT%7Dw_j%5E2" style="display:inline-block;margin: 0;">）</p>
</li>
<li>
<p><img src="https://math.now.sh?inline=l" style="display:inline-block;margin: 0;"> 是 <strong> 损失函数</strong>，表示预测值 <img src="https://math.now.sh?inline=%5Chat%7By%7D_i" style="display:inline-block;margin: 0;"> 和真实值 <img src="https://math.now.sh?inline=y_i" style="display:inline-block;margin: 0;"> 之间的误差。</p>
</li>
<li>
<p><img src="https://math.now.sh?inline=%5COmega" style="display:inline-block;margin: 0;"> 用来控制（回归树）模型的复杂度，避免过拟合。</p>
</li>
</ul>
<p><img src="https://math.now.sh?inline=%282%29" style="display:inline-block;margin: 0;"> 中目标函数的优化参数是模型（即 <img src="https://math.now.sh?inline=f_k" style="display:inline-block;margin: 0;">）。因为整个 Xgboost 模型是以求和回归树的方式训练的，所以相当于训练中每一轮都加入一个回归树模型，即</p>
<ul>
<li>
<p>一开始，预测值 <img src="https://math.now.sh?inline=%5Chat%7By%7D_i%5E%7B%280%29%7D%3D0" style="display:inline-block;margin: 0;"></p>
</li>
<li>
<p>之后每一轮都是前一轮的预测值加上一个新的回归树模型，即在第 <img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"> 轮，模型的预测值为</p>
<p><img src="https://math.now.sh?inline=%5Chat%7By%7D_i%5E%7B%28t%29%7D%3D%5Chat%7By%7D_i%5E%7B(t-1)%7D%2Bf_t(x_i)" style="display:inline-block;margin: 0;"></p>
</li>
</ul>
<p>于是目标函数变为</p>
<p><img src="https://math.now.sh?inline=%5Ccal%7BL%7D%5E%7B%28t%29%7D%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dl(y_i%2C%5Chat%7By%7D_i%5E%7B(t-1)%7D%2Bf_t(x_i))%2B%5COmega(f_t)" style="display:inline-block;margin: 0;"></p>
<p>为了能快速优化目标函数，我们将目标函数进行二阶泰勒展开，得到近似目标函数（也就是说这个展开后的函数近似原来的目标函数，但并不完全相等）：</p>
<p><img src="https://math.now.sh?inline=%5Ccal%7BL%7D%5E%7B%28t%29%7D%20%5Csimeq%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Bl(y_i%2C%5Chat%7By%7D_i%5E%7B(t-1)%7D)%2Bg_if_t(x_i)%2B%5Cfrac12h_if_t%5E2(x_i)%5D%2B%5COmega(f_t)" style="display:inline-block;margin: 0;"></p>
<p>其中 <img src="https://math.now.sh?inline=g_i%3D%5Cpartial_%7B%5Chat%7By%7D%5E%7B%28t-1%29%7D%7Dl(y_i%2C%5Chat%7By%7D%5E%7B(t-1)%7D)" style="display:inline-block;margin: 0;">，<img src="https://math.now.sh?inline=h_i%3D%5Cpartial%5E2_%7B%5Chat%7By%7D%5E%7B%28t-1%29%7D%7Dl(y_i%2C%5Chat%7By%7D%5E%7B(t-1)%7D)" style="display:inline-block;margin: 0;"> 分别是损失函数的一阶和二阶偏导。</p>
<p>然后，我们可以移除目标函数中的常量，得到</p>
<p><img src="https://math.now.sh?inline=%5Cbar%7B%5Ccal%7BL%7D%7D%5E%7B%28t%29%7D%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Bg_if_t(x_i)%2B%5Cfrac12h_if_t%5E2(x_i)%5D%2B%5COmega(f_t)%20%5Ctag%7B3%7D" style="display:inline-block;margin: 0;"></p>
<p>令 <img src="https://math.now.sh?inline=I_j%3D%5C%7Bi%7Cq%28x_i%29%3Dj%5C%7D" style="display:inline-block;margin: 0;"> 为叶子节点 <img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"> 中样本的集合（即被划分到该叶子节点的所有样本），我们可以将 <img src="https://math.now.sh?inline=%283%29" style="display:inline-block;margin: 0;"> 写为</p>
<p><img src="https://math.now.sh?inline=%5Cbegin%7Baligned%7D%20%5Cbar%7B%5Ccal%7BL%7D%7D%5E%7B%28t%29%7D%20%26%20%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Bg_if_t(x_i)%2B%5Cfrac12h_if_t%5E2(x_i)%5D%2B%5Cgamma%20T%20%2B%20%5Cfrac12%5Clambda%5Csum_%7Bj%3D1%7D%5E%7BT%7Dw_j%5E2%20%5C%5C%20%26%20%3D%5Csum_%7Bj%3D1%7D%5E%7BT%7D%5B(%5Csum_%7Bi%20%5Cin%20I_j%7Dg_i)w_j%2B%5Cfrac12(%5Csum_%7Bi%20%5Cin%20I_j%7Dh_i%2B%5Clambda)w_j%5E2%5D%2B%5Cgamma%20T%20%5Cend%7Baligned%7D%20%5Ctag%7B4%7D" style="display:inline-block;margin: 0;"></p>
<p>求这个目标函数的最优解</p>
<p><img src="https://math.now.sh?inline=w_j%5E*%3D-%5Cfrac%7B%5Csum_%7Bi%20%5Cin%20I_j%7Dg_i%7D%7B%5Csum_%7Bi%20%5Cin%20I_j%7Dh_i%2B%5Clambda%7D%20%5Ctag%7B5%7D" style="display:inline-block;margin: 0;"></p>
<p>带入 <img src="https://math.now.sh?inline=w_j%5E*" style="display:inline-block;margin: 0;"> 得到最优目标函数</p>
<p><img src="https://math.now.sh?inline=%5Cbar%7B%5Ccal%7BL%7D%7D%5E%7B%28t%29%7D(q)%3D-%5Cfrac12%5Csum_%7Bj%3D1%7D%5E%7BT%7D%5Cfrac%7B(%5Csum_%7Bi%20%5Cin%20I_j%7Dg_i)%5E2%7D%7B%5Csum_%7Bi%20%5Cin%20I_j%7Dh_i%2B%5Clambda%7D%2B%5Cgamma%20T%20%5Ctag%7B6%7D" style="display:inline-block;margin: 0;"></p>
<p><img src="https://math.now.sh?inline=%5Cbar%7B%5Ccal%7BL%7D%7D%5E%7B%28t%29%7D(q)" style="display:inline-block;margin: 0;"> 可以用来衡量一个树结构 <img src="https://math.now.sh?inline=q" style="display:inline-block;margin: 0;"> 的好坏（类似决策树中的纯度），<img src="https://math.now.sh?inline=%5Cbar%7B%5Ccal%7BL%7D%7D%5E%7B%28t%29%7D(q)" style="display:inline-block;margin: 0;"> 越小，树结构越好。</p>
<h3 id="节点划分">节点划分</h3>
<p>通常我们无法枚举所有树结构，所以我们用一种贪婪算法：从单个叶节点开始，迭代地给树添加分支。</p>
<p>假设 <img src="https://math.now.sh?inline=I_L" style="display:inline-block;margin: 0;"> 和 <img src="https://math.now.sh?inline=I_R" style="display:inline-block;margin: 0;"> 是节点分裂后的左节点的样本集和右节点的样本集，令 <img src="https://math.now.sh?inline=I%3DI_L%20%5Ccup%20I_R" style="display:inline-block;margin: 0;">，节点切分后的损失函数为</p>
<p><img src="https://math.now.sh?inline=%5Ccal%7BL%7D_%7Bsplit%7D%3D%5Cfrac12%5B%5Cfrac%7B%28%5Csum_%7Bi%20%5Cin%20%5Cmit%7BI%7D_L%7Dg_i%29%5E2%7D%7B%5Csum_%7Bi%20%5Cin%20%5Cmit%7BI%7D_L%7Dh_i%2B%5Clambda%7D%2B%5Cfrac%7B(%5Csum_%7Bi%20%5Cin%20%5Cmit%7BI%7D_R%7Dg_i)%5E2%7D%7B%5Csum_%7Bi%20%5Cin%20%5Cmit%7BI%7D_R%7Dh_i%2B%5Clambda%7D-%5Cfrac%7B(%5Csum_%7Bi%20%5Cin%20%5Cmit%7BI%7D%7Dg_i)%5E2%7D%7B%5Csum_%7Bi%20%5Cin%20%5Cmit%7BI%7D%7Dh_i%2B%5Clambda%7D%5D-%5Cgamma%20%5Ctag%7B7%7D" style="display:inline-block;margin: 0;"></p>
<p>我们的目标是找到一个特征，使切分后的损失减少最大。<img src="https://math.now.sh?inline=%5Cgamma" style="display:inline-block;margin: 0;"> 除了控制树的复杂度，也作为阈值，当分裂后增益大于 <img src="https://math.now.sh?inline=%5Cgamma" style="display:inline-block;margin: 0;"> 时才分裂，起到了预剪枝作用。</p>
<h3 id="缩减和列采样">缩减和列采样</h3>
<p>缩减和列采样也用来防止过拟合。</p>
<ul>
<li>缩减是在每一步 tree boosting （增加树）后引入缩减系数 <img src="https://math.now.sh?inline=%5Ceta" style="display:inline-block;margin: 0;"> ，对新增加的权重进行收缩，减少每棵树的影响，为以后加入的树留下改进模型的空间。</li>
<li>列采样通常用在随机森林中，它有时比传统的行采样更能防止过拟合，它也加速了并行计算。</li>
</ul>
<p>之后还有一部分算法，比如寻找最佳分割点等，下次补充。</p>
<p>参考：</p>
<p><a href="https://arxiv.org/pdf/1603.02754v1.pdf">XGBoost: A Scalable Tree Boosting System</a>（这是 Xgboost 的论文）</p>
<p><a href="https://www.jianshu.com/p/a62f4dce3ce8">XGBoost 原理</a>（这篇基本是翻译了 Xgboost 的论文，有些英文看不明白的可以看看中文，不过我看到一半发现一处翻译错误，已在我的这篇订正）</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>xgboost</tag>
      </tags>
  </entry>
</search>
